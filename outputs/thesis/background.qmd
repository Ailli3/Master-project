---
editor: 
  markdown: 
    wrap: 72
---

# Background {#sec-bg}

Linear (mixed) models are the primary method of analysis of experimental data with response that have an approximate Normal distribution. In this chapter, we expand on the model 
$$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau} + \boldsymbol{\epsilon}$$
to consider linear mixed models for the analysis of experimental data with a more general experimental structure.

## Linear mixed model

Linear mixed model extends linear model by incorporating additionally
incorporating random effects into the model that effectively give
greater flexibility and capability to incorporate known correlated
structures into the model. We now consider a linear mixed model $$
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}+\boldsymbol{\epsilon}
$$ {#eq-lmm} here $\boldsymbol{y}$ is $n\times 1$ vector for $n$
observations, $\boldsymbol{\tau}$ is a $t\times1$ parameter vector of
treatment factors, $\boldsymbol{u}$ is a $q \times1$ parameter vector of
blocking effects, and $\boldsymbol{\epsilon}$ is the $n\times 1$ error
vector, $\boldsymbol{X}$ and $\boldsymbol{Z}$ are design matrices of
dimension $n \times t$ and $n \times q$ for treatment factors and
blocking factors, respectively. We here assume blocking factors are
random effect, with random error $\boldsymbol{\epsilon}$ we have
$$
\begin{bmatrix}
\boldsymbol{u} \\
\boldsymbol{\epsilon} 
\end{bmatrix}
\sim
N\left(
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol{0}
\end{bmatrix}
,
\begin{bmatrix}
\boldsymbol{G} & \boldsymbol{0} \\
\boldsymbol{0} & \boldsymbol{R}
\end{bmatrix}
\right)
$$ {#eq-dist}
where $\boldsymbol{G}$ is the $q \times q$ variance matrix for
$\boldsymbol{u}$ and $\boldsymbol{R}$ is $n\times n$ variance matrix for
$\boldsymbol{\epsilon}$.

Throughout this thesis, we assume $\boldsymbol{R} = \sigma^2\mathbf{I}_n$ and $\boldsymbol{G}$ is a block diagonal matrix where each block is a scaled identity matrix. In practice, these structures can be more complex, e.g. @williamsConstructionCrossoverDesigns2007 consider autoregressive and linear variance structures for crossover designs. 

In addition, we assume in this thesis that the treatment effects are fixed and blocking factors (i.e. the row and column factors in row-column design) are random effects. It should be noted, however, that the treatment effects can be random effects based on the purpose of the experiment [@robinsonThatBLUPGood1991;@butlerDesignFieldExperiments2014].

### Estimation of model parameters

#### Fixed and random effects 

@hendersonEstimationEnvironmentalGenetic1959 first derived the set of equations, referred to as _mixed model equations_ by @smithMultiplicativeMixedModels1999, as a precursor to the estimation of fixed effects and predictions of random effects. These equations are derived from maximizing the joint log density of the response and random effects as shown in @lem-mme.

::: {#lem-mme}

## Mixed model equations

The mixed model equations shown below are derived from maximising the joint log density of $\boldsymbol{y}$ and $\boldsymbol{u}$.

$$\boldsymbol{C}\hat{\boldsymbol{\beta}} = \boldsymbol{W}\boldsymbol{R}^{-1}\boldsymbol{y},$$ where $\boldsymbol{C} = \boldsymbol{W}^\top\boldsymbol{R}^{-1}\boldsymbol{W} + \boldsymbol{G}^*$, $\boldsymbol{W} = \begin{bmatrix}\boldsymbol{X} & \boldsymbol{Z} \end{bmatrix}$, $\hat{\boldsymbol{\beta}} = (\hat{\boldsymbol{\tau}}^\top, \hat{\boldsymbol{u}}^\top)^\top$ and $\boldsymbol{G}^* = \begin{bmatrix}\boldsymbol{0}_{p\times p} & \boldsymbol{0}_{p\times q} \\ \boldsymbol{0}_{q\times p} & \boldsymbol{G}^{-1}\end{bmatrix}$.

::: proof

Given the model [-@eq-lmm] with assumption in [-@eq-dist], we have $\boldsymbol{u}\sim N(\boldsymbol{0},\boldsymbol{G})$ and $\boldsymbol{y}|\boldsymbol{u}\sim N(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u},\boldsymbol{R})$ with probability density functions as
$$\begin{aligned}f_u(\boldsymbol{u}) &= \frac{1}{\sqrt{(2\pi)^{q}|\boldsymbol{G}|}}\exp\left(-\frac{1}{2}\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right)\\
f_{y|u}(\boldsymbol{y}|\boldsymbol{u}) &= \frac{1}{\sqrt{(2\pi)^{n}|\boldsymbol{R}|}}\exp\left(-\frac{1}{2}(\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}))^\top\boldsymbol{R}^{-1}(\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}))\right).
\end{aligned}$$
Ignoring the constants, we have
$$\begin{aligned}
\log f_u(\boldsymbol{u}) &= -\frac{1}{2}\left(\log |\boldsymbol{G}|+\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right)\\
\log f_{y|u}(\boldsymbol{y}|\boldsymbol{u}) &=-\frac{1}{2}\left(\log |\boldsymbol{R}|+(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})^\top\boldsymbol{R}^{-1}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})\right).
\end{aligned}$$
The joint density of $\boldsymbol{y}$ and $\boldsymbol{u}$ is given as 

$$f_{y,u}(\boldsymbol{y}, \boldsymbol{u}) = f_{y|u}(\boldsymbol{y}|\boldsymbol{u})f_u(\boldsymbol{u}),$$ {#eq-density}
and taking the log of both sides for @eq-density, we have the log of joint density as:
$$\begin{aligned}
\mathcal{L} :=& \log f_{y,u}(\boldsymbol{y}, \boldsymbol{u}) \\
=& \log f_{y|u}(\boldsymbol{y}|\boldsymbol{u})+\log f_u(\boldsymbol{u})\\
=& -\frac{1}{2}\left(\log|\boldsymbol{R}|+\log|\boldsymbol{G}|+(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})^\top \mathbf{R}^{-1}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})+\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right).
\end{aligned}$$
We determine that
$\frac{\partial\mathcal{L}}{\partial\boldsymbol{\tau}}=\frac{\partial\mathcal{L}}{\partial\boldsymbol{u}}=\boldsymbol{0}$,
and write the equation into a matrix form
$$
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+ \boldsymbol{G}^{-1}
\end{bmatrix}
\begin{bmatrix}
\hat{\boldsymbol{\tau}}\\
\hat{\boldsymbol{u}}
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{y}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{y}
\end{bmatrix}.
$$ {#eq-ll}

$$
\Rightarrow (\boldsymbol{W}^\top\boldsymbol{R}^{-1}\boldsymbol{W} + \boldsymbol{G}^*)\hat{\boldsymbol{\beta}}
=\boldsymbol{W}\boldsymbol{R}^{-1}\boldsymbol{y}
$$
Substituting the above $\boldsymbol{C}$ leads to our conclusion
:::

:::


@thm-blue provides the specific form for estimation of the parameters.
:::: {#thm-blue}

## Best linear unbiased estimates and predictions

The solutions to the mixed models equations in @lem-mme (shown below) give the best linear unbiased estimates (BLUEs) for $\boldsymbol{\tau}$ and best linear unbiased predictions (BLUPs) for $\boldsymbol{u}$ provided that $\boldsymbol{G}$ and $\boldsymbol{R}$ are known.

$$\begin{aligned}
\hat{\boldsymbol{\tau}} &= (\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{y}\\
\hat{\boldsymbol{u}} &= \boldsymbol{G}\boldsymbol{Z}^\top\boldsymbol{V}^{-1}(\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{\tau}}),
\end{aligned}$${#eq-esti}

where $\boldsymbol{V} = \boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top + \boldsymbol{R}$. In this paper, we primarily discuss the treatment effect, so we provide the proof for $\hat{\boldsymbol{\tau}}$.

::: proof
By cancelling $\boldsymbol{u}$, we have
$$
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})\boldsymbol{Z}^\top\boldsymbol{R}^{-1}y=\boldsymbol{X}^\top\boldsymbol{R}^{-1}y\\
$$
$$
\Rightarrow \boldsymbol{X}^\top[\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}]\boldsymbol{X}\boldsymbol{\tau}=\boldsymbol{X}^\top[\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}]y\\
$$
$$
\Rightarrow \boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}\boldsymbol{\tau}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}
$$
where
$\boldsymbol{P}=\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}$,
let
$\boldsymbol{C}_{11}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}$
then we have the form of estimation for
$\hat{\boldsymbol{\tau}}$, which is
$$
\boldsymbol{C}_{11}\hat{\boldsymbol{\tau}}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}
$$
and the estimation of $\boldsymbol{\tau}$ is
$\hat{\boldsymbol{\tau}}=\boldsymbol{C}_{11}^{-1}\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}$,
Now, we only need to prove that $\boldsymbol{P}=\boldsymbol{V}^{-1}$.

$$\begin{aligned}
\boldsymbol{P}\boldsymbol{V} &=(\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1})(\boldsymbol{R}+\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top - \boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top \\
&\quad \quad \quad \quad \quad \quad\quad\quad -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top\\
&=\boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top(\boldsymbol{I}+\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&=\boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{Z}^\top+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{I}+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G})\boldsymbol{Z}^\top\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{G}^{-1}+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z})\boldsymbol{G}\boldsymbol{Z}^\top\\
&= \boldsymbol{I}+\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top-\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top\\
& = \boldsymbol{I}
\end{aligned}$$
so we have $\hat{\boldsymbol{\tau}}=\boldsymbol{C}_{11}^{-1}\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}=\hat{\boldsymbol{\tau}}= (\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{y}$.
:::

:::

#### Variance parameters

In practice, the variance parameters for $\boldsymbol{G}$ and $\boldsymbol{R}$ are unknown and must be estimated from data. As a consequence, the solutions in @thm-blue are referred to as Empirical-BLUEs (E-BLUE) or Empirical-BLUPs (E-BLUPs).

The variance parameters may be estimated using a method of moment approach, however this approach can only be utilized for simple variance structures with balanced numbers of treatments. 

The modern prevalent approach to estimating variance parameters use maximum likelihood estimates (MLEs) or residual maximum likelihood (REML) estimates [@pattersonRecoveryInterblockInformation1971]. The latter approach takes into account the loss of degrees of freedom to the fixed effects so the estimates of variance are less biased. Interested readers should see @verbylaConditionalDerivationResidual1990 for the derivation of REML and @smithMultiplicativeMixedModels1999 for computational strategies involving the average information algorithm [@gilmourAverageInformationREML1995;@johnsonRestrictedMaximumLikelihood1995].

### Variance analyse of estimators
In linear mixed model analysis, when estimating the accuracy of parameters and variance, we typically use the Fisher information matrix to provide a method for variance analysis. For the linear mixed model @eq-lmm, we have the log-likelihood of it is then given as:
$$
\log\ell(\boldsymbol{\tau};\boldsymbol{y}) = -\frac{n}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau})^\top(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}).
$$
The $(i,j)$-th entry of the Fisher information matrix is defined as
$$
I_{ij}(\boldsymbol{\tau})=-\mathbb{E}\left(\frac{\partial^2}{\partial\tau_i\partial\tau_j}\log\ell(\boldsymbol{\tau};\boldsymbol{y})\right)
$$ 
where $\tau_i$ is the $i$-th entry of $\boldsymbol{\tau}$.

::: {#lem-fim-lm}
The Fisher information matrix of @eq-lm for $\boldsymbol{\tau}$ is given as
$$
\boldsymbol{C}_{\text{Fisher}} = -\mathbb{E}\left(\frac{\partial^2}{\partial\boldsymbol{\tau}\partial\boldsymbol{\tau}^\top}\log\ell(\boldsymbol{\tau};\boldsymbol{y})\right)
$$
:::

And we have $\boldsymbol{\tau}\sim N(\hat{\boldsymbol{\tau}},\boldsymbol{C}_{\text{Fisher}}^{-1})$. We will provide the specific form of this matrix and the related proof in the next section.

### A-criterion

Now we have our estimation for the treatment factor, and experimental
design aims to further refine our design by focusing on the precision of
these estimates. Specifically, we aim to optimize the design so that the
treatment effects are estimated with minimal variance, ensuring that the
differences between any two treatment levels are as small as possible.
To achieve this, we introduce the A-value as a criterion for evaluating
the design.


Optimizing the A-value is crucial in row-column designs, for it directly relates to the precision of the treatment effect estimates. The A-value, a measure of design efficiency, quantifies how well the experimental design minimizes the variability when estimating treatment effects. By focusing on minimizing the A-value, we aim to achieve a design that provides the most precise estimates of the difference of the effects between two treatments. A lower A-value means that the design is more efficient, leading to smaller variances for the difference between treatment effect estimations.

Recall that in this thesis, we consider treatment factors $\boldsymbol{\tau}$ are fixed, to elucidate the influence of A-criterion. Our discussion of the A-criterion below is always based on this assumption.
::: {#def-A-value}
Basing on the model formula @eq-lmm, and a estimation of treatment
factor $\hat{\boldsymbol{\tau}}$ has $n_{\tau}$ factors. A-criterion
measure the average predicted error variance of different treatments.
Let
$V_{ij}= var(\hat{\tau}_i-\hat{\tau}_j)=var(\hat{\tau}_i)+var(\hat{\tau}_j)-2cov(\hat{\tau}_i,\hat{\tau}_j)$,
and an A-value $\mathcal{A}$ is
$$
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}\sum_{i}\sum_{j<i}V_{ij}
$${#eq-a}
:::

To discover the relationship between the A-value and the design matrix,
I need to find the variance-covariance matrix of
$\hat{\boldsymbol{\tau}}$. In fact, it can be proof that


::: {#lem-cov}
For treatment effect and its estimator @eq-esti, we have $(\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})\sim N(0,\boldsymbol{C}_{11}^{-1})$.

::: proof
From @eq-lmm and basing on assumption @eq-dist we have
$$
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}+\boldsymbol{\epsilon}\sim N(\boldsymbol{X}\boldsymbol{\tau},\boldsymbol{R}+\boldsymbol{ZGZ}^\top),
$${#eq-asp}

 so also we have
$\hat{\boldsymbol{\tau}}\sim N(\boldsymbol{\tau},\boldsymbol{X}^\top\boldsymbol{R}^{-1}(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)(\boldsymbol{X}^\top\boldsymbol{R}^{-1})^\top)$

We denote
$$\begin{aligned}
\boldsymbol{M} &= \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{X}\\
\boldsymbol{N} &= \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z}\\
\boldsymbol{J} &= \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{X}\\
\boldsymbol{K} &= \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1}\\
\end{aligned}$$
In the context of row-column design, the $\boldsymbol{K}$ matrix is invertible. Schur complement of $\boldsymbol{K}$ is 
$$
\boldsymbol{S} = \boldsymbol{M} - \boldsymbol{N} \boldsymbol{K}^{-1} \boldsymbol{J}
$$
And the inverse matrix of $\boldsymbol{C}$ can be written as
$$
\boldsymbol{C}^{-1}=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{S}^{-1} & -\boldsymbol{S}^{-1} \boldsymbol{N} \boldsymbol{K}^{-1} \\
-\boldsymbol{K}^{-1} \boldsymbol{J} \boldsymbol{S}^{-1} & \boldsymbol{K}^{-1} + \boldsymbol{K}^{-1} \boldsymbol{J} \boldsymbol{S}^{-1} \boldsymbol{N} \boldsymbol{K}^{-1}
\end{bmatrix}
$${#eq-iv}
So from @eq-ll we have 
$$
\begin{bmatrix}
\hat{\boldsymbol{\tau}} \\
\hat{\boldsymbol{u}}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{y}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{y}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{U}_1\\
\boldsymbol{U}_2
\end{bmatrix}\boldsymbol{y}
$$
Here
$$\begin{aligned}
&\boldsymbol{U}_1 = \boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\\
&\boldsymbol{U}_2 = \boldsymbol{C}^{21}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{22}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}
\end{aligned}$$

From @eq-ll and @eq-asp, we have
$\hat{\boldsymbol{\tau}}\sim N(\boldsymbol{\tau},\boldsymbol{U}_1(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)\boldsymbol{U}_1^\top)$

And we have following results

$$\begin{aligned}
\begin{bmatrix}
\boldsymbol{U}_1 \\
\boldsymbol{U}_2
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X} & \boldsymbol{Z}
\end{bmatrix}
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X} & \boldsymbol{Z}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
(
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1}
\end{bmatrix}-
\begin{bmatrix}
0 & 0 \\
0 & \boldsymbol{G}^{-1}
\end{bmatrix})\\
&=
\boldsymbol{I}-
\begin{bmatrix}
0 & -\boldsymbol{C}^{12}\boldsymbol{G}^{-1} \\
0 & -\boldsymbol{C}^{22}\boldsymbol{G}^{-1}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{I} & -\boldsymbol{C}^{12}\boldsymbol{G}^{-1} \\
0 & \boldsymbol{I}-\boldsymbol{C}^{22}\boldsymbol{G}^{-1}
\end{bmatrix}
\end{aligned}$$

So we have
$$\begin{aligned}
\boldsymbol{U}_1\boldsymbol{X}&=\boldsymbol{I}\\
\boldsymbol{U}_1\boldsymbol{Z}&=-\boldsymbol{C}^{12}\boldsymbol{G}^{-1}\\
\end{aligned}$$
For the variance of estimation we have
$$\begin{aligned}
var(\hat{\boldsymbol{\tau}})
&=
\boldsymbol{U}_1(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)\boldsymbol{U}_1^\top\\
&= \boldsymbol{U}_1\boldsymbol{R}\boldsymbol{U}_1^\top+\boldsymbol{U}_1\boldsymbol{ZGZ}^\top\boldsymbol{U}_1^\top\\
&= (\boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{R}^{-1})\boldsymbol{R}\boldsymbol{U}_1^\top+\boldsymbol{U}_1\boldsymbol{ZGZ}^\top\boldsymbol{U}_1^\top\\
&=\boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{U}_1^\top+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{U}_1^\top+\boldsymbol{C}^{12}\boldsymbol{G}^{-1}\boldsymbol{G}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top\\
&=\boldsymbol{C}^{11}-\boldsymbol{C}^{12}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top+\boldsymbol{C}^{12}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top\\
&=\boldsymbol{C}^{11}
\end{aligned}$$

what is the relation between $\boldsymbol{C}_{11}$ and $\boldsymbol{C}^{11}$? From @eq-iv we have $\boldsymbol{C}^{11}=\boldsymbol{S}^{-1}$ and $\boldsymbol{S}$ is 
$$
\boldsymbol{S} = \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{X} - \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} (\boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1})^{-1} \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{X}
$$
And base on the complement of $\boldsymbol{C}_{11}$, we rewrite the $\boldsymbol{S}$

$$\begin{aligned}
\boldsymbol{S}
&=\boldsymbol{X}^\top(\boldsymbol{R}^{-1} - \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} (\boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1})^{-1} \boldsymbol{Z}^\top \boldsymbol{R}^{-1})\boldsymbol{X}\\
&=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}\\
&=\boldsymbol{C}_{11}
\end{aligned}$$

So $var(\hat{\boldsymbol{\tau}})=\boldsymbol{C}^{11}=\boldsymbol{C}_{11}^{-1}$, which means $(\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})\sim N(0,\boldsymbol{C}_{11}^{-1})$.
:::

:::


To examine a specific form of $\boldsymbol{\tau}$, in general case, we do linear transform
on $\boldsymbol{\tau}$:
$\hat{\boldsymbol{\pi}}=\boldsymbol{D}\hat{\boldsymbol{\tau}}$, where
$\boldsymbol{D}$ is some transform matrix, so we have
$\boldsymbol{D}(\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})=\boldsymbol{\pi}-\hat{\boldsymbol{\pi}}\sim N(0,\boldsymbol{D}\boldsymbol{C}_{11}^{-}\boldsymbol{D}^\top)$.
We denote
$\boldsymbol{\Lambda}=\boldsymbol{D}\boldsymbol{C}_{11}^{-}\boldsymbol{D}^\top$. If $\boldsymbol{D}$ is identical matrix $\boldsymbol{I}$, then $\boldsymbol{\Lambda}=\boldsymbol{C}_{11}^{-1}$ and $\hat{\boldsymbol{\pi}}=\hat{\boldsymbol{\tau}}$.

We know that A-criterion is the mean of predicted error variance of the parameter from @eq-a 
i.e. 
$$
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}\sum_{i}\sum_{j<i}V_{ij}
$$ 
Having variance-covariance matrix $\boldsymbol{\Lambda}=\boldsymbol{C}_{11}^{-1}$, we can rewrite
the sum part as
$n_{\tau}tr(\boldsymbol{\Lambda})-\mathbb{1}_{n_{\tau}}^\top\boldsymbol{\Lambda}\mathbb{1}_{n_{\tau}}$.So
we have
$$
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}[n_{\tau}tr(\boldsymbol{\Lambda})-\mathcal{1}_{n_{\tau}}^\top\boldsymbol{\Lambda}\mathcal{1}_{n_{\tau}}]
$${#eq-an}
same result from @butler2013model

Derivation above indicate that
$\mathcal{A}\propto tr(\boldsymbol{\Lambda})$, A-criterion as the mean
of predicted error variance of the parameter, we prefer it as small as
possible to obtain a accurate result from experiment, which means the
trace of virance-covirance matrix $\boldsymbol{\Lambda}$ should be as
small as possible. And this is our goal on optimal experimental design.



## Neighbor balance and eveness of distribution

### Concepts of NB and ED

@piepho2018neighbor emphasis the the concepts of neighbour balance and
even distribution are crucial to mitigating biases and ensuring the
reliability of results in row-column design.

Neighbour balance (NB) refers to the principle that, in a row-column
experimental design, the frequency with which two treatments are
adjacent or near each other should not be excessively high. High
adjacency frequency between two treatments can lead to mutual influence,
which may cause bias to the experimental results. For example, if the
effect of one treatment can spread to neighbouring areas, frequent
adjacency could interfere with accurate measurement of each treatment's
true effect next to it. Therefore, it is essential to control the
adjacency frequency of different treatments to prevent high adjacency
for two specific treatments.

Even distribution (ED) aims to ensure that different replications of the
same treatment are widely distributed across the experimental field,
rather than being clustered in a specific area. This strategy helps to
avoid biases caused by specific environmental conditions in certain
parts of the experiment field. If replications of one treatment are over
concentrated in one area, unique environmental factors in that area
might affect the treatment's performance, leading to biased
observations. By evenly distributing replications, environmental
interference can be minimized, so that we can enhance the reliability of
the experimental results.

See @fig-NBED for instances where bad NB and ED occur.

### Measuring NB and ED

#### Evaluating NB with adjacency matrix

In @piepho2018neighbor, there is a assumption that they are optimizing a
binary design, which means each treatment appears only once in each row
and column. Under this assumption, their balancing mechanism considers
diagonal adjacency, Knight moves, and even more distant points to ensure
an optimal balance. In my optimization process, I begin with a randomly
selected design matrix. Consequently, my approach considers not only
diagonal adjacency but also the adjacent points directly above, below,
to the left, and to the right.

We use an adjacency matrix to count the number of times each treatment is
adjacent to another. This matrix serves as a crucial tool in my
optimization process, enabling precise tracking and adjustment of
treatment placements to achieve neighbour balance.

We denote the adjacency matrix as $\mathcal{U}$, and for treatment
$i$ and $j$ in treatment set $T$, the symbol $\mathcal{U}_{ij}$ represents the
count of times treatment $i$ is adjacent to treatment $j$. Here
"adjacent" means treatment $j$ is located next to treatment $i$

For Given design $\mathcal{D}$ and $\mathcal{D}_{r,c}$ represents the
treatment at row $r$ and column $c$. So $\mathcal{U}_{ij}$ can be
expressed as:

$$
\mathcal{U}_{ij}=\sum_{r=1}^{n_r}\sum_{c=1}^{n_c}I_{r,c}(i) F_{r,c}(j)
$$
where
$$
F_{r,c}(j)=
\sum_{m \in \{-1,0,1\}}\sum_{n \in \{-1,1\}}I_{r+m,c+n}(j)+\sum_{m \in \{-1,1\}}I_{r+m,c}(j)
$$
$n_r$ and $n_c$ are total number of rows and columns and $I_{r,c}(\cdot)$ is
the indicator function, which takes value under following cases
$$
I_{r,c}(i)=
\begin{cases}
1 & \text{if } \mathcal{D}_{r,c}=i \\
0 & \text{if } \mathcal{D}_{r,c}\neq i \; \text{or } r<1,r>n_r,c<1,c>n_c\\
\end{cases}
$$

The function $F_{r,c}(j)$ here is actually counting the
times that treatment $j$ occurs at places around the position row
$r$ and column $c$.

We measure NB by taking difference of the maximum and minimum of the elements in adjacency matrix
$\mathcal{U}$. Our NB criteria is
$$
C_{NB}=\max\{\mathcal{U}_{ij}\}-\min\{\mathcal{U}_{ij}\}  \quad i,j\in T
$$

#### Evaluating ED with minimum row and column span
The goal of evaluating the evenness of distribution (ED) is to find the row and column spans for treatments across the entire design matrix. We would like this value as large as possible This ensures that the treatments $t\in T$ is distributed as evenly as possible within the rows and columns, reducing clustering and promoting a balanced design.

The row span for a given treatment $t \in T$ is defined as the difference between the maximum and minimum row indices where t appears in experiment field.
$$
RS(t)=\max\{r:\mathcal{D}_{r,c}=t\}-\min\{r:\mathcal{D}_{r,c}=t\} \quad 1<r<n_r,\quad 1<c<n_c
$$
And the minimum row span of a design $\mathcal{D}$ is 
$$
MRS(\mathcal{D})=\min\{RS(t)\},\quad t \in T
$$
Same for column span
$$
CS(t)=\max\{c:\mathcal{D}_{r,c}=t\}-\min\{c:\mathcal{D}_{r,c}=t\} \quad 1<r<n_r,\quad 1<c<n_c
$$
$$
MCS(\mathcal{D})=\min\{CS(t)\},\quad t \in T
$$
So, for the changes in the design matrix $\mathcal{D}$ during the search process, we tend to accept only those changes where the Minimum Row Span ($MRS$) and Minimum Column Span ($MCS$) remain the same or become smaller.


