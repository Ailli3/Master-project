# Background {#sec-bg}


## Linear model 

Suppose we have a linear model,

$$\boldsymbol{y}=\mathbf{X}\boldsymbol{\tau} + \boldsymbol{\epsilon}$$ {#eq-lm}
where $\boldsymbol{y}$ is $n\times 1$ vector of $n$ observations, $\boldsymbol{\tau}$ is  a $t\times 1$ vector of fixed effects, $\boldsymbol{\epsilon}$ is the $n\times 1$  vector for error, and $\mathbf{X}$ is a design matrix has size $n\times t$. We assume that  $\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2\textbf{I}_{n})$ and hence $\boldsymbol{y} \sim N(\mathbf{X}\boldsymbol{\tau}, \sigma^2\mathbf{I}_n)$.

The log-likelihood of @eq-lm is then given as: 

$$
\log\ell(\boldsymbol{\tau};\boldsymbol{y}) = -\frac{n}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2\sigma^2}(\boldsymbol{y}-\textbf{X}\boldsymbol{\tau})^\top(\boldsymbol{y}-\textbf{X}\boldsymbol{\tau}).
$$
The $(i,j)$-th entry of the Fisher information matrix is defined as 

$$
I_{ij}(\boldsymbol{\tau})=-\mathbb{E}\left(\frac{\partial^2}{\partial\tau_i\partial\tau_j}\log\ell(\boldsymbol{\tau};\boldsymbol{y})\right)
$$
where $\tau_i$ is the $i$-th entry of $\boldsymbol{\tau}$.


::: {#lem-fim-lm}

The Fisher information matrix of @eq-lm is given as
$$
\mathbf{C} = \frac{\partial^2}{\partial\boldsymbol{\tau}\partial\boldsymbol{\tau}^\top}\log\ell(\boldsymbol{\tau};\boldsymbol{y})=-\frac{1}{\sigma^2}\textbf{X}^\top\textbf{X}
$$
:::

::: proof

Where is the proof?

:::


::: {#lem-lm-var}

The variance of the fixed effects for @eq-lm is equivalent to the inverse of the Fisher information matrix, i.e. $var(\hat{\boldsymbol{\tau}})=\sigma^2(\textbf{X}^\top\textbf{X})^{-1} = \mathbf{C}^{-1}.$

:::

::: proof

Add proof

:::


# Linear mixed model


Linear mixed model extends linear model by incorporating additionally incorporating random effects into the model that effectively give greater flexibility and capability to incorporate known correlated structures into the model. We now consider a linear mixed model
$$
\boldsymbol{y}=\textbf{X}\boldsymbol{\tau}+\textbf{Z}\boldsymbol{u}+\boldsymbol{\epsilon}
$$ {#eq-lmm}
here $\boldsymbol{y}$ is $n\times 1$ vector for $n$ observations, $\boldsymbol{\tau}$ is a $t\times1$ parameter vector of fixed effects, $\boldsymbol{u}$ is a $q \times1$ parameter vector of random effects, and $\boldsymbol{\epsilon}$ is the $n\times 1$ error vector, $\textbf{X}$ and $\textbf{Z}$ are design matrices of dimension  $n \times t$ and $n \times q$ for fixed effects and random effects, respectively. We assume for random effects and error vector $\epsilon$
$$
\begin{bmatrix}
\boldsymbol{u} \\
\boldsymbol{\epsilon} 
\end{bmatrix}
\sim
N\left(
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol{0}
\end{bmatrix}
,
\begin{bmatrix}
\textbf{G} & \mathbf{0} \\
\mathbf{0} & \textbf{R}
\end{bmatrix}
\right),
$$
where $\textbf{G}$ is the $q \times q$ variance matrix for $\boldsymbol{u}$ and $\textbf{R}$ is $n\times n$ variance matrix for $\boldsymbol{\epsilon}$.

##  A-criterion

We first start with a simple example, that is, we are interesting in examine fixed effect part $\boldsymbol{\tau}$, to elucidate the influence of A-criterion. 

From @butler2013optimal, we conduct a maximum log likelihood by following objective function:
$$
\log f_Y(\boldsymbol{y}|\boldsymbol{u};\boldsymbol{\tau},\textbf{R})+\log f_u(\boldsymbol{u};\textbf{G})
$$
basing on assumption, we have $\boldsymbol{y}=\textbf{X}\boldsymbol{\tau}+\textbf{Z}\boldsymbol{u}+\boldsymbol{\epsilon}\sim N(\textbf{X}\boldsymbol{\tau},\textbf{R}+\textbf{ZGZ}^\top)$. So for objective function, we can write out the distributions 

$$
\boldsymbol{y}|\boldsymbol{u};\boldsymbol{\tau},\textbf{R}\sim N(\textbf{X}\boldsymbol{\tau}+\textbf{Z}\boldsymbol{u},\textbf{R}) \\
\boldsymbol{u}\sim N(\boldsymbol{0},\textbf{G})
$$

::: {#lem-joint-density-lmm}

So log of joint density is given as

\begin{align*}
\mathscr{L}&=\log f_Y(\boldsymbol{y}|\boldsymbol{u};\boldsymbol{\tau},\textbf{R})+\log f_u(\boldsymbol{u};\textbf{G})\\
&=-\frac{1}{2}\left(\log|\textbf{R}|+\log|\textbf{G}|+(\boldsymbol{y}-\textbf{X}\boldsymbol{\tau}-\textbf{Z}\boldsymbol{u})^\top \mathbf{R}^{-1}(\boldsymbol{y}-\textbf{X}\boldsymbol{\tau}-\textbf{Z}\boldsymbol{u})+\boldsymbol{u}^\top\textbf{G}^{-1}\boldsymbol{u}\right)
\end{align*}

:::

::: proof

Where is the proof?

:::


We determine that $\frac{\partial\mathscr{L}}{\partial\boldsymbol{\tau}}=\frac{\partial\mathscr{L}}{\partial\boldsymbol{u}}=\boldsymbol{0}$, and write the equation into a matrix form
$$
\begin{bmatrix}
\textbf{X}^\top\textbf{R}^{-1}\textbf{X} & \textbf{X}^\top\textbf{R}^{-1}\textbf{Z}\\
\textbf{Z}^\top\textbf{R}^{-1}\textbf{X} & \textbf{Z}^\top\textbf{R}^{-1}\textbf{Z}+ \textbf{G}^{-1}
\end{bmatrix}
\begin{bmatrix}
\hat{\boldsymbol{\tau}}\\
\hat{\boldsymbol{u}}
\end{bmatrix}=
\begin{bmatrix}
\textbf{X}^\top\textbf{R}^{-1}\boldsymbol{y}\\
\textbf{Z}^\top\textbf{R}^{-1}\boldsymbol{y}
\end{bmatrix}$$

Let
$$
\textbf{C}=
\begin{bmatrix}
\textbf{X}^\top\textbf{R}^{-1}\textbf{X} & \textbf{X}^\top\textbf{R}^{-1}\textbf{Z}\\
\textbf{Z}^\top\textbf{R}^{-1}\textbf{X} & \textbf{Z}^\top\textbf{R}^{-1}\textbf{Z}+ \textbf{G}^{-1}
\end{bmatrix} 
\quad
\hat{\boldsymbol{\beta}}=\begin{bmatrix}
\hat{\boldsymbol{\tau}}\\
\hat{\boldsymbol{u}}
\end{bmatrix}
\quad
\textbf{W}=\begin{bmatrix}\textbf{X} &\textbf{Z}\end{bmatrix}
$$

We can rewrite the equation in a standard form $\textbf{C}\hat\beta=\textbf{W}^T\textbf{R}^{-1}y$. As we mentioned above, we are interesting in examine fixed effect part $\tau$ so we need a standard form only for $\tau$, we do this by cancelling $\mu$, We have
$$
\boldsymbol{X}^T\textbf{R}^{-1}\textbf{X}\tau+\textbf{X}^T\textbf{R}^{-1}\textbf{Z}(\textbf{Z}^T\textbf{R}^{-1}\textbf{Z}+\textbf{G}^{-1})\textbf{Z}^T\textbf{R}^{-1}y=\textbf{X}^T\textbf{R}^{-1}y\\
$$
$$
\Rightarrow \textbf{X}^{T}[\textbf{R}^{-1}-\textbf{R}^{-1}\textbf{Z}(\textbf{Z}^T\textbf{R}^{-1}\textbf{Z}+\textbf{G}^{-1})^{-1}\textbf{Z}^{T}\textbf{R}^{-1}]\textbf{X}\tau=\textbf{X}^{T}[\textbf{R}^{-1}-\textbf{R}^{-1}\textbf{Z}(\textbf{Z}^T\textbf{R}^{-1}\textbf{Z}+\textbf{G}^{-1})^{-1}\textbf{Z}^{T}\textbf{R}^{-1}]y\\
$$
$$
\Rightarrow \textbf{X}^{T}\textbf{P}\textbf{X}\tau=\textbf{X}^{T}\textbf{P}y
$$
where $P=\textbf{R}^{-1}-\textbf{R}^{-1}\textbf{Z}(\textbf{Z}^T\textbf{R}^{-1}\textbf{Z}+\textbf{G}^{-1})^{-1}\textbf{Z}^{T}\textbf{R}^{-1}$, let $\textbf{C}_{11}=\textbf{X}^{T}\textbf{P}\textbf{X}$ then we have the standard form for $\hat\tau$, which is $\textbf{C}_{11}\tau=\textbf{X}^T\textbf{P}y$, and $\textbf{C}_{11}$ is the corresponding Fisher information matrix.

So we have $\tau-\hat\tau\sim N(0,\textbf{C}_{11}^{-1})$.To examine a specific form of $\tau$, we do linear transform on $\tau$: $\hat\pi=\textbf{D}\hat\tau$, where $\textbf{D}$ is some transform matrix, so we have $\textbf{D}(\tau-\hat\tau)=\pi-\hat\pi\sim N(0,\textbf{D}\textbf{C}_{11}^{-}\textbf{D}^{T})$. We denote $\boldsymbol{\Lambda}=\textbf{D}\textbf{C}_{11}^{-}\textbf{D}^{T}$.

A-criterion is the mean of predicted error variance of the parameter. i.e. 
$$
\mathscr{A}=\frac{1}{n_{\pi}(n_{\pi}-1)}\sum_{i}\sum_{j<i} predicted\quad error\quad variance\quad of\quad(\hat\pi_i-\hat\pi_j)
$$
where $n_{\pi]$ is the row number of vector$\pi$.For error variance part, we have 
$$
\sum_{i}\sum_{j<i} predicted\quad error\quad variance\quad of\quad(\hat\pi_i-\hat\pi_j)=\sum_{i}\sum_{j<i}var(\hat\pi_i-\hat\pi_j)=\sum_{i}\sum_{j<i}[var(\hat\pi_i)+var(\hat\pi_j)-2cov(\hat\pi_i,\hat\pi_j)]
$$
from virance-covirance matrix $\boldsymbol{\Lambda}$, we can rewrite the sum part as $n_{\pi}tr(\boldsymbol{\Lambda})-\mathbb{1}_{n_{\pi}}^{T}\boldsymbol{\Lambda}\mathbb{1}_{n_{\pi}}$.So we have
$$
\mathscr{A}=\frac{1}{n_{\pi}(n_{\pi}-1)}[n_{\pi}tr(\boldsymbol{\Lambda})-\mathbb{1}_{n_{\pi}}^{T}\boldsymbol{\Lambda}\mathbb{1}_{n_{\pi}}]
$$
same result from [@butler2013model]

Derivation above indicate that $\mathscr{A}\propto tr(\boldsymbol{\Lambda})$, A-criterion as the  mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix $\boldsymbol{\Lambda}$ should be as small as possible. And this is our goal on optimal experimental design.
