[
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Modeling row-column design\nAs mentioned in previous chapter, we use a linear mixed model (LMM) to model the row-column design having two distinct sources of variation, typically referred to as “row” and “column” factors. This design structure appears frequently in agricultural and industrial trials, where treatments are applied across units organized in a grid-like pattern, and both row and column effects may influence the outcomes.\nIn my assumptions, the row and column effects are treated as random effects, which means that they are random factors for spatial or systematic factors across different rows and columns of the experiment field. The treatment effects, on the other hand, are treated as fixed effects because they represent the primary factors of interest that we wish to evaluate in terms of their influence on the response variable.\nRecalling Equation 2.2, the treatment effects are modeled as fixed effects, represented by the treatment design matrix \\boldsymbol{X} with parameter vector \\boldsymbol{\\tau}, measuring the influence of each treatment on the response variable. The matrix \\boldsymbol{X} is constructed such that each row corresponds to an experimental unit, and indicators in each column indicates whether a treatment is applied or not.\nThe random effects are modeled through the matrix \\boldsymbol{Z} and parameter vector \\boldsymbol{u}. Matrix \\boldsymbol{Z} is designed to capture the row and column structure of the experimental field, in which entries represent the position of each experimental unit located in some specific rows and columns. The parameters in vector \\boldsymbol{u} are corresponding row and column effects. They are assumed to follow a normal distribution with mean zero and variance-covariance matrix \\boldsymbol{G}.\nWith \\boldsymbol{y} as the vector of observed responses and \\boldsymbol{\\epsilon} as error term, a row-column design can be modeled by Equation 2.2 . where \\boldsymbol{X\\tau} represents the fixed treatment effects and \\boldsymbol{Zu} captures the random variations basing on rows and columns.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#modeling-row-column-design",
    "href": "methods.html#modeling-row-column-design",
    "title": "3  Methods",
    "section": "",
    "text": "3.1.1 Random effects matrix\nThe design matrix for the random effects, which is row and column in my linear mixed model follows a binary indicator structure. For example, we a have a 4\\times 4 experiment field, having 4 rows, 4 columns and 16 units. Then random effects matrix should be a 16 \\times 8 matrix containing binary indicator as the one presented. \n\\begin{bmatrix}\n\\begin{array}{cccc|cccc}\n1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\\\\n\\end{array}\n\\end{bmatrix}\n Each row corresponds to a specific experimental unit, while the columns represent the row and column factors in the experimental layout.In this matrix, the first set of columns represents the column effects, while the second set of columns represents the row effects. Each entry in this matrix is binary, where a value of 1 indicates that the experimental unit belongs to a specific row or column, and a 0 indicates otherwise. For example, the first row of the matrix has a 1 in both first and fifth columns, meaning that the corresponding unit of it is in the first column and the first row. This structure ensures that each unit is uniquely associated with one row and one column, and we can model the random effects accordingly.\nIn a more general case, suppose we have a m\\times n row-column experiment field. We should have a random effect matrix with mn rows and m+n columns with binary numbers. In this paper, we assume that the row effects and column effects are independent with each other. However, in more complex experimental design cases, they may be potentially correlated. The design of the random effects matrix, which separates the row and column effects as independent variables, simplifies the modeling process and the analysis of potential correlations between these effects in more advanced settings. This structure allows for easier identification and analysis of interactions between row and column effects, making the model flexible and adaptable to different levels of complexity in experimental designs.\n\n\n3.1.2 Design matrix for treatments\nThe design matrix for the treatment effects is constructed to capture the influence of each treatment on the response variable. In a row-column experimental design, each experimental unit is assigned a specific treatment.The entries in design matrix represents these assignments using binary indicators. Like random effects matrix each row in the matrix corresponds to an experimental unit, while each column represents a different treatment. Here we still use 4\\times 4 experiment field as example, and suppose we have 4 different treatments for each have 4 replications, needing 16 experiment unite. An example design matrix \\boldsymbol{X} for treatments should be a 16\\times 4 matrix with binary indicators as shown below\n\n\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n\\end{bmatrix}\n For a given experimental unit, that is a given row, the design matrix contains a 1 in the column corresponding to the treatment applied to that unit, and 0 elsewhere.This structure allows for a clear and efficient representation of which treatment is applied to each unit. For example, the first row of the example design matrix represents the first treatment is applied in the unit locating on the first column, first row.\nif there are t treatments and N experimental units, the design matrix will have N rows and t columns. Then the design matrix for treatment \\boldsymbol{X} with size N\\times t, should satisfy that for any row n_{i} \n\\sum_{j=1}^{t} \\boldsymbol{X}_{n_{i},j}=1\n That is, there is only one treatment can be applied in each experiment unit. And for any treatment t_j with r_j replications, it has \n\\sum_{i=1}^{N}\\boldsymbol{X}_{i,t_j}=r_j\n All replications of a treatment are applied in experimental field.\n\n\n3.1.3 Assumptions for A-value calculation\nIt is important to clarify the key assumptions made in this study before calculating A value for a row-column design. Recalling Equation 2.7, for calculating A value we need the covariance matrix for the random effects, matrix \\boldsymbol{G}, covariance matrix for the error term, matrix \\boldsymbol{R}, transformation matrix \\boldsymbol{D}, random effect matrix \\boldsymbol{Z} and treatment design matrix \\boldsymbol{X}. We need some basic setup for these matrices.\nAssume that we now have a row-column matrix with n_r rows, n_r columns and n_rn_c plots.\nFor the covariance matrix for the random effects, matrix \\boldsymbol{G}, which captures the variability introduced by the row and column effects. I assume it is a (R+C)\\times(R+C) diagonal matrix, that is, it has following form,\n\n\\boldsymbol{G}_{diag} =\n\\begin{bmatrix}\n\\sigma_{G_c}^2\\boldsymbol{I}_{n_c} & 0 \\\\\n0 & \\sigma_{G_r}^2\\boldsymbol{I}_{n_r}\n\\end{bmatrix}\n \\boldsymbol{I}_{n_c} is a {n_c}\\times{n_c} identity matrix, same to \\boldsymbol{I}_{n_r}. And \\sigma_{G_c} and \\sigma_{G_r} is a scale constant. This means that the influences of the rows and columns are independent of each other, meaning there is no correlation between different row and column effects in the design.\nSimilarly, for the covariance matrix for the error term, matrix \\boldsymbol{R}, I assume that it is also diagonal, indicating that the residual errors are uncorrelated across different experimental units. In this case we have \n\\boldsymbol{R}_{diag} = \\sigma_{R}^2\\boldsymbol{I}_{n_rn_c}\n with identity matrix \\boldsymbol{I}_{n_rn_c} and scale constant \\sigma_{R}.\nButler (2013) have introduced linear transformations of the treatment parameter vector \\boldsymbol{\\tau} by using a transformation matrix \\boldsymbol{D}, which allows for the investigation of linear combinations of treatments. However, in this paper, I simplify the approach by setting \\boldsymbol{D} as the identity matrix \\boldsymbol{I}_{RC}. This means that we focus on the individual effects of the treatments rather than their linear combinations.\nThese assumptions makes the structure of the model becomes more straightforward, allowing us to concentrate on the direct estimation of treatment effects while maintaining independence among the random effects and error terms.\nIt’s important to note that in our design, the random effect matrix \\boldsymbol{Z} remains constant during the optimization process.This means that while the row and column effects are accounted for as random effects, their structure does not change.\nWith these assumptions in place, the A-value in the context of a row-column design dependents only on the treatment design matrix \\boldsymbol{X}. This means that the primary factor influencing the A-value is the distribution and arrangement of treatments within the design, and it directly impacts the variance of the treatment effect estimates. Therefore, optimizing the A-value under these assumptions becomes a problem of optimizing the treatment distribution in the design, ensuring that the treatments are arranged in such a way that the variance of the estimates is minimized.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#searching-strategy",
    "href": "methods.html#searching-strategy",
    "title": "3  Methods",
    "section": "3.2 Searching Strategy",
    "text": "3.2 Searching Strategy\nBefore introducing the details of the searching strategy, it is important to establish a solid foundation by proving that the minimum of the A-value exists. The existence of the minimum A-value implies that the A-value has a lower bound, ensuring that the process of iteration optimizing the experimental design is not endless. As we continue to search for smaller A-values, this guarantees that we can eventually stop when the A-value stabilizes or a sufficient number of iterations reached.\nThis allows us to conclude that we have found an optimal or near-optimal design. Therefore, the existence of this lower bound serves as a critical foundation for our iterative search, giving us confidence that the optimization will converge to a solution.\n\n3.2.1 Existence of the minimum of A-value\nTo prove that the minimum of the A-value exists, we establish a objective function, that is, the A-value function A(\\boldsymbol{X}) maps design matrix for treatment \\boldsymbol{X} to its A-value. It is a function that maps the design space to \\mathbb{R}. \nA: \\Omega \\to \\mathbb{R}, \\quad \\boldsymbol{X} \\mapsto A(\\boldsymbol{X})\n Here \\Omega is the design space contains all possible design matrix \\boldsymbol{X}.\nNow we proof the existence of minimum value of \\boldsymbol{X}, where \\boldsymbol{X}\\in\\Omega.\n\nProof. Recalling Equation 2.7\n\n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}[n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}]\n This expression is well-defined for all valid design matrices \\boldsymbol{X}. So function A(\\boldsymbol{X}) is well-defined.\nThe A-value represents the average variance of the difference treatment effect estimates, and since variances are always positive, the A-value is naturally bounded from below by zero. So we have \nA(\\boldsymbol{X})\\geq0\n which implies that the A-criterion is bounded below.\nIn experimental design, the treatment design matrix \\boldsymbol{X} can take on a finite number of possible permutations, especially in practical row-column designs where the number of treatments and experimental units is fixed. In a finite search space, a lower bounded function has its minimum value.\n\n\n\n3.2.2 General structure\nIn Piepho, Michel, and Williams (2018), the optimization of NB and ED was typically carried out under the assumption that the A-value was already optimal or fixed. This required identifying a set of solutions that maintained the A-value while improving the balance and distribution properties. In addition to assuming the A-value is fixed, another approach they used is to randomly select a design and then optimizing ED and NB.This process would be repeated multiple times, and the design with the best A-value will be selected.\n\n\n\n\n\n\nMethod been used\n\n\n\n\nFigure 3.1\n\n\n\nThese approach separates the optimization of ED and NB from the A-value, while I try to merge these two process into one algorithm. I use pairwise permutation among treatments to change treatment design during iterations. And to avoid design with bad ED and NB, I am using some criteria to filter the permutation, only maintain or better properties are accepted. In this way, a row-column design that satisfies multiple optimization requirements is achieved.\n\n\n\n\n\n\nAttempting method\n\n\n\n\nFigure 3.2\n\n\n\nBasing on optimal design search methods share a common set of features in exploring the design space given in Butler (2013), my searching method contains following part:\n\nA calculation method for an optimal criterion for a given design matrix \\boldsymbol{X}\nAn interchange policy to switch the design with in search space \\Omega\nAn acceptance policy for a new design.\nA stopping rule to terminate the search.\n\nThe criterion calculation part has already been discussed earlier. We will now introduce interchange policy of switching the design.\n\n\n3.2.3 Permutations and filtering\nWe use permutations of the treatments to update the design matrix for treatment. We randomly select two different treatments and swap them within the design matrix during the permutation process, without making drastic changes.\nLet \\boldsymbol{X} be the current design matrix for treatments, where each row corresponds to an experimental unit, and each column represents a treatment. Suppose we randomly select two different treatments, t_i and t_j located in ith row and jth row respectively. \\boldsymbol{X}_{new} can be written as \n\\boldsymbol{X}_{new} = \\boldsymbol{P}_{ij}\\boldsymbol{X}\n with permutation matrix defined as \n\\boldsymbol{P}_{i j}=\\left[\n\\begin{array}{cccccccc}\n1 & 0 & \\cdots & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 & \\cdots &0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots & \\cdots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & \\cdots & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n0 & 0 & \\cdots & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots & & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & \\cdots & 0 & \\cdots & 1\n\\end{array}\\right]\n It is a identity matrix with ith row and jth row swapped.\nWhen performing permutations on the design matrix, I apply checks based on the metrics of evenness of distribution (ED) and neighbour balance (NB). The goal is to ensure that only the permutations which improve or at least maintain desirable values for ED and NB are accepted, while others are filtered out.\nA random permutation of treatments is generated by swapping two different treatments in the design matrix, as described earlier using a permutation matrix. Once the new design matrix \\boldsymbol{X}_{new} with corresponding design \\mathcal{D}_{new} is created, the next step is to evaluate the quality of the permutation by calculating the ED and NB values for the new configuration. As afore-mentioned, we have NB and ED criteria for \\boldsymbol{X}_{new}, that is C_{NB}', MRS(\\mathcal{D}_{new}) and MCS(\\mathcal{D}_{new}). Comparing the newly generated design matrix (offspring) with the original design matrix (parent), We accept the new permutation, if the ED and NB values improves or maintains them without significantly worse. In practice, we often set a tolerance for the ED and NB values. We generally allow ED and NB to become slightly worse, as it’s not necessary for them to strictly improve or remain unchanged in every iteration. Our goal is to achieve a balance between ED, NB, and the A-value. For instance, ED might increase slightly while NB decreases a little, as long as the overall balance between the three objectives is maintained. This approach has the added benefit of lowering the acceptance threshold for permutations, which speeds up the algorithm during the random selection process. For instance, we set tolerance for ED and NB as T_{ED} and T_{NB}, which are none negative numbers. Current design matrix \\boldsymbol{X} corresponding design \\mathcal{D} has ED and NB valueC_{NB}, MRS(\\mathcal{D}) and MCS(\\mathcal{D}). New design matrix \\boldsymbol{X}_{new} mentioned above is accepted when \n\\begin{align*}\n&(C_{NB}'\\leq C_{NB}+T_{NB}) \\\\\n\\land & (MRS(\\mathcal{D}_{new})\\geq MRS(\\mathcal{D})-T_{ED})\\\\\n\\land & (MCS(\\mathcal{D}_{new})\\geq MCS(\\mathcal{D})-T_{ED})\n\\end{align*}\n\\tag{3.1} is true.\n\n\n3.2.4 Random search\nThe random search algorithm begins with a randomly selected design matrix. To ensure that the search is efficient and avoids getting trapped in local optima, we introduce the concept of step length. This parameter determines how many permutations we consider in each iteration. Given the computational constraints, especially when the number of treatments or rows and columns increases, it is impractical to check all possible permutations of a design and evaluate each for acceptance. However, we aim to explore as many permutations as possible to avoid falling into local optima.\nAt each iteration, we randomly generate a set of permutations. For each generated permutation, we apply the filtering step check by Equation 3.1. If the permutation is not accepted, we randomly select another one and repeat the process. This continues until we have successfully selected a number of permutations equal to the step length.\nWe denote step length as s. tolerance for ED and NB as t_{ED} and t_{NB}. For a design matrix \\boldsymbol{X}, its ED criteria - minimum row span and minimum column span is, mrs({\\boldsymbol{X}}) and mcs({\\boldsymbol{X}}), and NB criteria is denote as C_{NB}({\\boldsymbol{X}}).\n\n\nInput: Original design matrix \\boldsymbol{X}, step length s, tolerance for ED and NB as t_{ED} and t_{NB}.\nInitialize: k = 1, ED and NB value for \\boldsymbol{X} as mrs({\\boldsymbol{X}}), mcs({\\boldsymbol{X}}) and C_{NB}({\\boldsymbol{X}}).\nWhile k &lt; s:\n\nGenerate a random permutation matrix \\boldsymbol{P} by selecting two different treatments.\nApply permutation: \\boldsymbol{X_{new} = X P}.\nCalculate new values of ED and NB, mrs({\\boldsymbol{X}_{new}}), mcs({\\boldsymbol{X}_{new}}) and C_{NB}({\\boldsymbol{X}_{new}}).\nIf new values satisfy mrs({\\boldsymbol{X}_{new}}) &gt;= mrs({\\boldsymbol{X}}) - t_{ED}, mcs({\\boldsymbol{X}_{new}}) &gt;= mcs({\\boldsymbol{X}}) - t_{ED} and C_{NB}({\\boldsymbol{X}_{new}}) &lt;= C_{NB}({\\boldsymbol{X}}) + t_{NB}, accept and remember this \\boldsymbol{X}_{new}.\nIncrement k.\nIf k = s, output all selected permutations.\n\nOutput:A set of k permutations of original design matrix \\boldsymbol{X}\n\n\nThe Random Search algorithm starts with a randomly selected initial design matrix \\boldsymbol{X}_0, and we aim to minimize the A-value associated with the design. We set a maximum number of iterations M and a step length s.\nThe process for each iteration can be described as follows:\nWe denote a random design matrix \\boldsymbol{X}_0, and iteration counter k. Maximum number of iteration is M. And denote the A-value for a random design matrix \\boldsymbol{X} is A(\\boldsymbol{X}). Current design matrix during the iteration we have is denoted as \\boldsymbol{X}_c\n\nInitialization: Start with a random design matrix \\boldsymbol{X}_0, set the iteration counter `k = 0, maximum number of iteration as M and step-length s. A-value for the current design is A_c = A(\\boldsymbol{X}_0).\nIteration: For each iteration k, where k &lt; M:\n\nGenerate s random permutations \\{\\boldsymbol{X}_i\\}, i=1,2,\\cdots,s of the current design matrix X_c.\nCalculate the A-value A(\\boldsymbol{X}_i)for each permutation \\boldsymbol{X}_i.\nCompare the A-values for all s permutations with the current design X_c.\nSelect the permutation with the smallest A-value, \\boldsymbol{X}_j, among the s candidates.\nIf A(\\boldsymbol{X}_j) &lt; A(\\boldsymbol{X}_c) update the current design matrix: \\boldsymbol{X}_c = \\boldsymbol{X}_j; otherwise, retain the current design.\n\nTermination: Repeat this process until the maximum number of iterations M is reached.\n\n\n\n3.2.5 Simulated annealing\nSimulated Annealing (SA) is a global optimization algorithm inspired by the annealing process in metallurgy. At higher temperatures, the algorithm allows the acceptance of worse solutions to escape local optima; as the temperature decreases, it converges toward an optimal solution.\nButler (2013) state that the Boltzmann probability \nP(E)\\propto e^{[-E/kt]}\n offer a pathway to measuring the accept possibility during the algorithm, where E is the energy of the state,for a given temperature t. The constant k is Boltzmann’s constant. The energy E corresponds to the value of the objective function, in our case is A-value. During the iterative process, suppose we have a design (state) having A-value (energy) A_1 at time t, and we are shifting our design into a new design with A-value A_2, resulting in an energy change A_{\\bigtriangleup} = A_2 - A_1. If A_{\\bigtriangleup} is negative, that is A_2 &lt; A_1, we always accept new design since we have lower A-value. If A_{\\bigtriangleup} is positive, acceptance follows the Metropolis criterion: a random number, \\delta \\in [0,1], is generated, and A_2 is accepted if \\delta \\leq exp(-A_{\\bigtriangleup}/kt). So we have acceptance rate P(A_2) for a new A-value A_2. \nP(A_2)=\n\\begin{cases}\n1 & \\text{if } A_2&lt;A_1 \\\\\n\\exp(-A_{\\bigtriangleup}/kt) & \\text{if } A_2&gt;A_1\\\\\n\\end{cases}\n\\tag{3.2} The basic elements of SA given by Bertsimas and Tsitsiklis (1993) is as followed\n\nA finite set S.\nA real-valued cost function J defined on S. Let S^*\\subset S be the set of global minima of the function J, assumed to be a proper subset of S.\nFor each i\\in S, a set S(i) \\subset S - {i}, called the set of neighbours of i.\nFor every i, a collection of positive coefficients q_{ij}, j\\in S(i), such that \\sum_{j\\in S(i)} q_{ij} = 1. It is assumed that j\\in S(i), if i \\in S(j).\nA nonincreasing function T:N\\rightarrow (0, \\infty), called the cooling schedule. Here N is the set of positive integers, and T(t) is called the temperature at time t.\nAn initial “state” x(0)\\in S.\n\nWe are applying these elements to a SA that fits our case. Our SA having following elements.\n\nA finite searching space \\{\\boldsymbol{X}\\} consisting all design matrix.\nAn A-value function A(\\boldsymbol{X}) defined on \\{\\boldsymbol{X}\\} and it is real-valued. There is a set of \\{\\boldsymbol{X}^*\\} that having optimal A-value and \\{\\boldsymbol{X}^*\\}\\in \\{\\boldsymbol{X}\\}\nFor each design matrix \\boldsymbol{X} in searching space, we consider all possible permutations filtered by Equation 3.1 are neighbours of \\boldsymbol{X}, which is a subset of searching space \\{\\boldsymbol{X}\\}.\nAll possible permutations have equal possibility to be chose, and the sum of the possibility it equal to 1.\nA nonincreasing function T:N\\rightarrow (0, \\infty), T(t) is called the temperature at t-th iteration.\nA random initial design \\boldsymbol{X}_0\n\nSuppose we have a current design matrix \\boldsymbol{X}_i and its neighbours filtered by Equation 3.1 contains n_p numbers of design. The next design is determined as follows:\nA design matrix \\boldsymbol{X}_j is randomly picked from the neighbours of \\boldsymbol{X}_i. Suppose there is n_p design matrices in the neighbours of \\boldsymbol{X}_i, then the probability of selecting \\boldsymbol{X}_j among all neighbours is \\frac{1}{n_p}. This is actually the positive coefficients q_{ij} afore-mentioned in basic elements of SA. We now denote \\boldsymbol{X}(t) to be the design matrix at t-th iteration. Once \\boldsymbol{X}_j is chosen, the next design matrix is determined as follows: \nP(\\boldsymbol{X}(t+1)=\\boldsymbol{X}_j|\\boldsymbol{X}(t)=\\boldsymbol{X}_i)\n=\\begin{cases}\n1 & \\text{if } A(\\boldsymbol{X}_j)&lt;A(\\boldsymbol{X}_i) \\\\\n\\frac{1}{n_p}\\exp(-(A(\\boldsymbol{X}_j)-A(\\boldsymbol{X}_i))/T(t) & \\text{if } A(\\boldsymbol{X}_j)&gt;A(\\boldsymbol{X}_i)\\\\\n\\end{cases}\n\n\n3.2.5.1 Convergence analyze\nIn this section, We will discuss the convergence properties of the SA algorithm. During iteration process, temperature cooling schedule plays an important role in convergence. It determines whether the algorithm will reach an optimal or near-optimal solution over time. Basing on work in Sasaki and Hajek (1988), Bertsimas and Tsitsiklis (1993) gives a conclusion on convergence properties, with afore-mentioned basic element. Define that state i communicates with S^* at height h if there exists a path in S that starts at i and ends at some element of S^* and the largest value of J along the path is J(i)+h. Denote d^* be the smallest number such that every i \\in S communicates with S^* at height d^*.\n\nThe SA algorithm converges if and only if \n\\lim_{t\\to 0} T(t)=0\n and \n\\sum_{t=1}^{\\infty} \\exp[-\\frac{d^*}{T(t)}]=\\infty\n\\tag{3.3}\n\nTo ensure the condition above, the mostly chose cooling schedule is \nT(t) = \\frac{d}{\\log t}\n\\tag{3.4} Here d is some constant. It can be initial temperature.\n\nProof. It is obvious that \\lim_{t\\to 0} T(t)=0 in Equation 3.4\nBring Equation 3.4 in to Equation 3.3, we have \n\\sum_{t=1}^{\\infty} \\exp[-\\frac{d^*\\log (t)}{d}]=\\sum_{t=1}^{\\infty}t^{-\\frac{d^*}{d}}\n It is a harmonic series and it diverge when \\frac{d^*}{d}&lt;1, that is, d^*&lt;d. So SA of we have a large enough d (initial temperature).\n\nAlthough logarithmic cooling theoretically guarantees convergence to the global optimum, it is computationally demanding and has a very slow convergence rate in practice, making it less feasible for large-scale problems. To address these limitations, exponential cooling is introduced as a more practical alternative. While it does not offer a theoretical guarantee of reaching the global optimum, as noted by Kirkpatrick, Gelatt Jr, and Vecchi (1983). Because of limited time and computational resource, we will try to use exponential cooling in our as given in Aarts and Korst (1989) and well-practised It provides near-optimal solutions within a reasonable time, making it highly effective for large combinatorial optimization problems. \nT(t) = T_0 \\exp(-\\alpha*t)\n\\tag{3.5} Here T_0 is the initial temperature, and \\alpha is the cooling rate determine how fast the temperature drop.\n\n\n3.2.5.2 Algorithm\nFor accept probability Equation 3.2, we usually start with 0.8 and keep dropping when iteration goes.This probability depends on the magnitude of change in the objective function A-value. To simplify the initialization, we often conduct a few preliminary iterations to observe the range and rate of change in A-values, then use these observations to determine an initial temperature. Several studies have explored methods for autonomously selecting this initial temperature.\n\nInitialization: Start with a random design matrix \\boldsymbol{X}_0, set the iteration counter `k = 0, maximum number of iteration as M, A-value for the current design is A_c = A(\\boldsymbol{X}_0), initial temperature T_0 and a type of cooling schedule T(t).\nIteration: For each iteration k, k&lt;M:\n\nRandomly pick one permutation \\boldsymbol{X}_{new} among all neighbours of current design \\boldsymbol{X}_c.\nCalculate the A-value A(\\boldsymbol{X}_{new}) for permutation \\boldsymbol{X}_{new} and compare it with A(\\boldsymbol{X}_c).\nIf A(\\boldsymbol{X}_{new}) &lt; A(\\boldsymbol{X}_c), update \\boldsymbol{X}_c=\\boldsymbol{X}_{new}.\nIf A(\\boldsymbol{X}_{new}) &gt; A(\\boldsymbol{X}_c), update \\boldsymbol{X}_c=\\boldsymbol{X}_{new} with probability \\exp[-(A(\\boldsymbol{X}_{new}) - A(\\boldsymbol{X}_c))/T(t)].\n\nTermination: Repeat this process until the maximum number of iterations M is reached.\n\n\n\n\n\nAarts, Emile, and Jan Korst. 1989. Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing. John Wiley & Sons, Inc.\n\n\nBertsimas, Dimitris, and John Tsitsiklis. 1993. “Simulated Annealing.” Statistical Science 8 (1): 10–15.\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nKirkpatrick, Scott, C Daniel Gelatt Jr, and Mario P Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80.\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.\n\n\nSasaki, Galen H, and Bruce Hajek. 1988. “The Time Complexity of Maximum Matching by Simulated Annealing.” Journal of the ACM (JACM) 35 (2): 387–403.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Linear model\nSuppose we have a linear model,\n\\boldsymbol{y}=\\mathbf{X}\\boldsymbol{\\tau} + \\boldsymbol{\\epsilon} \\tag{2.1} where \\boldsymbol{y} is n\\times 1 vector of n observations, \\boldsymbol{\\tau} is a t\\times 1 vector of fixed effects, \\boldsymbol{\\epsilon} is the n\\times 1 vector for error, and \\mathbf{X} is a design matrix has size n\\times t. We assume that \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I}_{n}) and hence \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n).\nThe log-likelihood of Equation 2.1 is then given as:\n\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) = -\\frac{n}{2}\\log(2\\pi)-n\\log(\\sigma)-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}).\n The (i,j)-th entry of the Fisher information matrix is defined as\nI_{ij}(\\boldsymbol{\\tau})=-\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\tau_i\\partial\\tau_j}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)\n where \\tau_i is the i-th entry of \\boldsymbol{\\tau}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#linear-model",
    "href": "background.html#linear-model",
    "title": "2  Background",
    "section": "",
    "text": "Lemma 2.1 The Fisher information matrix of Equation 2.1 is given as \n\\mathbf{C} = -\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\n\nProof. The second derivative of the log-likelihood function \\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) is the Hessian matrix. We have \n\\frac{\\partial}{\\partial\\boldsymbol{\\tau}}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})\n and for second derivative is \n\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})==-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n And in linear model assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n) and the Fisher information matrix is unbiased because, in the expectation calculation process, we do not involve the randomness of \\boldsymbol{y}. The Fisher information matrix is actually determined by the design matrix \\boldsymbol{X} and the error variance \\sigma^2. Hence \n\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X} = -\\mathbf{C}\n So \\mathbf{C} = \\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\nLemma 2.2 The variance of the fixed effects for Equation 2.1 is equivalent to the inverse of the Fisher information matrix, i.e. var(\\hat{\\boldsymbol{\\tau}})=\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1} = \\mathbf{C}^{-1}.\n\n\nProof. We know that the MLE of \\boldsymbol{\\tau} in a linear model is \\hat{\\boldsymbol{\\tau}}=(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}. By assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n). So \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}).So we have var(\\hat{\\boldsymbol{\\tau}}) = \\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}, which is exactly the inverse of Fisher information matrix \\mathbf{C}^{-1}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#linear-mixed-model",
    "href": "background.html#linear-mixed-model",
    "title": "2  Background",
    "section": "2.2 Linear mixed model",
    "text": "2.2 Linear mixed model\nLinear mixed model extends linear model by incorporating additionally incorporating random effects into the model that effectively give greater flexibility and capability to incorporate known correlated structures into the model. We now consider a linear mixed model \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\n\\tag{2.2} here \\boldsymbol{y} is n\\times 1 vector for n observations, \\boldsymbol{\\tau} is a t\\times1 parameter vector of treatment factors, \\boldsymbol{u} is a q \\times1 parameter vector of blocking effects, and \\boldsymbol{\\epsilon} is the n\\times 1 error vector, \\boldsymbol{X} and \\boldsymbol{Z} are design matrices of dimension n \\times t and n \\times q for treatment factors and blocking factors, respectively. We here assume blocking factors are random effect, with random error \\boldsymbol{\\epsilon} we have \n\\begin{bmatrix}\n\\boldsymbol{u} \\\\\n\\boldsymbol{\\epsilon}\n\\end{bmatrix}\n\\sim\nN\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n\\boldsymbol{0}\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n\\boldsymbol{G} & \\mathbf{0} \\\\\n\\mathbf{0} & \\boldsymbol{R}\n\\end{bmatrix}\n\\right),\n where \\boldsymbol{G} is the q \\times q variance matrix for \\boldsymbol{u} and \\boldsymbol{R} is n\\times n variance matrix for \\boldsymbol{\\epsilon}.\n\n2.2.1 A-criterion\nOptimizing the A-value is crucial in row-column designs, for it directly relates to the precision of the treatment effect estimates. The A-value, a measure of design efficiency, quantifies how well the experimental design minimizes the variability when estimating treatment effects.\nBy focusing on minimizing the A-value, we aim to achieve a design that provides the most precise estimates of treatment effects. A lower A-value means that the design is more efficient, leading to smaller variances for the difference between treatment effect estimations.\nWe first start with a simple example, that is, we consider treatment factors \\boldsymbol{\\tau} are fixed, to elucidate the influence of A-criterion.\nbasing on assumption, we have \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau},\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\n\\tag{2.3} So for objective function, we can write out the distributions\n\n\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R}) \\\\\n\\boldsymbol{u}\\sim N(\\boldsymbol{0},\\boldsymbol{G})\n\nWe want to give a precise estimation on \\boldsymbol{\\tau}. As we mentioned, we have the distribution for response variable \\boldsymbol{y}\\sim We can use generalized least squares(GLS) by rewrite the model as: \n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\tau} + \\zeta\n Here \\zeta = \\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(0, \\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top). Henderson (1975) shows that the GLS estimation of \\boldsymbol{\\tau} is any solution to \n\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X}\\hat{\\boldsymbol{\\tau}}_{gls}=\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\n Here \\boldsymbol{V}=\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top. So \\hat{\\boldsymbol{\\tau}}_{gls} = (\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\nHenderson et al. (1959) emphasis that computing matrix \\boldsymbol{V} which is often large is difficult. So here we use joint log likelihood.\nFrom David Butler (2013), we conduct a maximum log likelihood by following objective function: \n\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\n :::{#lem-joint-density-lmm} So log of joint density is given as\n\\begin{align*}\n\\mathscr{L}&=\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\\\\\n&=-\\frac{1}{2}\\left(\\log|\\boldsymbol{R}|+\\log|\\boldsymbol{G}|+(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}\\right)\n\\end{align*}\n:::\n\nProof. We have density function for \\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R})\n\nf_y = \\frac{1}{\\sqrt{(2\\pi)^{n}|\\boldsymbol{R}|}}exp(-\\frac{1}{2}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u})))\n And density function for \\boldsymbol{u} \nf_u = \\frac{1}{\\sqrt{(2\\pi)^{l}|\\boldsymbol{G}|}}exp(-\\frac{1}{2}\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u})\n Ignoring constant part, we have \n\\log f_y=-\\frac{1}{2}[\\ln |\\boldsymbol{R}|+(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))]\n \n\\log f_u = -\\frac{1}{2}[\\ln |\\boldsymbol{G}+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}]\n So we have our log of joint density function.\n\nWe determine that \\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{\\tau}}=\\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{u}}=\\boldsymbol{0}, and write the equation into a matrix form \n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n\\tag{2.4}\nLet \n\\boldsymbol{C}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\quad\n\\hat{\\boldsymbol{\\beta}}_{llm}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n\\quad\n\\boldsymbol{W}=\\begin{bmatrix}\\boldsymbol{X} &\\boldsymbol{Z}\\end{bmatrix}\n By cancelling \\boldsymbol{u}, we have \n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}y=\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n where \\boldsymbol{P}=\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}, let \\boldsymbol{C}_{11}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X} then we have the form similar to GLS estimation for \\hat{\\boldsymbol{\\tau}}, which is \n\\boldsymbol{C}_{11}\\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n and the estimation of \\boldsymbol{\\tau} is \\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{C}_{11}^{-1}\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}, which is equivalent with GLS estimation\n\nProof. We only need to prove that \\boldsymbol{P}=\\boldsymbol{V}^{-1}. \\begin{align*}\n\\boldsymbol{P}\\boldsymbol{V} &=(\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})(\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top - \\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad\\quad\\quad -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top(\\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{Z}^\\top+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{I}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G})\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{G}^{-1}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z})\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top-\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n& = \\boldsymbol{I}\n\\end{align*} So \\hat{\\boldsymbol{\\tau}}_{llm} and \\hat{\\boldsymbol{\\tau}}_{gls} are equivalent, and we denote them as \\hat{\\boldsymbol{\\tau}}\n\nNow we have our estimation for the treatment factor, and experimental design aims to further refine our design by focusing on the precision of these estimates. Specifically, we aim to optimize the design so that the treatment effects are estimated with minimal variance, ensuring that the differences between any two treatment levels are as small as possible. To achieve this, we introduce the A-value as a criterion for evaluating the design.\n\nDefinition 2.1 Basing on the model formula Equation 2.2, and a estimation of treatment factor \\hat{\\boldsymbol{\\tau}} has n_{\\tau} factors. A-criterion measure the average predicted error variance of different treatments. Let V_{ij}= var(\\hat{\\tau}_i-\\hat{\\tau}_j)=var(\\hat{\\tau}_i)+var(\\hat{\\tau}_j)-2cov(\\hat{\\tau}_i,\\hat{\\tau}_j), and a A-value \\mathscr{A} is \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n\\tag{2.5}\n\nTo discover the relationship between the A-value and the design matrix, I need to find the variance-covariance matrix of \\hat{\\boldsymbol{\\tau}}. In fact, it can be proof that \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).\n(lemma and proof)\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)(\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1})^\\top)\nWe denote \\begin{align*}\n\\boldsymbol{M} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{N} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z}\\\\\n\\boldsymbol{J} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{K} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1}\\\\\n\\end{align*} In the context of row-column design, the \\boldsymbol{K} matrix is invertible. Schur complement of \\boldsymbol{K} is \n\\boldsymbol{S} = \\boldsymbol{M} - \\boldsymbol{N} \\boldsymbol{K}^{-1} \\boldsymbol{J}\n And the inverse matrix of \\boldsymbol{C} can be written as \n\\boldsymbol{C}^{-1}=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{S}^{-1} & -\\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1} \\\\\n-\\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} & \\boldsymbol{K}^{-1} + \\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1}\n\\end{bmatrix}\n\\tag{2.6} So from Equation 2.4 we have \n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}} \\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{U}_1\\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\\boldsymbol{y}\n Here \\begin{align*}\n&\\boldsymbol{U}_1 = \\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\\\\n&\\boldsymbol{U}_2 = \\boldsymbol{C}^{21}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{22}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{align*}\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top)\nAnd we have following results\n\\begin{align*}\n\\begin{bmatrix}\n\\boldsymbol{U}_1 \\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n(\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1}\n\\end{bmatrix}-\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & \\boldsymbol{G}^{-1}\n\\end{bmatrix})\\\\\n&=\n\\boldsymbol{I}-\n\\begin{bmatrix}\n0 & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & -\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{I} & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & \\boldsymbol{I}-\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\end{align*}\nSo we have \\begin{align*}\n\\boldsymbol{U}_1\\boldsymbol{X}&=\\boldsymbol{I}\\\\\n\\boldsymbol{U}_1\\boldsymbol{Z}&=-\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\\\\n\\end{align*} For the variance of estimation we have \\begin{align*}\nvar(\\hat{\\boldsymbol{\\tau}})\n&=\n\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top\\\\\n&= \\boldsymbol{U}_1\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&= (\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&=\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\boldsymbol{G}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}-\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top+\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}\n\\end{align*}\nWhat is \\boldsymbol{C}^{11}? what is the relation between \\boldsymbol{C}_{11} and \\boldsymbol{C}^{11}? From Equation 2.6 we have \\boldsymbol{C}^{11}=\\boldsymbol{S}^{-1} and \\boldsymbol{S} is \n\\boldsymbol{S} = \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\n And base on the complement of \\boldsymbol{C}_{11}, we rewrite the \\boldsymbol{S} \n\\begin{align*}\n\\boldsymbol{S}\n&=\\boldsymbol{X}^\\top(\\boldsymbol{R}^{-1} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1})\\boldsymbol{X}\\\\\n&=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\\\\n&=\\boldsymbol{C}_{11}\n\\end{align*}\n So var(\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{C}^{11}=\\boldsymbol{C}_{11}^{-1}.\nSo we have \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).To examine a specific form of \\boldsymbol{\\tau}, in general case, we do linear transform on \\boldsymbol{\\tau}: \\hat{\\boldsymbol{\\pi}}=\\boldsymbol{D}\\hat{\\boldsymbol{\\tau}}, where \\boldsymbol{D} is some transform matrix, so we have \\boldsymbol{D}(\\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{\\pi}-\\hat{\\boldsymbol{\\pi}}\\sim N(0,\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top). We denote \\boldsymbol{\\Lambda}=\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top. If \\boldsymbol{D} is identical matrix \\boldsymbol{I}, then \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1} and \\hat{\\boldsymbol{\\pi}}=\\hat{\\boldsymbol{\\tau}}.\nWe know that A-criterion is the mean of predicted error variance of the parameter from Equation 2.5 i.e.  \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n Having variance-covariance matrix \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1}, we can rewrite the sum part as n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}.So we have \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}[n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}]\n\\tag{2.7} same result from DG Butler, Smith, and Cullis (2013)\nDerivation above indicate that \\mathscr{A}\\propto tr(\\boldsymbol{\\Lambda}), A-criterion as the mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix \\boldsymbol{\\Lambda} should be as small as possible. And this is our goal on optimal experimental design.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#neighbor-balance-and-eveness-of-distribution",
    "href": "background.html#neighbor-balance-and-eveness-of-distribution",
    "title": "2  Background",
    "section": "2.3 Neighbor balance and eveness of distribution",
    "text": "2.3 Neighbor balance and eveness of distribution\n\n2.3.1 Concepts of NB and ED\nPiepho, Michel, and Williams (2018) emphasis the the concepts of neighbor balance and even distribution are crucial to mitigating biases and ensuring the reliability of results in row-column design.\nNeighbor balance (NB) refers to the principle that, in a row-column experimental design, the frequency with which two treatments are adjacent or near each other should not be excessively high. High adjacency frequency between two treatments can lead to mutual influence, which may cause bias to the experimental results. For example, if the effect of one treatment can spread to neighboring areas, frequent adjacency could interfere with accurate measurement of each treatment’s true effect next to it. Therefore, it is essential to control the adjacency frequency of different treatments to prevent high adjacency for two specific treatments.\nEven distribution(ED) aims to ensure that different replications of the same treatment are widely distributed across the experimental field, rather than being clustered in a specific area. This strategy helps to avoid biases caused by specific environmental conditions in certain parts of the experiment field. If replications of one treatment are over concentrated in one area, unique environmental factors in that area might affect the treatment’s performance, leading to biased observations. By evenly distributing replications, environmental interference can be minimized, so that we can enhance the reliability of the experimental results.\n(maybe some example plots or pictures)\n\n\n2.3.2 Measuring NB and ED\n\n2.3.2.1 Evaluating NB with adjacency matrix\nIn Piepho, Michel, and Williams (2018), there is a assumption that they are optimizing a binary design, which means each treatment appears only once in each row and column. Under this assumption, their balancing mechanism considers diagonal adjacency, Knight moves, and even more distant points to ensure an optimal balance. In my optimization process, I begin with a randomly selected design matrix. Consequently, my approach considers not only diagonal adjacency but also the adjacent points directly above, below, to the left, and to the right.\nI use an adjacency matrix to count the number of times each treatment is adjacent to another. This matrix serves as a crucial tool in my optimization process, enabling precise tracking and adjustment of treatment placements to achieve neighbor balance.\nWe denote the adjacency matrix as \\boldsymbol{A}, and for treatment i and j in treatment set T \\boldsymbol{A}_{ij} represents the count of times treatment i is adjacent to treatment j. Here “adjacent” means treatment j is located next to treatment i (maybe a picture to show it)\nFor Given design \\mathcal{D} and \\mathcal{D}_{r,c} represents the treatment at row r and column c. So \\boldsymbol{A}_{ij} can be expressed as:\n\n\\boldsymbol{A}_{ij}=\\sum_{r=1}^{R}\\sum_{c=1}^{C}I_{r,c}(i) F_{r,c}(j)\n where \nF_{r,c}(j)=\n\\sum_{m \\in \\{-1,0,1\\}}\\sum_{n \\in \\{-1,1\\}}I_{r+m,c+n}(j)+\\sum_{m \\in \\{-1,1\\}}I_{r+m,c}(j)\n R and C are total number of rows and columns and I_{r,c}(\\cdot) is the indicator function, which takes value under following cases \nI_{r,c}(i)=\n\\begin{cases}\n1 & \\text{if } \\mathcal{D}_{r,c}=i \\\\\n0 & \\text{if } \\mathcal{D}_{r,c}\\neq i & \\text{or } r&lt;1,r&gt;R,c&lt;1,c&gt;C\\\\\n\\end{cases}\n\nThe function F_{r,c}(j) here is actually counting the times that treatment j occurs at places around the position row r and column c.\nWe measure NB by taking the maximum of the elements in adjacency matrix \\boldsymbol{A}. Our NB criteria is \nC_{NB}=max\\{\\boldsymbol{A}_{ij}\\}-min\\{\\boldsymbol{A}_{ij}\\}  \\quad i,j\\in T\n\n\n\n2.3.2.2 Evaluating ED with minimum row and column span\nThe goal of evaluating the evenness of distribution (ED) is to find the row and column spans for treatments across the entire design matrix. We would like this value as large as possible This ensures that the treatments t\\in T is distributed as evenly as possible within the rows and columns, reducing clustering and promoting a balanced design.\nThe row span for a given treatment t \\in T is defined as the difference between the maximum and minimum row indices where t appears in experiment field. \nRS(t)=max\\{r:\\mathcal{D}_{r,c}=t\\}-min\\{r:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n And the minimum row span of a design \\mathcal{D} is \nMRS(\\mathcal{D})=min\\{RS(t)\\},\\quad t \\in T\n Same for column span \nCS(t)=max\\{c:\\mathcal{D}_{r,c}=t\\}-min\\{c:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n \nMCS(\\mathcal{D})=min\\{CS(t)\\},\\quad t \\in T\n So, for the changes in the design matrix \\mathcal{D} during the search process, we tend to accept only those changes where the Minimum Row Span (MRS) and Minimum Column Span (MCS) remain the same or become smaller.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nHenderson, Charles R. 1975. “Best Linear Unbiased Estimation and Prediction Under a Selection Model.” Biometrics, 423–47.\n\n\nHenderson, Charles R, Oscar Kempthorne, Shayle R Searle, and CM Von Krosigk. 1959. “The Estimation of Environmental and Genetic Trends from Records Subject to Culling.” Biometrics 15 (2): 192–218.\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In the field of experimental design, efficient methods are crucial for ensuring accurate and reliable results. One such method is the row-column design, controlling variability in experiments involving two factors, typically arranged in rows and columns. The row-column design can be considered as an extension of the Latin square design with more flexibility, allowing for different numbers of rows, columns, and treatments. This flexibility makes row-column designs applicable to a wider range of experimental settings.\nTo offer a row-column design that gives a precise estimation of treatment effects, one way is to seek the optimal value of some statistic criteria, for example, A-criteria(links to sections), minimizing the variance of elementary treatment contrasts. Using linear mixed model and assuming fixed treatment effects and random blocking effect, Butler (2013) has show the relation between optimizing design and minimizing the value of A-criteria, and show some possible algorithms to search optimal design in feasible set. These algorithms mainly focus on comparing the arrangements of different treatments, that is, doing permutations, and calculating their A values, optimizing design by iterations.\nHowever, some undesired cluster of replications or some treatment may occur when algorithm are doing permutations along rows and columns. Piepho, Michel, and Williams (2018) found that such clustering is considered undesirable by experimenters who worry that irregular environmental gradients might negatively impact multiple replications of the same treatment, potentially leading to biased treatment effect estimates. Williams emphasis that there is a need to design a strategy to avoid clustering and achieve even distribution of treatment replications among the experimental field. Two properties of design are introduced. Even distribution of treatment replications, abbreviated as ED, and neighbor balance, abbreviated as NB. A good ED ensures every replications of a treatment are widely spread in experimental field, and NB helps to avoid replications of the some treatment cluster together repeatedly. Williams introduce a scoring system to analysis ED and NB for a specific design, and introduce a algorithm to optimize ED, NB and some average efficiency factor can be represented by a specific statistic criteria.(maybe saying some improvement is needed)\nWe offer an optimization strategy for a design problem, which we can improving ED and NB during optimizing statistic criteria for a design, and avoid unwanted clustering and self-adjacency on the resulting design.In this algorithm, we use A-criteria to evaluate the efficiency of a design. Before the algorithm, we randomly generate a design as an initial design, and calculate the A-criteria as initial value. We update design by selecting a better among its neighbors. The neighbors are pair-wise permutations of a design. Typically, we select a neighbor from all pairwise permutations of a design for iteration, but this does not ensure ED and NB. To ensure ED and NB during optimization, we need to add some constraints when generating the pairwise permutations.(maybe explain what is the constraints) By filtering design with bad ED and NB, we then optimize the statistic criteria of the design.\n(sections in the thesis)\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "4  Results",
    "section": "",
    "text": "4.1 Simulation set-up\nTo begin the Results section, we’ll look at the foundational set-up of my simulation. In order to streamline the computation within limited computational resources, I implemented a series of simplifications.\nFor \\boldsymbol{G} matrix, I set it to be a diagonal matrix with equal values along the diagonal. It implies that there is no correlation between rows and columns, meaning that the effects for rows and columns are considered independent. And the equal values along the diagonal indicate that the variance of these row and column effects is the same. In this simulation it is \\boldsymbol{G}_s = 10\\boldsymbol{I}_{n_r+n_c}\nFor \\boldsymbol{R} matrix, similarly, it is set as a diagonal matrix with identical values along the diagonal. implying that the geographical locations of different plots are independent of each other and variance of residuals is uniform across plots. In this simulation it is \\boldsymbol{R}_s = 0.1\\boldsymbol{I}_{n_r\\times n_c}\nTo compare the differences between algorithms, we used the design function from the R package blocksdesign to calculate the optimized A-value A_{blocksdesign} basing on Edmondson (2020), which we then compared to the results from our iterative process.\nFor the consistency across all algorithms, we aimed to explore the largest feasible range of row, column, and treatment combinations. We set the maximum number of iterations M=2000 for each algorithm. And we add an early termination criterion: if the iterative algorithm’s A-value approaches A_{blocksdesign} within a specified tolerance T_A, the algorithm would stop iterating and output the A-value and design matrix. In this simulation we set T_A=2\\times 10^{-4} Therefore we can evaluate the performance of different algorithms across various combinations of row, column, and treatment numbers by comparing the total computational time required, the actual iteration number before reaching either M or an early termination based on A_{blocksdesign}, and the absolute difference between the algorithm’s and A_{blocksdesign}.Basing on these evaluations, we can identify how effectively each algorithm performs under different complexity levels and determine the algorithm’s efficiency and accuracy across different cases.\nIn setting up random selection, the final parameter is the step length s. During runtime, the process of selecting permutations for step-length iterations often accounts for a large portion of the total functioning time.Additionally, this selection becomes especially challenging when the row, column, and treatment numbers are low. In such cases, while applying filtering criteria, it can be difficult to find a sufficient number of unique permutations that meet the requirements, as the design itself may not contain enough options. To ensure the algorithm runs smoothly and completes within a reasonable time, we set the step length to s=3.\nFor the SA algorithm, we need to set the initial temperature T_0.It actually depends on the acceptance rate for higher A-values at the start of the iteration process. As mentioned in the previous section, we typically aim for an initial acceptance rate of around 0.8. That is we hope $(-A_{}/T_0) $. We often lack precise information on A_{\\bigtriangleup} before the algorithm, we set the initial temperature based on an empirical estimate. We set T_0=1. Some adaptive algorithms for setting the initial temperature are often discussed in the literature.\nTo examine combinations of row, column, and treatment numbers, we tested a wide range of configurations. Row number ranged from 5 to 25, and column number varied from 5 to 16. For each row and column combination, we tested treatment numbers of 10, 20, 50, 80, and 100. To ensure that the blocksdesign functions operated correctly and to maintain general applicability, we ship a combination, if the product of rows and columns was less than twice the treatment number.It ensures that each treatment could have at least two replication. For each combination of row, column and treatment number, we conduct three replication for more accurate observation.\nFor the cases when treatment number cannot evenly divide the product of rows and columns, suppose n_{plot}=n_r\\times n_c and treatment number is n_{\\tau}, we set base replication number as \\lfloor \\frac{n_{plot}}{n_{\\tau}} \\rfloor, and randomly pick n_{plot}\\quad mod\\quad n_{\\tau} treatments assign \\lfloor \\frac{n_{plot}}{n_{\\tau}} \\rfloor+1 replication to these treatments. This approach ensures that the number of each treatment is as close as possible to one another.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#result-analyze",
    "href": "results.html#result-analyze",
    "title": "4  Results",
    "section": "4.2 Result analyze",
    "text": "4.2 Result analyze\n\n4.2.1 General behviour of algorithms\nNow we observe the overall behaviour of the algorithm, to examine the change of A-value during the process. We here use example with n_c=n_r=15 and n_\\tau=50. we now use blocksdesign functions to generate a optimal design \\mathcal{D}_{op} with A-value A_{op} = 0.5027 under set-up, NB criteria C_{NB}(\\mathcal{D}_{op})=3 and ED criteria MRS(\\mathcal{D}_{op})=MCS(\\mathcal{D}_{op})=5. See detailed assignment as follow\n\n\n\n\n\n\nblocksdesign out put\n\n\n\n\nFigure 4.1\n\n\n\n\n4.2.1.1 Random selection\nWe now look at how random selection behave during the process. The iteration stops at 549-th step with output A-value A_{rs}=0.050478 and NB criteria C_{NB}(\\mathcal{D}_{rs})=4 and ED criteria MRS(\\mathcal{D}_{rs})=5 and MCS(\\mathcal{D}_{rs})=7. The changes in the A-value over iterations and the specific design are shown in the figure below.\n\n\n\n\n\n\nA v.s. Iteration (Random search)\n\n\n\n\nFigure 4.2\n\n\n\n\n\n\n\n\n\nRandom search out put design\n\n\n\n\nFigure 4.3\n\n\n\nFrom the results, the A-value obtained by Random Search is relatively close to that of the block design, though slightly larger, as is the NB statistic. Recall that we would like A-value and C_{NB} to be as small as possible, and for both MRC and MRC, the larger the better. Therefore, the Random Search outcome has a larger minimum column span, meaning it preform better on evenness of distribution.\n\n\n4.2.1.2 SA with log cooling schedule\nFor SA using the log cooling schedule Equation 3.4,Unlike random selection, the algorithm accepts higher A-values with a certain probability, causing fluctuations in A-values throughout the iterations, resulting in rises and falls rather than a steady decline. Details can be seen in the figure below.\n\n\n\n\n\n\nA v.s. Iteration (SA with log cooling schedule)\n\n\n\n\nFigure 4.4\n\n\n\n\n\n\n\n\n\nSA with log cooling schedule out put design\n\n\n\n\nFigure 4.5\n\n\n\nAs it indicate in the plot, the process went through all iterations, reaching maximum iteration number M. At the 2000th iteration, the A-value is A_{SAlog}=0.050609 with NB criteria C_{SAlog}(\\mathcal{D}_{rs})=5 and ED criteria MRS(\\mathcal{D}_{SAlog})=7 and MCS(\\mathcal{D}_{SAlog})=6.\nCompared to random selection, the optimization of the A-value in SA is less effective, since the limitation of a maximum iteration number M.Similar to the NB statistic, where SA shows a less optimal outcome than the block design. However, the two ED statistics given by SA are significantly improved compared to those provided by the blockdesign function and random selection. Because of the slower decrease in acceptance rates for higher A-values with log cooling schedule, we observe that the algorithm occasionally continues to accept larger A-values even in the latter stages.\n\n\n4.2.1.3 SA with exp cooling schedule\nThis is also SA but with an exponential cooling schedule Equation 3.5. Although exponential cooling schedule does not theoretically guarantee SA convergence to the global optimum, it can produce a relatively near-optimal solution within finite computational resources and time constraints. Because of the rapid temperature decrease in exponential cooling, the algorithm initially shows fluctuations in A-values at the beginning of the iterations. However, as the number of iterations increases, the acceptance rate for larger A-values declines quickly, and algorithm tend to avoid accepting higher A-values.Details are shown in the figure below\n\n\n\n\n\n\nA v.s. Iteration (SA with exp cooling schedule)\n\n\n\n\nFigure 4.6\n\n\n\n\n\n\n\n\n\nA v.s. Iteration (SA with exp cooling schedule)\n\n\n\n\nFigure 4.7\n\n\n\nThe iteration process stopped at the 1341-st iteration because the A-value had come sufficiently close to the blockdesign A-value. The results show that the A-value achieved is A_{SAexp} = 0.050473, with an NB statistic C_{SAexp}(\\mathcal{D}_{rs})=4, and ED statistics MRS(\\mathcal{D}_{SAexp})=5 and MCS(\\mathcal{D}_{SAexp})=6.\nThe exponential cooling schedule achieved similar A-values and NB statistics to those of random selection. However, its performance on the ED statistic was less effective compared to both random selection and log cooling. The reason may be the early termination of iterations in exponential cooling, which limited the exploration of the whole design matrix space.\nIn contrast, random selection evaluates multiple permutations (equal to the step length s) in each iteration, which allows it to consider a broader range of potential solutions. Meanwhile, log cooling slows down the convergence, allow it has sufficient iterations to explore the solution space, although it selects only one neighbour at a time.\n\n\n\n4.2.2 Analysis by Algorithm\nNow we begin to explore the relationship between function runtime and the number of rows, columns, and treatments. Generally, runtime is proportional to the number of iterations. Here, we have set the maximum number of iterations to 2000. If in the simulation most runs reach the maximum number of iterations, then evaluating the relationship between runtime and rows, columns, and treatment numbers will no longer be meaningful.Therefore, before the analysis, we need to examine the distribution of iteration numbers. If most of the simulations do not reach the maximum number of iterations, we can use runtime as a measure of computational efficiency.\nFor random selection, many simulations did not reach the maximum number of iterations, which can be observed in the figure.\n\n\n\n\n\n\n#rows v.s. runtime (random selection)\n\n\n\n\nFigure 4.8\n\n\n\nSo we present the relationship between runtime and the number of rows under different treatment numbers, as shown in the figures below. As mentioned earlier, we have three replications for each combination of rows, columns, and treatment numbers. We calculate the average of the three replications for each combination as the final result.\n\n\n\n\n\n\n#rows v.s. runtime (random selection)\n\n\n\n\nFigure 4.9\n\n\n\nWe can observe that, under different numbers of columns, the relationship between rows and runtime tends to be similar. Here, we will not discuss the origin or significance of this trend but instead focus on the relation between variables and runtime. There is no clear positive or negative relationship between the number of rows and runtime. We see that as the number of rows increases, the runtime does not consistently increase or decrease, which is related to the randomness of random selection. During the iteration process, we might be lucky enough to find a good design early and terminate the iteration, or the A-value may gradually decrease over the iterations.\nHowever, by observing the vertical axis of different subplots, we can see that as the number of treatments increases, the range of runtime fluctuations also increases. To demonstrate this, we present the relationship between the number of treatments and runtime under different rows and columns.\n\n\n\n\n\n\n#treatment v.s. runtime (random selection)\n\n\n\n\nFigure 4.10\n\n\n\nBy observing the figure, we can see that as the number of treatments increases, runtime shows a positive correlation, indicating that with more treatments, random selection requires more time to find permutations and make comparisons.\nThis approach leads us to consider the relationship between the number of treatments and runtime. However, for SA with log cooling, due to the slow convergence of log cooling and the limitation of a maximum number of iterations, the algorithm often runs until the maximum number of iterations is reached. This makes it difficult to observe the relationship between the number of treatments and runtime, as runtime is often related to the maximum number of iterations. See the image below for details.\n\n\n\n\n\n\nIteration number distribution (SA with log cooling schedule)\n\n\n\n\nFigure 4.11\n\n\n\nIn the figure, we can see that only when the number of rows and columns is small, meaning the design space is limited, SA with log cooling can terminate before reaching the maximum number of iterations. In most other cases, the algorithm stops when it reaches the maximum number of iterations. Therefore, we use the difference d = (A_{SAlog}-A_{op})/A_{op} between the A-value obtained by SA with log cooling at the maximum number of iterations and the A-value calculated by the blockdesign function to evaluate the impact of the number of rows, columns, and treatments on the efficiency of the algorithm.\nWe first look at the influence of the number of rows on the distance. Similar to the analysis method used for random selection, we provide the following figure.\n\n\n\n\n\n\n#rows v.s. difference (SA with exp cooling schedule)\n\n\n\n\nFigure 4.12\n\n\n\nFrom the above figure, it is evident that the number of rows and columns is positively correlated with the difference. The greater the number of rows, the larger the difference. Similarly, the greater the number of columns, the higher the lines (towards the red), indicating a larger difference. Based on previous experience, we attempt to examine the relationship between the number of treatments and distance, as shown in the figure below.\n\n\n\n\n\n\n#treatment v.s. difference (SA with exp cooling schedule)\n\n\n\n\nFigure 4.13\n\n\n\nThe results show that the treatment number is negatively correlated with the difference. In other words, as the treatment number increases, the resulting value (whether the iteration stops early or reaches the maximum number of iterations) tends to be closer to the optimal value. However, this does not necessarily mean that the absolute distance to the optimal value decreases.\nFor SA with exponential cooling, we also examine the distribution of the iteration numbers. ::: {#fig-align style=“text-align: center”}  :::\nIt can be seen that most of the simulations did not reach the maximum number of iterations, so we can use runtime to compare the effects of rows, columns, and treatment numbers on the algorithm.\n\n\n\n\n\n\n#rows v.s. runtime (random selection)\n\n\n\n\nFigure 4.14\n\n\n\nWe can see that when the treatment number is small, the influence of rows and columns on runtime is not significant. This could be because the design space is smaller, and for randomness, the algorithm stops after fewer iterations. As the treatment number increases, the impact of rows and columns on runtime becomes apparent: the greater the number of rows and columns, the longer the runtime.This effect can also be observed when examining the influence of the treatment number on runtime.\n\n\n\n\n\n\n#rows v.s. runtime (random selection)\n\n\n\n\nFigure 4.15\n\n\n\nSimilarly, we can observe that as the treatment number increases, the runtime also increases. Additionally, as the number of rows increases, the vertical axis of the plot also grows, indicating their positive correlation.\n\n\n4.2.3 Comparison Between Algorithms\nNow we compare the effectiveness of different algorithms. To simplify the process, we focus on observing the impact of the number of rows, columns, and treatments on the average runtime and the average difference under equivalent conditions. For example, when comparing the effect of the number of rows on the difference for different algorithms, we take the average of the difference for different column counts and treatment numbers at the same row level to simplify the visualization and makes the comparison more intuitive. Before making comparisons, it is important to note that, for our computational resource limitations, we have limited the maximum iteration number to 2000. Therefore, our comparison cannot determine which algorithm is better. Therefore, it aims to compare the performance of different algorithms under the same computational constraints. Here, we compare the effects of row number, column number, and treatment number on the difference and runtime across different algorithms.\n\n4.2.3.1 Comparing with difference\nNow, we use the difference to compare the computational efficiency of different algorithms. We present the figures showing the influence of row number, column number, and treatment number on the difference for the three algorithms. Note that for random selection and SA with exponential cooling, the method for calculating the difference is the same as for SA with logarithmic cooling, which means it represents the relative difference, that is d = (A_{op}-A_{result})/A_{op}.\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.16\n\n\n\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.17\n\n\n\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.18\n\n\n\nOverall, it can be observed that for the difference, random selection has the smallest value, SA with exponential cooling is slightly larger, and SA with logarithmic cooling has the largest value. Moreover, SA with logarithmic cooling is more influenced by the three variables.\nIn general, for all three algorithms, as the number of rows and columns increases, the relative difference also increases. However, the difference is negatively correlated with the treatment number. As mentioned earlier, a smaller relative difference does not necessarily mean that the absolute distance to the optimal value decreases. We can use the figure below to support this point.\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.19\n\n\n\nIt can be seen here that if we consider the absolute difference, it is actually positively correlated with the treatment number.\n\n\n4.2.3.2 Comparing with runtime\nNow, let’s see what the comparison looks like from the perspective of runtime. We present the following figure to illustrate this.\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.20\n\n\n\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.21\n\n\n\n\n\n\n\n\n\n#rows v.s. difference)\n\n\n\n\nFigure 4.22\n\n\n\nFrom the figures, we can see that SA with exponential cooling is the fastest, while SA with log cooling takes a bit longer, and random selection requires the longest time. When the number of rows, columns, and treatments is still small, for the design space is limited and the randomness of the algorithms, the differences are less noticeable. However, as these numbers increase, the differences in runtime become more significant, with random selection showing the fastest increase in runtime. Overall, the number of rows, columns, and treatments are all positively correlated with runtime, but the treatment number has a more direct impact on runtime.\n\n\n\n4.2.4 Conclusion\nNow, we summarize the computational performance of the three algorithms under constrained computational resources.\nFirst, for random selection, as seen in the analysis above, this algorithm provides relatively accurate results but requires a longer runtime. When the number of treatments increases, the runtime significantly rises. In our simulations, the step-length was set to 3. Increasing the step-length would allow random selection to search the design space more extensively but would require more time.\nNext is Simulated Annealing (SA). Overall, the iteration speed is faster compared to random selection. And increasing the number of rows, columns, and treatments requires more iterations and longer runtime. Although the log cooling schedule theoretically ensures convergence to the global optimum, as shown in the simulations above, it may require more iterations to reach the global optimum. On the other hand, the exponential cooling schedule converges faster, and it often ends iteration process early in the simulations. As mentioned earlier, while it cannot theoretically ensure convergence to the global optimum, it can provide an approximately optimal solution within a relatively short time frame under constrained computational resources and avoid local optima to some extent.\n\n\n\n\nEdmondson, Rodney N. 2020. “Multi-Level Block Designs for Comparative Experiments.” Journal of Agricultural, Biological and Environmental Statistics 25 (4): 500–522.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "5.1 Limitations\nIn this section, we focus on examining the limitations and shortcomings of our algorithm, along with discussing the practical constraints observed in the experimental simulation results. Our analysis is organized into the following parts: computational efficiency and convergence Rate, balancing multi-objective optimization, and limitations and generalizability in practical applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "discussion.html#future-directions",
    "href": "discussion.html#future-directions",
    "title": "5  Discussion",
    "section": "5.2 Future Directions",
    "text": "5.2 Future Directions\n\n5.2.1 Penalty objective function\nIncorporate a penalty term into the objective function. When adjacent treatments in the design are the same, this penalty term increases the value of the objective function, making the optimization process more likely to avoid having the same treatment in adjacent positions.\nFor example, we can define a following penalty function. For example we have a design matrix X, the penalty for self-adjacency is\n\nf(X) = \\sum_{(i,j)\\in X}I(X_i = X_j)\n Here i and j are different plots next to each other in the experiment field, and I(X_i = X_j) is an indicator function equals to 1 if i plot and j plot have the same treatment.\nSo basing on this penalty function, For a certain design X we change our function into:\n\nF(X) = \\bar{f}_A^{RC} + \\lambda \\cdot f(X)\n here \\lambda &lt; 0\nOr use A-criteria : \nG(x) = t\\mathscr{A} + (1-t)\\cdot f(X)\n here 0\\leq t \\leq 1 and we minimize it\nUsually we have such mathematical programming of inequality constrained optimization problem: we minimize objective function f_0(x) with inequality constrains f_i(x)\\leq 0, i\\in I=\\{1,2,\\cdots,m\\}.And we have a well-known penalty function for this problem is \nF (x, \\rho) = f_0(x) + \\rho \\sum_{i\\in I}max\\{f_i(x),0\\}\n and a corresponding constrained penalty optimization problem is to minimize penalty function F_2 (x, \\rho), detailed information in Meng et al. (2013)\n\n\n5.2.2 Algorithm improvment\n\n5.2.2.1 SA-based multiobjective optimization algorithms\nWhen discussing the A-value, NB statistic, and ED statistic, our goal is actually to find a process that can simultaneously optimize all three, or finding a balance between three statistic criteria.SA is often used in combinatorial optimization problems, as we are using it here to optimize the arrangement of treatment plots. Suman and Kumar (2006) claim that in recent studies, SA has been applied in many multi-objective optimization problems, because of its simplicity and capability of producing a Pareto set of solutions in single run with very little computational cost.\nWe say a solution is non-dominated if none of its objective values can be improved without worsening at least one other objective. The Pareto set (or Pareto front) is a set of non-dominated solutions in a multi-objective optimization problem. Suman and Kumar (2006) introduce several SA for multi-objective optimization.\nSA-based multi-objective optimization algorithms given by Suppapitnarm et al. (2000) is a promising approach. Instead of Equation 3.2 and using permutation filtering, they give probability step write as this form \nP = \\min(1, \\Pi_{i=1}^N\\exp\\{\\frac{-\\bigtriangleup s_i}{T_i}\\})\n Here N is the number of objective functions, and for each objective function i, \\bigtriangleup s_i is the difference of objective function between two solution and T_i is the current temperature. Control the optimization rates of different objective functions by setting different cooling schedules for each.\n\n\n5.2.2.2 Memory-Based Optimization Algorithms - TABU search\nSame with SA, Tabu Search is an optimization algorithm used for solving combinatorial problems. It is a type of local search method that enhances basic hill-climbing algorithms by using memory structures to avoid revisiting previously explored solutions, introduced in Butler, Smith, and Cullis (2013). This helps the search process escape from local optima and encourages exploration of new areas in the solution space. Tabu Search maintains a tabu list, a short-term memory that keeps track of recent moves or solutions that should not be revisited for a certain number of iterations. This is mathematically represented as a set \\mathcal{T} where recent solutions X_t are stored for a fixed period k iterations: \n\\mathcal{T} = \\{x_t|t \\in [t-k,t)\\}\n If a solution x' \\in \\mathcal{T}, x' is considered “tabu” and cannot be revisited. Therefore, besides our two approaches here—random search with step-length and SA that probabilistically accepts worse objective function values—this type of short-term memory-enhanced algorithm (Memory-Enhanced Algorithms) helps ensure efficient computation, minimizing resource consumption while maximizing exploration within the solution space and helping escape local optima.\n\n\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nMeng, Zhiqing, Chuangyin Dang, Min Jiang, Xinsheng Xu, and Rui Shen. 2013. “Exactness and Algorithm of an Objective Penalty Function.” Journal of Global Optimization 56: 691–711.\n\n\nSuman, Balram, and Prabhat Kumar. 2006. “A Survey of Simulated Annealing as a Tool for Single and Multiobjective Optimization.” Journal of the Operational Research Society 57 (10): 1143–60.\n\n\nSuppapitnarm, A, Keith A Seffen, Geoff T Parks, and PJ Clarkson. 2000. “A Simulated Annealing Algorithm for Multiobjective Optimization.” Engineering Optimization 33 (1): 59–85.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "discussion.html#limitations",
    "href": "discussion.html#limitations",
    "title": "5  Discussion",
    "section": "",
    "text": "5.1.1 Computational efficiency and convergence rate\nBased on the line plots from the previous section, we observe that the convergence rate of our three algorithms significantly slow down as they approach the optimal solution. This often results in giving a near-optimal solutions, although it performs reasonably well within the limited number of iterations M in our simulation. For iterative methods, the algorithm actually explores the entire design matrix space by evaluating permutations of the matrix. This causes our algorithms, especially random selection, to have long runtimes when a large number of iterations is performed. For certain fixed design assumptions, such as the diagonal \\boldsymbol{G} matrix and \\boldsymbol{R} matrix we used previously, an algorithm that optimizes based on matrix structure could improve efficiency in such cases.\n\n\n5.1.2 Balancing multi-objective optimization\nWe aim to optimize A while avoiding bad NB and ED statistics, which essentially represents a multi-objective optimization problem. In our current approach, we incorporate filtering steps during random search permutation generation and SA neighbour generation to ensure that NB and ED improve gradually throughout the iterations. Although this approach allows for the stepwise optimization of all three statistics, it fails to bing in the correlations among them and lacks the ability to balance and trade off between the three. Using a multi-objective optimization method would allow us to place these three statistics on an equal priority level. In our current approach, however, the A-value has consistently been the primary variable driving changes in the design matrix.\n\n\n5.1.3 Limitations and generalizability in practical applications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  }
]