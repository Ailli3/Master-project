[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANU Thesis",
    "section": "",
    "text": "Preface\nThis thesis template is intended for honours, masters or PhD students at the Australian National University (ANU) who wish to write their thesis using the Quarto document format. It is highly recommended for students who code using Python, R or Julia and have many computational or analysis results in their thesis.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#benefits",
    "href": "index.html#benefits",
    "title": "ANU Thesis",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using Quarto document include:\n\nIt allows you to write your thesis in a simple markup language called Markdown. This means that you can focus on writing your thesis without having to worry about formatting.\nThe document can be output to a variety of formats including PDF, HTML, Word, and LaTeX.\nCode can be easily embedded in the document and executed. This means that you can include the results of your analysis in your thesis without having to manually copy and paste them. This is a good reproducible and scientific practice.\nYou can easily integrate with aspects of GitHub (edit, reporting an issue, etc).\n\nThe above outlined benefits can also be considered as best practice. Version controlling and collaborative writing (via Git and GitHub) are important in managing multiple versions of your thesis and in collaborating with your supervisory team. Embedding code in your thesis is a good practice in reproducible research. Making your thesis in HTML format can allow for interactive web elements to be embedded while PDF format can be for general distribution and printing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ANU Thesis",
    "section": "Getting started",
    "text": "Getting started\nThere are several systems that you are expected to know to use this template. These include:\n\nMarkdown syntax for writing\nQuarto or R Markdown syntax (note that these works for Python or Julia too) for embedding code\n(Optional) Git and GitHub for hosting",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#frequently-asked-questions",
    "href": "index.html#frequently-asked-questions",
    "title": "ANU Thesis",
    "section": "Frequently asked questions",
    "text": "Frequently asked questions\n\nWhat about Overleaf?\nANU has a professional account for Overleaf, which is great for those that use LaTeX regularly. Unfortunately, there is no equivalent system with track changes in Quarto. You can output the tex file from Quarto document and use this in Overleaf. The changes made in this tex document however has to be manually transferred back to the Quarto document. If your main output is mainly mathematical and you have little to no code outputs, Overleaf is probably a better choice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "This thesis is composed of my original work, and contains no material previously published or written by another person except where due reference has been made in the text. I have clearly stated the contribution by others to jointly-authored works that I have included in my thesis.\nI have clearly stated the contribution of others to my thesis as a whole, including statistical assistance, study design, data analysis, significant technical procedures, professional editorial advice, financial support and any other original research work used or reported in my thesis. The content of my thesis is the result of work I have carried out since the commencement of my higher degree by research candidature and does not include a substantial part of work that has been submitted to qualify for the award of any other degree or diploma in any university or other tertiary institution. I have clearly stated which parts of my thesis, if any, have been submitted to qualify for another award.\n\nYour Name\n2024-01-24\n\n\nPublications\nAccepted or in-press publication in this thesis.\nSubmitted manuscripts included in this thesis.\nOther publications during candidature.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "I would like to express my sincere gratitude to my dog, Chuckles, for eating my research notes multiple times. If it wasn’t for you, I would have finished this thesis earlier.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Reproducible research is an essential paradigm that promotes the idea that scientific investigations should be transparent, verifiable, and accessible to others. In an era where the scientific community faces concerns about the replicability of research findings, adopting reproducible research practices becomes imperative.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Method",
    "section": "",
    "text": "This report underscores the transformative impact of reproducible research on the scientific landscape, promoting a commitment to openness and accountability for the benefit of the entire research community.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ. 2023. Quarto: R Interface to ’Quarto’ Markdown\nPublishing System. https://CRAN.R-project.org/package=quarto.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Appendix A — Tools",
    "section": "",
    "text": "This thesis was written using Quarto 1.4.545 (Allaire 2023) and the following system and R packages:\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.2.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Australia/Sydney\n date     2024-01-24\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.0)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.1)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.1)\n later         1.3.2   2023-12-06 [1] CRAN (R 4.3.1)\n processx      3.8.3   2023-12-10 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.0)\n quarto        1.3     2023-09-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.3.1)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.1)\n rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n xfun          0.41    2023-11-01 [1] CRAN (R 4.3.1)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.1)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nAllaire, JJ. 2023. Quarto: R Interface to ’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Linear model\nSuppose we have a linear model,\n\\boldsymbol{y}=\\mathbf{X}\\boldsymbol{\\tau} + \\boldsymbol{\\epsilon} \\tag{2.1} where \\boldsymbol{y} is n\\times 1 vector of n observations, \\boldsymbol{\\tau} is a t\\times 1 vector of fixed effects, \\boldsymbol{\\epsilon} is the n\\times 1 vector for error, and \\mathbf{X} is a design matrix has size n\\times t. We assume that \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I}_{n}) and hence \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n).\nThe log-likelihood of Equation 2.1 is then given as:\n\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) = -\\frac{n}{2}\\log(2\\pi)-n\\log(\\sigma)-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}).\n The (i,j)-th entry of the Fisher information matrix is defined as\nI_{ij}(\\boldsymbol{\\tau})=-\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\tau_i\\partial\\tau_j}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)\n where \\tau_i is the i-th entry of \\boldsymbol{\\tau}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#linear-model",
    "href": "background.html#linear-model",
    "title": "2  Background",
    "section": "",
    "text": "Lemma 2.1 The Fisher information matrix of Equation 2.1 is given as \n\\mathbf{C} = -\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\n\nProof. The second derivative of the log-likelihood function \\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) is the Hessian matrix. We have \n\\frac{\\partial}{\\partial\\boldsymbol{\\tau}}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})\n and for second derivative is \n\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})==-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n And in linear model assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n) and the Fisher information matrix is unbiased because, in the expectation calculation process, we do not involve the randomness of \\boldsymbol{y}. The Fisher information matrix is actually determined by the design matrix \\boldsymbol{X} and the error variance \\sigma^2. Hence \n\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X} = -\\mathbf{C}\n So \\mathbf{C} = \\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\nLemma 2.2 The variance of the fixed effects for Equation 2.1 is equivalent to the inverse of the Fisher information matrix, i.e. var(\\hat{\\boldsymbol{\\tau}})=\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1} = \\mathbf{C}^{-1}.\n\n\nProof. We know that the MLE of \\boldsymbol{\\tau} in a linear model is \\hat{\\boldsymbol{\\tau}}=(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}. By assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n). So \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}).So we have var(\\hat{\\boldsymbol{\\tau}}) = \\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}, which is exactly the inverse of Fisher information matrix \\mathbf{C}^{-1}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#a-criterion",
    "href": "background.html#a-criterion",
    "title": "2  Background",
    "section": "3.1 A-criterion",
    "text": "3.1 A-criterion\nWe first start with a simple example, that is, we consider treatment factors \\boldsymbol{\\tau} are fixed, to elucidate the influence of A-criterion.\nFrom David Butler (2013), we conduct a maximum log likelihood by following objective function: \n\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R})+\\log f_u(\\boldsymbol{u};\\textbf{G})\n basing on assumption, we have \\boldsymbol{y}=\\textbf{X}\\boldsymbol{\\tau}+\\textbf{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(\\textbf{X}\\boldsymbol{\\tau},\\textbf{R}+\\textbf{ZGZ}^\\top). So for objective function, we can write out the distributions\n\n\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R}\\sim N(\\textbf{X}\\boldsymbol{\\tau}+\\textbf{Z}\\boldsymbol{u},\\textbf{R}) \\\\\n\\boldsymbol{u}\\sim N(\\boldsymbol{0},\\textbf{G})\n\n\nLemma 3.1 So log of joint density is given as\n\\begin{align*}\n\\mathscr{L}&=\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R})+\\log f_u(\\boldsymbol{u};\\textbf{G})\\\\\n&=-\\frac{1}{2}\\left(\\log|\\textbf{R}|+\\log|\\textbf{G}|+(\\boldsymbol{y}-\\textbf{X}\\boldsymbol{\\tau}-\\textbf{Z}\\boldsymbol{u})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\textbf{X}\\boldsymbol{\\tau}-\\textbf{Z}\\boldsymbol{u})+\\boldsymbol{u}^\\top\\textbf{G}^{-1}\\boldsymbol{u}\\right)\n\\end{align*}\n\n\nProof. Where is the proof?\n\nWe determine that \\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{\\tau}}=\\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{u}}=\\boldsymbol{0}, and write the equation into a matrix form \n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+ \\textbf{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\boldsymbol{y}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n\nLet \n\\textbf{C}=\n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+ \\textbf{G}^{-1}\n\\end{bmatrix}\n\\quad\n\\hat{\\boldsymbol{\\beta}}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\n\\quad\n\\textbf{W}=\\begin{bmatrix}\\textbf{X} &\\textbf{Z}\\end{bmatrix}\n\nRobinson (1991) shows that \n\\hat{\\boldsymbol{\\beta}}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\\sim\nN(\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}, \\textbf{C}^{-1})\n\nWe can rewrite the equation in a standard form \\textbf{C}\\hat\\beta=\\textbf{W}^\\top\\textbf{R}^{-1}y. So \\textbf{C} is the Fisher information matrix for this linear mixed model. As we mentioned above, we are interesting in examine fixed effect part \\boldsymbol{\\tau}. For the properties of variance-covariance matrix \\textbf{C}^{-1} can be written as \n\\textbf{C}^{-1}=\n\\begin{bmatrix}\n\\textbf{C}^{11} & \\textbf{C}^{12}\\\\\n\\textbf{C}^{21} & \\textbf{C}^{22}\n\\end{bmatrix}\n \\textbf{C}^{11} is a t \\times t matrix and variance-covariance matrix for treatment factors. We can caluclate \\textbf{C}_{11} by writing out standard form for \\boldsymbol{\\tau} and cancelling \\boldsymbol{u}.We have \n\\boldsymbol{X}^\\top\\textbf{R}^{-1}\\textbf{X}\\boldsymbol{\\tau}+\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})\\textbf{Z}^\\top\\textbf{R}^{-1}y=\\textbf{X}^\\top\\textbf{R}^{-1}y\\\\\n \n\\Rightarrow \\textbf{X}^\\top[\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}]\\textbf{X}\\boldsymbol{\\tau}=\\textbf{X}^\\top[\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}]y\\\\\n \n\\Rightarrow \\textbf{X}^\\top\\textbf{P}\\textbf{X}\\boldsymbol{\\tau}=\\textbf{X}^\\top\\textbf{P}y\n where \\textbf{P}=\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}, let \\textbf{C}_{11}=\\textbf{X}^\\top\\textbf{P}\\textbf{X} then we have the standard form for \\hat{\\boldsymbol{\\tau}}, which is \\textbf{C}_{11}\\boldsymbol{\\tau}=\\textbf{X}^\\top\\textbf{P}y, and \\textbf{C}_{11} is the corresponding Fisher information matrix.\nSo we have \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\textbf{C}_{11}^{-1}).To examine a specific form of \\boldsymbol{\\tau}, we do linear transform on \\boldsymbol{\\tau}: \\hat{\\boldsymbol{\\pi}}=\\textbf{D}\\hat{\\boldsymbol{\\tau}}, where \\textbf{D} is some transform matrix, so we have \\textbf{D}(\\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{\\pi}-\\hat{\\boldsymbol{\\pi}}\\sim N(0,\\textbf{D}\\textbf{C}_{11}^{-}\\textbf{D}^\\top). We denote \\boldsymbol{\\Lambda}=\\textbf{D}\\textbf{C}_{11}^{-}\\textbf{D}^\\top.\nA-criterion is the mean of predicted error variance of the parameter. i.e.  \n\\mathscr{A}=\\frac{1}{n_{\\pi}(n_{\\pi}-1)}\\sum_{i}\\sum_{j&lt;i} predicted\\quad error\\quad variance\\quad of\\quad(\\hat\\pi_i-\\hat\\pi_j)\n where n_{\\pi} is the row number of vector \\pi.For error variance part, we have \n\\sum_{i}\\sum_{j&lt;i} predicted\\quad error\\quad variance\\quad of\\quad(\\hat\\pi_i-\\hat\\pi_j)=\\sum_{i}\\sum_{j&lt;i}var(\\hat\\pi_i-\\hat\\pi_j)=\\sum_{i}\\sum_{j&lt;i}[var(\\hat\\pi_i)+var(\\hat\\pi_j)-2cov(\\hat\\pi_i,\\hat\\pi_j)]\n from virance-covirance matrix \\boldsymbol{\\Lambda}, we can rewrite the sum part as n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}.So we have \n\\mathscr{A}=\\frac{1}{n_{\\pi}(n_{\\pi}-1)}[n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}]\n same result from DG Butler, Smith, and Cullis (2013)\nDerivation above indicate that \\mathscr{A}\\propto tr(\\boldsymbol{\\Lambda}), A-criterion as the mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix \\boldsymbol{\\Lambda} should be as small as possible. And this is our goal on optimal experimental design.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nRobinson, George K. 1991. “That BLUP Is a Good Thing: The Estimation of Random Effects.” Statistical Science, 15–32.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In the field of experimental design, efficient methods are crucial for ensuring accurate and reliable results. One such method is the row-column design, controlling variability in experiments involving two factors, typically arranged in rows and columns. The row-column design can be considered as an extension of the Latin square design with more flexibility, allowing for different numbers of rows, columns, and treatments. This flexibility makes row-column designs applicable to a wider range of experimental settings.\nTo offer a row-column design that gives a precise estimation of treatment effects, one way is to seek the optimal value of some statistic criteria, for example, A-criteria(links to sections), minimizing the variance of elementary treatment contrasts. Using linear mixed model and assuming fixed treatment effects and random blocking effect, Butler (2013) has show the relation between optimizing design and minimizing the value of A-criteria, and show some possible algorithms to search optimal design in feasible set. These algorithms mainly focus on comparing the arrangements of different treatments, that is, doing permutations, and calculating their A values, optimizing design by iterations.\nHowever, some undesired cluster of replications or some treatment may occur when algorithm are doing permutations along rows and columns. Piepho, Michel, and Williams (2018) found that such clustering is considered undesirable by experimenters who worry that irregular environmental gradients might negatively impact multiple replications of the same treatment, potentially leading to biased treatment effect estimates. Williams emphasis that there is a need to design a strategy to avoid clustering and achieve even distribution of treatment replications among the experimental field. Two properties of design are introduced. Even distribution of treatment replications, abbreviated as ED, and neighbor balance, abbreviated as NB. A good ED ensures every replications of a treatment are widely spread in experimental field, and NB helps to avoid replications of the some treatment cluster together repeatedly. Williams introduce a scoring system to analysis ED and NB for a specific design, and introduce a algorithm to optimize ED, NB and some average efficiency factor can be represented by a specific statistic criteria.(maybe saying some improvement is needed)\nWe offer an optimization strategy for a design problem, which we can improving ED and NB during optimizing statistic criteria for a design, and avoid unwanted clustering and self-adjacency on the resulting design.In this algorithm, we use A-criteria to evaluate the efficiency of a design. Before the algorithm, we randomly generate a design as an initial design, and calculate the A-criteria as initial value. We update design by selecting a better among its neighbors. The neighbors are pair-wise permutations of a design. Typically, we select a neighbor from all pairwise permutations of a design for iteration, but this does not ensure ED and NB. To ensure ED and NB during optimization, we need to add some constraints when generating the pairwise permutations.(maybe explain what is the constraints) By filtering design with bad ED and NB, we then optimize the statistic criteria of the design.\n(sections in the thesis)\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "background.html#linear-mixed-model",
    "href": "background.html#linear-mixed-model",
    "title": "2  Background",
    "section": "2.2 Linear mixed model",
    "text": "2.2 Linear mixed model\nLinear mixed model extends linear model by incorporating additionally incorporating random effects into the model that effectively give greater flexibility and capability to incorporate known correlated structures into the model. We now consider a linear mixed model \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\n\\tag{2.2} here \\boldsymbol{y} is n\\times 1 vector for n observations, \\boldsymbol{\\tau} is a t\\times1 parameter vector of treatment factors, \\boldsymbol{u} is a q \\times1 parameter vector of blocking effects, and \\boldsymbol{\\epsilon} is the n\\times 1 error vector, \\boldsymbol{X} and \\boldsymbol{Z} are design matrices of dimension n \\times t and n \\times q for treatment factors and blocking factors, respectively. We here assume blocking factors are random effect, with random error \\boldsymbol{\\epsilon} we have \n\\begin{bmatrix}\n\\boldsymbol{u} \\\\\n\\boldsymbol{\\epsilon}\n\\end{bmatrix}\n\\sim\nN\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n\\boldsymbol{0}\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n\\boldsymbol{G} & \\mathbf{0} \\\\\n\\mathbf{0} & \\boldsymbol{R}\n\\end{bmatrix}\n\\right),\n where \\boldsymbol{G} is the q \\times q variance matrix for \\boldsymbol{u} and \\boldsymbol{R} is n\\times n variance matrix for \\boldsymbol{\\epsilon}.\n\n2.2.1 A-criterion\nWe first start with a simple example, that is, we consider treatment factors \\boldsymbol{\\tau} are fixed, to elucidate the influence of A-criterion.\nbasing on assumption, we have \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau},\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\n\\tag{2.3} So for objective function, we can write out the distributions\n\n\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R}) \\\\\n\\boldsymbol{u}\\sim N(\\boldsymbol{0},\\boldsymbol{G})\n\nWe want to give a precise estimation on \\boldsymbol{\\tau}. As we mentioned, we have the distribution for response variable \\boldsymbol{y}\\sim We can use generalized least squares(GLS) by rewrite the model as: \n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\tau} + \\zeta\n Here \\zeta = \\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(0, \\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top). Henderson (1975) shows that the GLS estimation of \\boldsymbol{\\tau} is any solution to \n\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X}\\hat{\\boldsymbol{\\tau}}_{gls}=\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\n Here \\boldsymbol{V}=\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top. So \\hat{\\boldsymbol{\\tau}}_{gls} = (\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\nHenderson et al. (1959) emphasis that computing matrix \\boldsymbol{V} which is often large is difficult. So here we use joint log likelihood.\nFrom David Butler (2013), we conduct a maximum log likelihood by following objective function: \n\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\n :::{#lem-joint-density-lmm} So log of joint density is given as\n\\begin{align*}\n\\mathscr{L}&=\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\\\\\n&=-\\frac{1}{2}\\left(\\log|\\boldsymbol{R}|+\\log|\\boldsymbol{G}|+(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}\\right)\n\\end{align*}\n:::\n\nProof. We have density function for \\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R})\n\nf_y = \\frac{1}{\\sqrt{(2\\pi)^{n}|\\boldsymbol{R}|}}exp(-\\frac{1}{2}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u})))\n And density function for \\boldsymbol{u} \nf_u = \\frac{1}{\\sqrt{(2\\pi)^{l}|\\boldsymbol{G}|}}exp(-\\frac{1}{2}\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u})\n Ignoring constant part, we have \n\\log f_y=-\\frac{1}{2}[\\ln |\\boldsymbol{R}|+(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))]\n \n\\log f_u = -\\frac{1}{2}[\\ln |\\boldsymbol{G}+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}]\n So we have our log of joint density function.\n\nWe determine that \\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{\\tau}}=\\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{u}}=\\boldsymbol{0}, and write the equation into a matrix form \n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n\\tag{2.4}\nLet \n\\boldsymbol{C}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\quad\n\\hat{\\boldsymbol{\\beta}}_{llm}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n\\quad\n\\boldsymbol{W}=\\begin{bmatrix}\\boldsymbol{X} &\\boldsymbol{Z}\\end{bmatrix}\n By cancelling \\boldsymbol{u}, we have \n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}y=\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n where \\boldsymbol{P}=\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}, let \\boldsymbol{C}_{11}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X} then we have the form similar to GLS estimation for \\hat{\\boldsymbol{\\tau}}, which is \n\\boldsymbol{C}_{11}\\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n and the estimation of \\boldsymbol{\\tau} is \\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{C}_{11}^{-1}\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}, which is equivalent with GLS estimation\n\nProof. We only need to prove that \\boldsymbol{P}=\\boldsymbol{V}^{-1}. \\begin{align*}\n\\boldsymbol{P}\\boldsymbol{V} &=(\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})(\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top - \\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad\\quad\\quad -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top(\\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{Z}^\\top+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{I}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G})\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{G}^{-1}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z})\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top-\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n& = \\boldsymbol{I}\n\\end{align*} So \\hat{\\boldsymbol{\\tau}}_{llm} and \\hat{\\boldsymbol{\\tau}}_{gls} are equivalent, and we denote them as \\hat{\\boldsymbol{\\tau}}\n\nNow we have our estimation for the treatment factor, and experimental design aims to further refine our design by focusing on the precision of these estimates. Specifically, we aim to optimize the design so that the treatment effects are estimated with minimal variance, ensuring that the differences between any two treatment levels are as small as possible. To achieve this, we introduce the A-value as a criterion for evaluating the design.\n\nDefinition 2.1 Basing on the model formula Equation 2.2, and a estimation of treatment factor \\hat{\\boldsymbol{\\tau}} has n_{\\tau} factors. A-criterion measure the average predicted error variance of different treatments. Let V_{ij}= var(\\hat{\\tau}_i-\\hat{\\tau}_j)=var(\\hat{\\tau}_i)+var(\\hat{\\tau}_j)-2cov(\\hat{\\tau}_i,\\hat{\\tau}_j), and a A-value \\mathscr{A} is \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n\\tag{2.5}\n\nTo discover the relationship between the A-value and the design matrix, I need to find the variance-covariance matrix of \\hat{\\boldsymbol{\\tau}}. In fact, it can be proof that \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).\n(lemma and proof)\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)(\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1})^\\top)\nWe denote \\begin{align*}\n\\boldsymbol{M} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{N} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z}\\\\\n\\boldsymbol{J} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{K} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1}\\\\\n\\end{align*} In the context of row-column design, the \\boldsymbol{K} matrix is invertible. Schur complement of \\boldsymbol{K} is \n\\boldsymbol{S} = \\boldsymbol{M} - \\boldsymbol{N} \\boldsymbol{K}^{-1} \\boldsymbol{J}\n And the inverse matrix of \\boldsymbol{C} can be written as \n\\boldsymbol{C}^{-1}=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{S}^{-1} & -\\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1} \\\\\n-\\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} & \\boldsymbol{K}^{-1} + \\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1}\n\\end{bmatrix}\n\\tag{2.6} So from Equation 2.4 we have \n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}} \\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{U}_1\\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\\boldsymbol{y}\n Here \\begin{align*}\n&\\boldsymbol{U}_1 = \\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\\\\n&\\boldsymbol{U}_2 = \\boldsymbol{C}^{21}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{22}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{align*}\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top)\nAnd we have following results\n\\begin{align*}\n\\begin{bmatrix}\n\\boldsymbol{U}_1 \\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n(\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1}\n\\end{bmatrix}-\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & \\boldsymbol{G}^{-1}\n\\end{bmatrix})\\\\\n&=\n\\boldsymbol{I}-\n\\begin{bmatrix}\n0 & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & -\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{I} & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & \\boldsymbol{I}-\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\end{align*}\nSo we have \\begin{align*}\n\\boldsymbol{U}_1\\boldsymbol{X}&=\\boldsymbol{I}\\\\\n\\boldsymbol{U}_1\\boldsymbol{Z}&=-\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\\\\n\\end{align*} For the variance of estimation we have \\begin{align*}\nvar(\\hat{\\boldsymbol{\\tau}})\n&=\n\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top\\\\\n&= \\boldsymbol{U}_1\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&= (\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&=\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\boldsymbol{G}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}-\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top+\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}\n\\end{align*}\nWhat is \\boldsymbol{C}^{11}? what is the relation between \\boldsymbol{C}_{11} and \\boldsymbol{C}^{11}? From Equation 2.6 we have \\boldsymbol{C}^{11}=\\boldsymbol{S}^{-1} and \\boldsymbol{S} is \n\\boldsymbol{S} = \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\n And base on the complement of \\boldsymbol{C}_{11}, we rewrite the \\boldsymbol{S} \n\\begin{align*}\n\\boldsymbol{S}\n&=\\boldsymbol{X}^\\top(\\boldsymbol{R}^{-1} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1})\\boldsymbol{X}\\\\\n&=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\\\\n&=\\boldsymbol{C}_{11}\n\\end{align*}\n So var(\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{C}^{11}=\\boldsymbol{C}_{11}^{-1}.\nSo we have \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).To examine a specific form of \\boldsymbol{\\tau}, in general case, we do linear transform on \\boldsymbol{\\tau}: \\hat{\\boldsymbol{\\pi}}=\\boldsymbol{D}\\hat{\\boldsymbol{\\tau}}, where \\boldsymbol{D} is some transform matrix, so we have \\boldsymbol{D}(\\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{\\pi}-\\hat{\\boldsymbol{\\pi}}\\sim N(0,\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top). We denote \\boldsymbol{\\Lambda}=\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top. If \\boldsymbol{D} is identical matrix \\boldsymbol{I}, then \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1} and \\hat{\\boldsymbol{\\pi}}=\\hat{\\boldsymbol{\\tau}}.\nWe know that A-criterion is the mean of predicted error variance of the parameter from Equation 2.5 i.e.  \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n Having variance-covariance matrix \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1}, we can rewrite the sum part as n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}.So we have \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}[n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}]\n\\tag{2.7} same result from DG Butler, Smith, and Cullis (2013)\nDerivation above indicate that \\mathscr{A}\\propto tr(\\boldsymbol{\\Lambda}), A-criterion as the mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix \\boldsymbol{\\Lambda} should be as small as possible. And this is our goal on optimal experimental design.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#neighbor-balance-and-even-distribution",
    "href": "background.html#neighbor-balance-and-even-distribution",
    "title": "2  Background",
    "section": "2.3 Neighbor balance and even distribution",
    "text": "2.3 Neighbor balance and even distribution\n\n2.3.1 Concepts of NB and ED\nPiepho, Michel, and Williams (2018) emphasis the the concepts of neighbor balance and even distribution are crucial to mitigating biases and ensuring the reliability of results in row-column design.\nNeighbor balance (NB) refers to the principle that, in a row-column experimental design, the frequency with which two treatments are adjacent or near each other should not be excessively high. High adjacency frequency between two treatments can lead to mutual influence, which may cause bias to the experimental results. For example, if the effect of one treatment can spread to neighboring areas, frequent adjacency could interfere with accurate measurement of each treatment’s true effect next to it. Therefore, it is essential to control the adjacency frequency of different treatments to prevent high adjacency for two specific treatments.\nEven distribution(ED) aims to ensure that different replications of the same treatment are widely distributed across the experimental field, rather than being clustered in a specific area. This strategy helps to avoid biases caused by specific environmental conditions in certain parts of the experiment field. If replications of one treatment are over concentrated in one area, unique environmental factors in that area might affect the treatment’s performance, leading to biased observations. By evenly distributing replications, environmental interference can be minimized, so that we can enhance the reliability of the experimental results.\n(maybe some example plots or pictures)\n\n\n2.3.2 Measuring NB and ED\n\n2.3.2.1 Evaluating NB with adjacency matrix\nIn Piepho, Michel, and Williams (2018), there is a assumption that they are optimizing a binary design, which means each treatment appears only once in each row and column. Under this assumption, their balancing mechanism considers diagonal adjacency, Knight moves, and even more distant points to ensure an optimal balance. In my optimization process, I begin with a randomly selected design matrix. Consequently, my approach considers not only diagonal adjacency but also the adjacent points directly above, below, to the left, and to the right.\nI use an adjacency matrix to count the number of times each treatment is adjacent to another. This matrix serves as a crucial tool in my optimization process, enabling precise tracking and adjustment of treatment placements to achieve neighbor balance.\nWe denote the adjacency matrix as \\boldsymbol{A}, and for treatment i and j in treatment set T \\boldsymbol{A}_{ij} represents the count of times treatment i is adjacent to treatment j. Here “adjacent” means treatment j is located next to treatment i (maybe a picture to show it)\nFor Given design \\mathcal{D} and \\mathcal{D}_{r,c} represents the treatment at row r and column c. So \\boldsymbol{A}_{ij} can be expressed as:\n\n\\boldsymbol{A}_{ij}=\\sum_{r=1}^{R}\\sum_{c=1}^{C}I_{r,c}(i) F_{r,c}(j)\n where \nF_{r,c}(j)=\n\\sum_{m \\in \\{-1,0,1\\}}\\sum_{n \\in \\{-1,1\\}}I_{r+m,c+n}(j)+\\sum_{m \\in \\{-1,1\\}}I_{r+m,c}(j)\n R and C are total number of rows and columns and I_{r,c}(\\cdot) is the indicator function, which takes value under following cases \nI_{r,c}(i)=\n\\begin{cases}\n1 & \\text{if } \\mathcal{D}_{r,c}=i \\\\\n0 & \\text{if } \\mathcal{D}_{r,c}\\neq i & \\text{or } r&lt;1,r&gt;R,c&lt;1,c&gt;C\\\\\n\\end{cases}\n\nThe function F_{r,c}(j) here is actually counting the times that treatment j occurs at places around the position row r and column c.\nWe measure NB by taking the maximum of the elements in adjacency matrix \\boldsymbol{A}. Our NB criteria is \nC_{NB}=max\\{\\boldsymbol{A}_{ij}\\} \\quad i,j\\in T\n\n\n\n2.3.2.2 Evaluating ED with minimum row and column span\nThe goal of evaluating the evenness of distribution (ED) is to find the row and column spans for treatments across the entire design matrix. We would like this value as large as possible This ensures that the treatments t\\in T is distributed as evenly as possible within the rows and columns, reducing clustering and promoting a balanced design.\nThe row span for a given treatment t \\in T is defined as the difference between the maximum and minimum row indices where t appears in experiment field. \nRS(t)=max\\{r:\\mathcal{D}_{r,c}=t\\}-min\\{r:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n And the minimum row span of a design \\mathcal{D} is \nMRS(\\mathcal{D})=min\\{RS(t)\\},\\quad t \\in T\n Same for column span \nCS(t)=max\\{c:\\mathcal{D}_{r,c}=t\\}-min\\{c:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n \nMCS(\\mathcal{D})=min\\{CS(t)\\},\\quad t \\in T\n So, for the changes in the design matrix \\mathcal{D} during the search process, we tend to accept only those changes where the Minimum Row Span (MRS) and Minimum Column Span (MCS) remain the same or become smaller.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nHenderson, Charles R. 1975. “Best Linear Unbiased Estimation and Prediction Under a Selection Model.” Biometrics, 423–47.\n\n\nHenderson, Charles R, Oscar Kempthorne, Shayle R Searle, and CM Von Krosigk. 1959. “The Estimation of Environmental and Genetic Trends from Records Subject to Culling.” Biometrics 15 (2): 192–218.\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Modeling row-column design\nAs mentioned in previous chapter, we use a linear mixed model (LMM) to model the row-column design having two distinct sources of variation, typically referred to as “row” and “column” factors. This design structure appears frequently in agricultural and industrial trials, where treatments are applied across units organized in a grid-like pattern, and both row and column effects may influence the outcomes.\nIn my assumptions, the row and column effects are treated as random effects, which means that they are random factors for spatial or systematic factors across different rows and columns of the experiment field. The treatment effects, on the other hand, are treated as fixed effects because they represent the primary factors of interest that we wish to evaluate in terms of their influence on the response variable.\nRecalling Equation 2.2, the treatment effects are modeled as fixed effects, represented by the treatment design matrix \\boldsymbol{X} with parameter vector \\boldsymbol{\\tau}, measuring the influence of each treatment on the response variable. The matrix \\boldsymbol{X} is constructed such that each row corresponds to an experimental unit, and indicators in each column indicates whether a treatment is applied or not. Detailed structure will be shown in section(?????)\nThe random effects are modeled through the matrix \\boldsymbol{Z} and parameter vector \\boldsymbol{u}. Matrix \\boldsymbol{Z} is designed to capture the row and column structure of the experimental field, in which entries represent the position of each experimental unit located in some specific rows and columns. The parameters in vector \\boldsymbol{u} are corresponding row and column effects. They are assumed to follow a normal distribution with mean zero and variance-covariance matrix \\boldsymbol{G}.\nWith \\boldsymbol{y} as the vector of observed responses and \\boldsymbol{\\epsilon} as error term, a row-column design can be modeled by Equation 2.2 . where \\boldsymbol{X\\tau} represents the fixed treatment effects and \\boldsymbol{Zu} captures the random variations basing on rows and columns.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#modeling-row-column-design",
    "href": "methods.html#modeling-row-column-design",
    "title": "3  Methods",
    "section": "",
    "text": "3.1.1 Random effects matrix\nThe design matrix for the random effects, which is row and column in my linear mixed model follows a binary indicator structure. For example, we a have a 4\\times 4 experiment field, having 4 rows, 4 columns and 16 units. Then random effects matrix should be a 16 \\times 8 matrix containing binary indicator as the one presented. \n\\begin{bmatrix}\n\\begin{array}{cccc|cccc}\n1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\\\\n\\end{array}\n\\end{bmatrix}\n Each row corresponds to a specific experimental unit, while the columns represent the row and column factors in the experimental layout.In this matrix, the first set of columns represents the column effects, while the second set of columns represents the row effects. Each entry in this matrix is binary, where a value of 1 indicates that the experimental unit belongs to a specific row or column, and a 0 indicates otherwise. For example, the first row of the matrix has a 1 in both first and fifth columns, meaning that the corresponding unit of it is in the first column and the first row. This structure ensures that each unit is uniquely associated with one row and one column, and we can model the random effects accordingly.\nIn a more general case, suppose we have a m\\times n row-column experiment field. We should have a random effect matrix with mn rows and m+n columns with binary numbers. In this paper, we assume that the row effects and column effects are independent with each other. However, in more complex experimental design cases, they may be potentially correlated. The design of the random effects matrix, which separates the row and column effects as independent variables, simplifies the modeling process and the analysis of potential correlations between these effects in more advanced settings. This structure allows for easier identification and analysis of interactions between row and column effects, making the model flexible and adaptable to different levels of complexity in experimental designs.\n\n\n3.1.2 Design matrix for treatments\nThe design matrix for the treatment effects is constructed to capture the influence of each treatment on the response variable. In a row-column experimental design, each experimental unit is assigned a specific treatment.The entries in design matrix represents these assignments using binary indicators. Like random effects matrix each row in the matrix corresponds to an experimental unit, while each column represents a different treatment. Here we still use 4\\times 4 experiment field as example, and suppose we have 4 different treatments for each have 4 replications, needing 16 experiment unite. An example design matrix \\boldsymbol{X} for treatments should be a 16\\times 4 matrix with binary indicators as shown below\n\n\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n\\end{bmatrix}\n For a given experimental unit, that is a given row, the design matrix contains a 1 in the column corresponding to the treatment applied to that unit, and 0 elsewhere.This structure allows for a clear and efficient representation of which treatment is applied to each unit. For example, the first row of the example design matrix represents the first treatment is applied in the unit locating on the first column, first row.\nif there are t treatments and N experimental units, the design matrix will have N rows and t columns. Then the design matrix for treatment \\boldsymbol{X} with size N\\times t, should satisfy that for any row n_{i} \n\\sum_{j=1}^{t} \\boldsymbol{X}_{n_{i},j}=1\n That is, there is only one treatment can be applied in each experiment unit. And for any treatment t_j with r_j replications, it has \n\\sum_{i=1}^{N}\\boldsymbol{X}_{i,t_j}=r_j\n All replications of a treatment are applied in experimental field.\n\n\n3.1.3 Assumptoins for A-value calculation\nIt is important to clarify the key assumptions made in this study before calculating A value for a row-column design. Recalling Equation 2.7, for calculating A value we need the covariance matrix for the random effects, matrix \\boldsymbol{G}, covariance matrix for the error term, matrix \\boldsymbol{R}, transformation matrix \\boldsymbol{D}, random effect matrix \\boldsymbol{Z} and treatment design matrix \\boldsymbol{X}. We need some basic setup for these matrices.\nAssume that we now have a row-column matrix with R rows, C columns and RC plots.\nFor the covariance matrix for the random effects, matrix \\boldsymbol{G}, which captures the variability introduced by the row and column effects. I assume it is a (R+C)\\times(R+C) diagonal matrix, that is, it has following form, \n\\boldsymbol{G}_{diag} = \\sigma_{G}^2\\boldsymbol{I}_{(R+C)}\n \\boldsymbol{I}_{(R+C)} is a (R+C)\\times(R+C) identity matrix. And \\sigma_{G} is a scale constant. This means that the influences of the rows and columns are independent of each other, meaning there is no correlation between row and column effects in the design.\nSimilarly, for the covariance matrix for the error term, matrix \\boldsymbol{R}, I assume that it is also diagonal, indicating that the residual errors are uncorrelated across different experimental units. In this case we have \n\\boldsymbol{R}_{diag} = \\sigma_{R}^2\\boldsymbol{I}_{RC}\n with identity matrix \\boldsymbol{I}_{RC} and scale constant \\sigma_{R}.\nButler (2013) have introduced linear transformations of the treatment parameter vector \\boldsymbol{\\tau} by using a transformation matrix \\boldsymbol{D}, which allows for the investigation of linear combinations of treatments. However, in this paper, I simplify the approach by setting \\boldsymbol{D} as the identity matrix \\boldsymbol{I}_{RC}. This means that we focus on the individual effects of the treatments rather than their linear combinations.\nThese assumptions makes the structure of the model becomes more straightforward, allowing us to concentrate on the direct estimation of treatment effects while maintaining independence among the random effects and error terms.\nIt’s important to note that in our design, the random effect matrix \\boldsymbol{Z} remains constant during the optimization process.This means that while the row and column effects are accounted for as random effects, their structure does not change.\nWith these assumptions in place, the A-value in the context of a row-column design dependents only on the treatment design matrix \\boldsymbol{X}. This means that the primary factor influencing the A-value is the distribution and arrangement of treatments within the design, and it directly impacts the variance of the treatment effect estimates. Therefore, optimizing the A-value under these assumptions becomes a problem of optimizing the treatment distribution in the design, ensuring that the treatments are arranged in such a way that the variance of the estimates is minimized.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#searching-strategy",
    "href": "methods.html#searching-strategy",
    "title": "3  Methods",
    "section": "3.2 Searching Strategy",
    "text": "3.2 Searching Strategy\nBefore introducing the details of the searching strategy, it is important to establish a solid foundation by proving that the minimum of the A-value exists. The existence of the minimum A-value implies that the A-value has a lower bound, ensuring that the process of iteration optimizing the experimental design is not endless. As we continue to search for smaller A-values, this guarantees that we can eventually stop when the A-value stabilizes or a sufficient number of iterations reached.\nThis allows us to conclude that we have found an optimal or near-optimal design. Therefore, the existence of this lower bound serves as a critical foundation for our iterative search, giving us confidence that the optimization will converge to a solution.\n\n3.2.1 Exitence of the minimum of A-value\nTo prove that the minimum of the A-value exists, we establish a objective function, that is, the A-value function A(\\boldsymbol{X}) maps design matrix for treatment \\boldsymbol{X} to its A-value. It is a function that maps the design space to \\mathbb{R}. \nA: \\Omega \\to \\mathbb{R}, \\quad \\boldsymbol{X} \\mapsto A(\\boldsymbol{X})\n Here \\Omega is the design space contains all possible design matrix \\boldsymbol{X}.\nNow we proof the existence of minimum value of \\boldsymbol{X}, where \\boldsymbol{X}\\in\\Omega.\n\nProof. Recalling Equation 2.7\n\n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}[n_{\\tau}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\tau}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\tau}}]\n This expression is well-defined for all valid design matrices \\boldsymbol{X}. So function A(\\boldsymbol{X}) is well-defined.\nThe A-value represents the average variance of the difference treatment effect estimates, and since variances are always positive, the A-value is naturally bounded from below by zero. So we have \nA(\\boldsymbol{X})\\geq0\n which implies that the A-criterion is bounded below.\nIn experimental design, the treatment design matrix \\boldsymbol{X} can take on a finite number of possible permutations, especially in practical row-column designs where the number of treatments and experimental units is fixed. In a finite search space, a lower bounded function has its minimum value.\n\n\n\n3.2.2 General stracture\nIn Piepho, Michel, and Williams (2018), the optimization of NB and ED was typically carried out under the assumption that the A-value was already optimal or fixed. This required identifying a set of solutions that maintained the A-value while improving the balance and distribution properties. In addition to assuming the A-value is fixed, another approach they used is to randomly select a design and then optimizing ED and NB.This process would be repeated multiple times, and the design with the best A-value will be selected.\n(images)\nThese approach separates the optimization of ED and NB from the A-value, while I try to merge these two process into one algorithm. I use pairwise permutation among treatments to change treatment design during iterations. And to avoid design with bad ED and NB, I am using some criteria to filter the permutation, only maintain or better properties are accepted. In this way, a row-column design that satisfies multiple optimization requirements is achieved.\n(images)\nBasing on optimal design search methods share a common set of features in exploring the design space given in Butler (2013), my searching method contains following part:\n\nA calculation method of objective criterion for a given design matrix \\boldsymbol{X}\nAn interchange policy to switch the design with in search space \\Omega\nAn acceptance policy for a new design.\nA stopping rule to terminate the search.\n\nThe criterion calculation part has already been discussed earlier. We will now introduce interchange policy of switching the design.\n\n\n3.2.3 Permutaions and filtering\nWe use permutations of the treatments to update the design matrix for treatment. We randomly select two different treatments and swap them within the design matrix during the permutation process, without making drastic changes.\nLet \\boldsymbol{X} be the current design matrix for treatments, where each row corresponds to an experimental unit, and each column represents a treatment. Suppose we randomly select two different treatments, t_i and t_j located in ith row and jth row respectively. \\boldsymbol{X}_{new} can be written as \n\\boldsymbol{X}_{new} = \\boldsymbol{X}\\boldsymbol{P}_{ij}\n with permutation matrix defined as \nP_{i j}=\\left[\n\\begin{array}{ccccccc}\n1 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & & \\vdots & \\vdots & & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & 0 & \\cdots & 1\n\\end{array}\\right]\n\n\n\n3.2.4 Random search\n\n\n3.2.5 Simulated annealing\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "background.html#neighbor-balance-and-eveness-of-distribution",
    "href": "background.html#neighbor-balance-and-eveness-of-distribution",
    "title": "2  Background",
    "section": "2.3 Neighbor balance and eveness of distribution",
    "text": "2.3 Neighbor balance and eveness of distribution\n\n2.3.1 Concepts of NB and ED\nPiepho, Michel, and Williams (2018) emphasis the the concepts of neighbor balance and even distribution are crucial to mitigating biases and ensuring the reliability of results in row-column design.\nNeighbor balance (NB) refers to the principle that, in a row-column experimental design, the frequency with which two treatments are adjacent or near each other should not be excessively high. High adjacency frequency between two treatments can lead to mutual influence, which may cause bias to the experimental results. For example, if the effect of one treatment can spread to neighboring areas, frequent adjacency could interfere with accurate measurement of each treatment’s true effect next to it. Therefore, it is essential to control the adjacency frequency of different treatments to prevent high adjacency for two specific treatments.\nEven distribution(ED) aims to ensure that different replications of the same treatment are widely distributed across the experimental field, rather than being clustered in a specific area. This strategy helps to avoid biases caused by specific environmental conditions in certain parts of the experiment field. If replications of one treatment are over concentrated in one area, unique environmental factors in that area might affect the treatment’s performance, leading to biased observations. By evenly distributing replications, environmental interference can be minimized, so that we can enhance the reliability of the experimental results.\n(maybe some example plots or pictures)\n\n\n2.3.2 Measuring NB and ED\n\n2.3.2.1 Evaluating NB with adjacency matrix\nIn Piepho, Michel, and Williams (2018), there is a assumption that they are optimizing a binary design, which means each treatment appears only once in each row and column. Under this assumption, their balancing mechanism considers diagonal adjacency, Knight moves, and even more distant points to ensure an optimal balance. In my optimization process, I begin with a randomly selected design matrix. Consequently, my approach considers not only diagonal adjacency but also the adjacent points directly above, below, to the left, and to the right.\nI use an adjacency matrix to count the number of times each treatment is adjacent to another. This matrix serves as a crucial tool in my optimization process, enabling precise tracking and adjustment of treatment placements to achieve neighbor balance.\nWe denote the adjacency matrix as \\boldsymbol{A}, and for treatment i and j in treatment set T \\boldsymbol{A}_{ij} represents the count of times treatment i is adjacent to treatment j. Here “adjacent” means treatment j is located next to treatment i (maybe a picture to show it)\nFor Given design \\mathcal{D} and \\mathcal{D}_{r,c} represents the treatment at row r and column c. So \\boldsymbol{A}_{ij} can be expressed as:\n\n\\boldsymbol{A}_{ij}=\\sum_{r=1}^{R}\\sum_{c=1}^{C}I_{r,c}(i) F_{r,c}(j)\n where \nF_{r,c}(j)=\n\\sum_{m \\in \\{-1,0,1\\}}\\sum_{n \\in \\{-1,1\\}}I_{r+m,c+n}(j)+\\sum_{m \\in \\{-1,1\\}}I_{r+m,c}(j)\n R and C are total number of rows and columns and I_{r,c}(\\cdot) is the indicator function, which takes value under following cases \nI_{r,c}(i)=\n\\begin{cases}\n1 & \\text{if } \\mathcal{D}_{r,c}=i \\\\\n0 & \\text{if } \\mathcal{D}_{r,c}\\neq i & \\text{or } r&lt;1,r&gt;R,c&lt;1,c&gt;C\\\\\n\\end{cases}\n\nThe function F_{r,c}(j) here is actually counting the times that treatment j occurs at places around the position row r and column c.\nWe measure NB by taking the maximum of the elements in adjacency matrix \\boldsymbol{A}. Our NB criteria is \nC_{NB}=max\\{\\boldsymbol{A}_{ij}\\} \\quad i,j\\in T\n\n\n\n2.3.2.2 Evaluating ED with minimum row and column span\nThe goal of evaluating the evenness of distribution (ED) is to find the row and column spans for treatments across the entire design matrix. We would like this value as large as possible This ensures that the treatments t\\in T is distributed as evenly as possible within the rows and columns, reducing clustering and promoting a balanced design.\nThe row span for a given treatment t \\in T is defined as the difference between the maximum and minimum row indices where t appears in experiment field. \nRS(t)=max\\{r:\\mathcal{D}_{r,c}=t\\}-min\\{r:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n And the minimum row span of a design \\mathcal{D} is \nMRS(\\mathcal{D})=min\\{RS(t)\\},\\quad t \\in T\n Same for column span \nCS(t)=max\\{c:\\mathcal{D}_{r,c}=t\\}-min\\{c:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n \nMCS(\\mathcal{D})=min\\{CS(t)\\},\\quad t \\in T\n So, for the changes in the design matrix \\mathcal{D} during the search process, we tend to accept only those changes where the Minimum Row Span (MRS) and Minimum Column Span (MCS) remain the same or become smaller.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nHenderson, Charles R. 1975. “Best Linear Unbiased Estimation and Prediction Under a Selection Model.” Biometrics, 423–47.\n\n\nHenderson, Charles R, Oscar Kempthorne, Shayle R Searle, and CM Von Krosigk. 1959. “The Estimation of Environmental and Genetic Trends from Records Subject to Culling.” Biometrics 15 (2): 192–218.\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  }
]