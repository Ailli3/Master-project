[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANU Thesis",
    "section": "",
    "text": "Preface\nThis thesis template is intended for honours, masters or PhD students at the Australian National University (ANU) who wish to write their thesis using the Quarto document format. It is highly recommended for students who code using Python, R or Julia and have many computational or analysis results in their thesis.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#benefits",
    "href": "index.html#benefits",
    "title": "ANU Thesis",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using Quarto document include:\n\nIt allows you to write your thesis in a simple markup language called Markdown. This means that you can focus on writing your thesis without having to worry about formatting.\nThe document can be output to a variety of formats including PDF, HTML, Word, and LaTeX.\nCode can be easily embedded in the document and executed. This means that you can include the results of your analysis in your thesis without having to manually copy and paste them. This is a good reproducible and scientific practice.\nYou can easily integrate with aspects of GitHub (edit, reporting an issue, etc).\n\nThe above outlined benefits can also be considered as best practice. Version controlling and collaborative writing (via Git and GitHub) are important in managing multiple versions of your thesis and in collaborating with your supervisory team. Embedding code in your thesis is a good practice in reproducible research. Making your thesis in HTML format can allow for interactive web elements to be embedded while PDF format can be for general distribution and printing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ANU Thesis",
    "section": "Getting started",
    "text": "Getting started\nThere are several systems that you are expected to know to use this template. These include:\n\nMarkdown syntax for writing\nQuarto or R Markdown syntax (note that these works for Python or Julia too) for embedding code\n(Optional) Git and GitHub for hosting",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#frequently-asked-questions",
    "href": "index.html#frequently-asked-questions",
    "title": "ANU Thesis",
    "section": "Frequently asked questions",
    "text": "Frequently asked questions\n\nWhat about Overleaf?\nANU has a professional account for Overleaf, which is great for those that use LaTeX regularly. Unfortunately, there is no equivalent system with track changes in Quarto. You can output the tex file from Quarto document and use this in Overleaf. The changes made in this tex document however has to be manually transferred back to the Quarto document. If your main output is mainly mathematical and you have little to no code outputs, Overleaf is probably a better choice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "This thesis is composed of my original work, and contains no material previously published or written by another person except where due reference has been made in the text. I have clearly stated the contribution by others to jointly-authored works that I have included in my thesis.\nI have clearly stated the contribution of others to my thesis as a whole, including statistical assistance, study design, data analysis, significant technical procedures, professional editorial advice, financial support and any other original research work used or reported in my thesis. The content of my thesis is the result of work I have carried out since the commencement of my higher degree by research candidature and does not include a substantial part of work that has been submitted to qualify for the award of any other degree or diploma in any university or other tertiary institution. I have clearly stated which parts of my thesis, if any, have been submitted to qualify for another award.\n\nYour Name\n2024-01-24\n\n\nPublications\nAccepted or in-press publication in this thesis.\nSubmitted manuscripts included in this thesis.\nOther publications during candidature.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "I would like to express my sincere gratitude to my dog, Chuckles, for eating my research notes multiple times. If it wasn’t for you, I would have finished this thesis earlier.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Reproducible research is an essential paradigm that promotes the idea that scientific investigations should be transparent, verifiable, and accessible to others. In an era where the scientific community faces concerns about the replicability of research findings, adopting reproducible research practices becomes imperative.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Method",
    "section": "",
    "text": "This report underscores the transformative impact of reproducible research on the scientific landscape, promoting a commitment to openness and accountability for the benefit of the entire research community.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ. 2023. Quarto: R Interface to ’Quarto’ Markdown\nPublishing System. https://CRAN.R-project.org/package=quarto.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Appendix A — Tools",
    "section": "",
    "text": "This thesis was written using Quarto 1.4.545 (Allaire 2023) and the following system and R packages:\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.2.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Australia/Sydney\n date     2024-01-24\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.0)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.1)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.1)\n later         1.3.2   2023-12-06 [1] CRAN (R 4.3.1)\n processx      3.8.3   2023-12-10 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.0)\n quarto        1.3     2023-09-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.3.1)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.1)\n rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n xfun          0.41    2023-11-01 [1] CRAN (R 4.3.1)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.1)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nAllaire, JJ. 2023. Quarto: R Interface to ’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Linear model\nSuppose we have a linear model,\n\\boldsymbol{y}=\\mathbf{X}\\boldsymbol{\\tau} + \\boldsymbol{\\epsilon} \\tag{2.1} where \\boldsymbol{y} is n\\times 1 vector of n observations, \\boldsymbol{\\tau} is a t\\times 1 vector of fixed effects, \\boldsymbol{\\epsilon} is the n\\times 1 vector for error, and \\mathbf{X} is a design matrix has size n\\times t. We assume that \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I}_{n}) and hence \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n).\nThe log-likelihood of Equation 2.1 is then given as:\n\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) = -\\frac{n}{2}\\log(2\\pi)-n\\log(\\sigma)-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}).\n The (i,j)-th entry of the Fisher information matrix is defined as\nI_{ij}(\\boldsymbol{\\tau})=-\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\tau_i\\partial\\tau_j}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)\n where \\tau_i is the i-th entry of \\boldsymbol{\\tau}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#linear-model",
    "href": "background.html#linear-model",
    "title": "2  Background",
    "section": "",
    "text": "Lemma 2.1 The Fisher information matrix of Equation 2.1 is given as \n\\mathbf{C} = -\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\n\nProof. The second derivative of the log-likelihood function \\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y}) is the Hessian matrix. We have \n\\frac{\\partial}{\\partial\\boldsymbol{\\tau}}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})=\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau})\n and for second derivative is \n\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})==-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n And in linear model assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n) and the Fisher information matrix is unbiased because, in the expectation calculation process, we do not involve the randomness of \\boldsymbol{y}. The Fisher information matrix is actually determined by the design matrix \\boldsymbol{X} and the error variance \\sigma^2. Hence \n\\mathbb{E}\\left(\\frac{\\partial^2}{\\partial\\boldsymbol{\\tau}\\partial\\boldsymbol{\\tau}^\\top}\\log\\ell(\\boldsymbol{\\tau};\\boldsymbol{y})\\right)=-\\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X} = -\\mathbf{C}\n So \\mathbf{C} = \\frac{1}{\\sigma^2}\\boldsymbol{X}^\\top\\boldsymbol{X}\n\n\nLemma 2.2 The variance of the fixed effects for Equation 2.1 is equivalent to the inverse of the Fisher information matrix, i.e. var(\\hat{\\boldsymbol{\\tau}})=\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1} = \\mathbf{C}^{-1}.\n\n\nProof. We know that the MLE of \\boldsymbol{\\tau} in a linear model is \\hat{\\boldsymbol{\\tau}}=(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}. By assumption we have \\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\tau}, \\sigma^2\\mathbf{I}_n). So \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}).So we have var(\\hat{\\boldsymbol{\\tau}}) = \\sigma^2(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}, which is exactly the inverse of Fisher information matrix \\mathbf{C}^{-1}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#a-criterion",
    "href": "background.html#a-criterion",
    "title": "2  Background",
    "section": "3.1 A-criterion",
    "text": "3.1 A-criterion\nWe first start with a simple example, that is, we consider treatment factors \\boldsymbol{\\tau} are fixed, to elucidate the influence of A-criterion.\nFrom David Butler (2013), we conduct a maximum log likelihood by following objective function: \n\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R})+\\log f_u(\\boldsymbol{u};\\textbf{G})\n basing on assumption, we have \\boldsymbol{y}=\\textbf{X}\\boldsymbol{\\tau}+\\textbf{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(\\textbf{X}\\boldsymbol{\\tau},\\textbf{R}+\\textbf{ZGZ}^\\top). So for objective function, we can write out the distributions\n\n\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R}\\sim N(\\textbf{X}\\boldsymbol{\\tau}+\\textbf{Z}\\boldsymbol{u},\\textbf{R}) \\\\\n\\boldsymbol{u}\\sim N(\\boldsymbol{0},\\textbf{G})\n\n\nLemma 3.1 So log of joint density is given as\n\\begin{align*}\n\\mathscr{L}&=\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\textbf{R})+\\log f_u(\\boldsymbol{u};\\textbf{G})\\\\\n&=-\\frac{1}{2}\\left(\\log|\\textbf{R}|+\\log|\\textbf{G}|+(\\boldsymbol{y}-\\textbf{X}\\boldsymbol{\\tau}-\\textbf{Z}\\boldsymbol{u})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\textbf{X}\\boldsymbol{\\tau}-\\textbf{Z}\\boldsymbol{u})+\\boldsymbol{u}^\\top\\textbf{G}^{-1}\\boldsymbol{u}\\right)\n\\end{align*}\n\n\nProof. Where is the proof?\n\nWe determine that \\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{\\tau}}=\\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{u}}=\\boldsymbol{0}, and write the equation into a matrix form \n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+ \\textbf{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\boldsymbol{y}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n\nLet \n\\textbf{C}=\n\\begin{bmatrix}\n\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}\\\\\n\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{X} & \\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+ \\textbf{G}^{-1}\n\\end{bmatrix}\n\\quad\n\\hat{\\boldsymbol{\\beta}}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\n\\quad\n\\textbf{W}=\\begin{bmatrix}\\textbf{X} &\\textbf{Z}\\end{bmatrix}\n\nRobinson (1991) shows that \n\\hat{\\boldsymbol{\\beta}}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}\\sim\nN(\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}\\\\\n\\hat{\\boldsymbol{u}}\n\\end{bmatrix}, \\textbf{C}^{-1})\n\nWe can rewrite the equation in a standard form \\textbf{C}\\hat\\beta=\\textbf{W}^\\top\\textbf{R}^{-1}y. So \\textbf{C} is the Fisher information matrix for this linear mixed model. As we mentioned above, we are interesting in examine fixed effect part \\boldsymbol{\\tau}. For the properties of variance-covariance matrix \\textbf{C}^{-1} can be written as \n\\textbf{C}^{-1}=\n\\begin{bmatrix}\n\\textbf{C}^{11} & \\textbf{C}^{12}\\\\\n\\textbf{C}^{21} & \\textbf{C}^{22}\n\\end{bmatrix}\n \\textbf{C}^{11} is a t \\times t matrix and variance-covariance matrix for treatment factors. We can caluclate \\textbf{C}_{11} by writing out standard form for \\boldsymbol{\\tau} and cancelling \\boldsymbol{u}.We have \n\\boldsymbol{X}^\\top\\textbf{R}^{-1}\\textbf{X}\\boldsymbol{\\tau}+\\textbf{X}^\\top\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})\\textbf{Z}^\\top\\textbf{R}^{-1}y=\\textbf{X}^\\top\\textbf{R}^{-1}y\\\\\n \n\\Rightarrow \\textbf{X}^\\top[\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}]\\textbf{X}\\boldsymbol{\\tau}=\\textbf{X}^\\top[\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}]y\\\\\n \n\\Rightarrow \\textbf{X}^\\top\\textbf{P}\\textbf{X}\\boldsymbol{\\tau}=\\textbf{X}^\\top\\textbf{P}y\n where \\textbf{P}=\\textbf{R}^{-1}-\\textbf{R}^{-1}\\textbf{Z}(\\textbf{Z}^\\top\\textbf{R}^{-1}\\textbf{Z}+\\textbf{G}^{-1})^{-1}\\textbf{Z}^\\top\\textbf{R}^{-1}, let \\textbf{C}_{11}=\\textbf{X}^\\top\\textbf{P}\\textbf{X} then we have the standard form for \\hat{\\boldsymbol{\\tau}}, which is \\textbf{C}_{11}\\boldsymbol{\\tau}=\\textbf{X}^\\top\\textbf{P}y, and \\textbf{C}_{11} is the corresponding Fisher information matrix.\nSo we have \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\textbf{C}_{11}^{-1}).To examine a specific form of \\boldsymbol{\\tau}, we do linear transform on \\boldsymbol{\\tau}: \\hat{\\boldsymbol{\\pi}}=\\textbf{D}\\hat{\\boldsymbol{\\tau}}, where \\textbf{D} is some transform matrix, so we have \\textbf{D}(\\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{\\pi}-\\hat{\\boldsymbol{\\pi}}\\sim N(0,\\textbf{D}\\textbf{C}_{11}^{-}\\textbf{D}^\\top). We denote \\boldsymbol{\\Lambda}=\\textbf{D}\\textbf{C}_{11}^{-}\\textbf{D}^\\top.\nA-criterion is the mean of predicted error variance of the parameter. i.e.  \n\\mathscr{A}=\\frac{1}{n_{\\pi}(n_{\\pi}-1)}\\sum_{i}\\sum_{j&lt;i} predicted\\quad error\\quad variance\\quad of\\quad(\\hat\\pi_i-\\hat\\pi_j)\n where n_{\\pi} is the row number of vector \\pi.For error variance part, we have \n\\sum_{i}\\sum_{j&lt;i} predicted\\quad error\\quad variance\\quad of\\quad(\\hat\\pi_i-\\hat\\pi_j)=\\sum_{i}\\sum_{j&lt;i}var(\\hat\\pi_i-\\hat\\pi_j)=\\sum_{i}\\sum_{j&lt;i}[var(\\hat\\pi_i)+var(\\hat\\pi_j)-2cov(\\hat\\pi_i,\\hat\\pi_j)]\n from virance-covirance matrix \\boldsymbol{\\Lambda}, we can rewrite the sum part as n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}.So we have \n\\mathscr{A}=\\frac{1}{n_{\\pi}(n_{\\pi}-1)}[n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}]\n same result from DG Butler, Smith, and Cullis (2013)\nDerivation above indicate that \\mathscr{A}\\propto tr(\\boldsymbol{\\Lambda}), A-criterion as the mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix \\boldsymbol{\\Lambda} should be as small as possible. And this is our goal on optimal experimental design.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nRobinson, George K. 1991. “That BLUP Is a Good Thing: The Estimation of Random Effects.” Statistical Science, 15–32.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In the field of experimental design, efficient methods are crucial for ensuring accurate and reliable results. One such method is the row-column design, controlling variability in experiments involving two factors, typically arranged in rows and columns. The row-column design can be considered as an extension of the Latin square design with more flexibility, allowing for different numbers of rows, columns, and treatments. This flexibility makes row-column designs applicable to a wider range of experimental settings.\nTo offer a row-column design that gives a precise estimation of treatment effects, one way is to seek the optimal value of some statistic criteria, for example, A-criteria(links to sections), minimizing the variance of elementary treatment contrasts. Using linear mixed model and assuming fixed treatment effects and random blocking effect, Butler (2013) has show the relation between optimizing design and minimizing the value of A-criteria, and show some possible algorithms to search optimal design in feasible set. These algorithms mainly focus on comparing the arrangements of different treatments, that is, doing permutations, and calculating their A values, optimizing design by iterations.\nHowever, some undesired cluster of replications or some treatment may occur when algorithm are doing permutations along rows and columns. Piepho, Michel, and Williams (2018) found that such clustering is considered undesirable by experimenters who worry that irregular environmental gradients might negatively impact multiple replications of the same treatment, potentially leading to biased treatment effect estimates. Williams emphasis that there is a need to design a strategy to avoid clustering and achieve even distribution of treatment replications among the experimental field. Two properties of design are introduced. Even distribution of treatment replications, abbreviated as ED, and neighbor balance, abbreviated as NB. A good ED ensures every replications of a treatment are widely spread in experimental field, and NB helps to avoid replications of the some treatment cluster together repeatedly. Williams introduce a scoring system to analysis ED and NB for a specific design, and introduce a algorithm to optimize ED, NB and some average efficiency factor can be represented by a specific statistic criteria.(maybe saying some improvement is needed)\nWe offer an optimization strategy for a design problem, which we can improving ED and NB during optimizing statistic criteria for a design, and avoid unwanted clustering and self-adjacency on the resulting design.In this algorithm, we use A-criteria to evaluate the efficiency of a design. Before the algorithm, we randomly generate a design as an initial design, and calculate the A-criteria as initial value. We update design by selecting a better among its neighbors. The neighbors are pair-wise permutations of a design. Typically, we select a neighbor from all pairwise permutations of a design for iteration, but this does not ensure ED and NB. To ensure ED and NB during optimization, we need to add some constraints when generating the pairwise permutations.(maybe explain what is the constraints) By filtering design with bad ED and NB, we then optimize the statistic criteria of the design.\n(sections in the thesis)\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "background.html#linear-mixed-model",
    "href": "background.html#linear-mixed-model",
    "title": "2  Background",
    "section": "2.2 Linear mixed model",
    "text": "2.2 Linear mixed model\nLinear mixed model extends linear model by incorporating additionally incorporating random effects into the model that effectively give greater flexibility and capability to incorporate known correlated structures into the model. We now consider a linear mixed model \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\n\\tag{2.2} here \\boldsymbol{y} is n\\times 1 vector for n observations, \\boldsymbol{\\tau} is a t\\times1 parameter vector of treatment factors, \\boldsymbol{u} is a q \\times1 parameter vector of blocking effects, and \\boldsymbol{\\epsilon} is the n\\times 1 error vector, \\boldsymbol{X} and \\boldsymbol{Z} are design matrices of dimension n \\times t and n \\times q for treatment factors and blocking factors, respectively. We here assume blocking factors are random effect, with random error \\boldsymbol{\\epsilon} we have \n\\begin{bmatrix}\n\\boldsymbol{u} \\\\\n\\boldsymbol{\\epsilon}\n\\end{bmatrix}\n\\sim\nN\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n\\boldsymbol{0}\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n\\boldsymbol{G} & \\mathbf{0} \\\\\n\\mathbf{0} & \\boldsymbol{R}\n\\end{bmatrix}\n\\right),\n where \\boldsymbol{G} is the q \\times q variance matrix for \\boldsymbol{u} and \\boldsymbol{R} is n\\times n variance matrix for \\boldsymbol{\\epsilon}.\n\n2.2.1 A-criterion\nWe first start with a simple example, that is, we consider treatment factors \\boldsymbol{\\tau} are fixed, to elucidate the influence of A-criterion.\nbasing on assumption, we have \n\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau},\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\n\\tag{2.3} So for objective function, we can write out the distributions\n\n\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R}) \\\\\n\\boldsymbol{u}\\sim N(\\boldsymbol{0},\\boldsymbol{G})\n\nWe want to give a precise estimation on \\boldsymbol{\\tau}. As we mentioned, we have the distribution for response variable \\boldsymbol{y}\\sim We can use generalized least squares(GLS) by rewrite the model as: \n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\tau} + \\zeta\n Here \\zeta = \\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\sim N(0, \\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top). Henderson (1975) shows that the GLS estimation of \\boldsymbol{\\tau} is any solution to \n\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X}\\hat{\\boldsymbol{\\tau}}_{gls}=\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\n Here \\boldsymbol{V}=\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top. So \\hat{\\boldsymbol{\\tau}}_{gls} = (\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{V}^{-1}\\boldsymbol{y}\nHenderson et al. (1959) emphasis that computing matrix \\boldsymbol{V} which is often large is difficult. So here we use joint log likelihood.\nFrom David Butler (2013), we conduct a maximum log likelihood by following objective function: \n\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\n ::: {#lem-joint-density-lmm} So log of joint density is given as\n\\begin{align*}\n\\mathscr{L}&=\\log f_Y(\\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R})+\\log f_u(\\boldsymbol{u};\\boldsymbol{G})\\\\\n&=-\\frac{1}{2}\\left(\\log|\\boldsymbol{R}|+\\log|\\boldsymbol{G}|+(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\tau}-\\boldsymbol{Z}\\boldsymbol{u})+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}\\right)\n\\end{align*}\n:::\n\nProof. We have density function for \\boldsymbol{y}|\\boldsymbol{u};\\boldsymbol{\\tau},\\boldsymbol{R}\\sim N(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u},\\boldsymbol{R})\n\nf_y = \\frac{1}{\\sqrt{(2\\pi)^{n}|\\boldsymbol{R}|}}exp(-\\frac{1}{2}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u})))\n And density function for \\boldsymbol{u} \nf_u = \\frac{1}{\\sqrt{(2\\pi)^{l}|\\boldsymbol{G}|}}exp(-\\frac{1}{2}\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u})\n Ignoring constant part, we have \n\\log f_y=-\\frac{1}{2}[\\ln |\\boldsymbol{R}|+(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{Z}\\boldsymbol{u}))]\n \n\\log f_u = -\\frac{1}{2}[\\ln |\\boldsymbol{G}+\\boldsymbol{u}^\\top\\boldsymbol{G}^{-1}\\boldsymbol{u}]\n So we have our log of joint density function.\n\nWe determine that \\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{\\tau}}=\\frac{\\partial\\mathscr{L}}{\\partial\\boldsymbol{u}}=\\boldsymbol{0}, and write the equation into a matrix form \n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n\\tag{2.4}\nLet \n\\boldsymbol{C}=\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+ \\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\quad\n\\hat{\\boldsymbol{\\beta}}_{llm}=\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}}_{llm}\\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n\\quad\n\\boldsymbol{W}=\\begin{bmatrix}\\boldsymbol{X} &\\boldsymbol{Z}\\end{bmatrix}\n By cancelling \\boldsymbol{u}, we have \n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X}\\boldsymbol{\\tau}+\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}y=\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top[\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}]y\\\\\n \n\\Rightarrow \\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\boldsymbol{\\tau}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n where \\boldsymbol{P}=\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}, let \\boldsymbol{C}_{11}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X} then we have the form similar to GLS estimation for \\hat{\\boldsymbol{\\tau}}, which is \n\\boldsymbol{C}_{11}\\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}\n and the estimation of \\boldsymbol{\\tau} is \\hat{\\boldsymbol{\\tau}}_{llm}=\\boldsymbol{C}_{11}^{-1}\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{y}, which is equivalent with GLS estimation\n\nProof. We only need to prove that \\boldsymbol{P}=\\boldsymbol{V}^{-1}. \\begin{align*}\n\\boldsymbol{P}\\boldsymbol{V} &=(\\boldsymbol{R}^{-1}-\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})(\\boldsymbol{R}+\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top - \\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad\\quad\\quad -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}\\boldsymbol{Z}^\\top(\\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&=\\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{Z}^\\top+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top)\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{I}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G})\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I} + \\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top -\\boldsymbol{R}^{-1}\\boldsymbol{Z}(\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1})^{-1}(\\boldsymbol{G}^{-1}+\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z})\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n&= \\boldsymbol{I}+\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top-\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\top\\\\\n& = \\boldsymbol{I}\n\\end{align*} So \\hat{\\boldsymbol{\\tau}}_{llm} and \\hat{\\boldsymbol{\\tau}}_{gls} are equivalent, and we denote them as \\hat{\\boldsymbol{\\tau}}\n\nNow we have our estimation for the treatment factor, and experimental design aims to further refine our design by focusing on the precision of these estimates. Specifically, we aim to optimize the design so that the treatment effects are estimated with minimal variance, ensuring that the differences between any two treatment levels are as small as possible. To achieve this, we introduce the A-value as a criterion for evaluating the design.\n\nDefinition 2.1 Basing on the model formula Equation 2.2, and a estimation of treatment factor \\hat{\\boldsymbol{\\tau}} has n_{\\tau} factors. A-criterion measure the average predicted error variance of different treatments. Let V_{ij}= var(\\hat{\\tau}_i-\\hat{\\tau}_j)=var(\\hat{\\tau}_i)+var(\\hat{\\tau}_j)-2cov(\\hat{\\tau}_i,\\hat{\\tau}_j), and a A-value \\mathscr{A} is \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n\\tag{2.5}\n\nTo discover the relationship between the A-value and the design matrix, I need to find the variance-covariance matrix of \\hat{\\boldsymbol{\\tau}}. In fact, it can be proof that \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).\n(lemma and proof)\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)(\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1})^\\top)\nWe denote \\begin{align*}\n\\boldsymbol{M} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{N} &= \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z}\\\\\n\\boldsymbol{J} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\\\\\n\\boldsymbol{K} &= \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1}\\\\\n\\end{align*} In the context of row-column design, the \\boldsymbol{K} matrix is invertible. Schur complement of \\boldsymbol{K} is \n\\boldsymbol{S} = \\boldsymbol{M} - \\boldsymbol{N} \\boldsymbol{K}^{-1} \\boldsymbol{J}\n And the inverse matrix of \\boldsymbol{C} can be written as \n\\boldsymbol{C}^{-1}=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\boldsymbol{S}^{-1} & -\\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1} \\\\\n-\\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} & \\boldsymbol{K}^{-1} + \\boldsymbol{K}^{-1} \\boldsymbol{J} \\boldsymbol{S}^{-1} \\boldsymbol{N} \\boldsymbol{K}^{-1}\n\\end{bmatrix}\n\\tag{2.6} So from Equation 2.4 we have \n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\tau}} \\\\\n\\hat{\\boldsymbol{u}}_{llm}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{U}_1\\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\\boldsymbol{y}\n Here \\begin{align*}\n&\\boldsymbol{U}_1 = \\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\\\\n&\\boldsymbol{U}_2 = \\boldsymbol{C}^{21}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{22}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{align*}\nFrom Equation 2.4 and Equation 2.3, we have \\hat{\\boldsymbol{\\tau}}\\sim N(\\boldsymbol{\\tau},\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top)\nAnd we have following results\n\\begin{align*}\n\\begin{bmatrix}\n\\boldsymbol{U}_1 \\\\\n\\boldsymbol{U}_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X} & \\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{C}^{11} & \\boldsymbol{C}^{12} \\\\\n\\boldsymbol{C}^{21} & \\boldsymbol{C}^{22}\n\\end{bmatrix}\n(\n\\begin{bmatrix}\n\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}\\\\\n\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{X} & \\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1}\\boldsymbol{Z}+\\boldsymbol{G}^{-1}\n\\end{bmatrix}-\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & \\boldsymbol{G}^{-1}\n\\end{bmatrix})\\\\\n&=\n\\boldsymbol{I}-\n\\begin{bmatrix}\n0 & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & -\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\boldsymbol{I} & -\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1} \\\\\n0 & \\boldsymbol{I}-\\boldsymbol{C}^{22}\\boldsymbol{G}^{-1}\n\\end{bmatrix}\n\\end{align*}\nSo we have \\begin{align*}\n\\boldsymbol{U}_1\\boldsymbol{X}&=\\boldsymbol{I}\\\\\n\\boldsymbol{U}_1\\boldsymbol{Z}&=-\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\\\\n\\end{align*} For the variance of estimation we have \\begin{align*}\nvar(\\hat{\\boldsymbol{\\tau}})\n&=\n\\boldsymbol{U}_1(\\boldsymbol{R}+\\boldsymbol{ZGZ}^\\top)\\boldsymbol{U}_1^\\top\\\\\n&= \\boldsymbol{U}_1\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&= (\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{R}^{-1}+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{R}^{-1})\\boldsymbol{R}\\boldsymbol{U}_1^\\top+\\boldsymbol{U}_1\\boldsymbol{ZGZ}^\\top\\boldsymbol{U}_1^\\top\\\\\n&=\\boldsymbol{C}^{11}\\boldsymbol{X}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{Z}^\\top\\boldsymbol{U}_1^\\top+\\boldsymbol{C}^{12}\\boldsymbol{G}^{-1}\\boldsymbol{G}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}-\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top+\\boldsymbol{C}^{12}(\\boldsymbol{G}^{-1})^\\top(\\boldsymbol{C}^{12})^\\top\\\\\n&=\\boldsymbol{C}^{11}\n\\end{align*}\nWhat is \\boldsymbol{C}^{11}? what is the relation between \\boldsymbol{C}_{11} and \\boldsymbol{C}^{11}? From Equation 2.6 we have \\boldsymbol{C}^{11}=\\boldsymbol{S}^{-1} and \\boldsymbol{S} is \n\\boldsymbol{S} = \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{X}\n And base on the complement of \\boldsymbol{C}_{11}, we rewrite the \\boldsymbol{S} \n\\begin{align*}\n\\boldsymbol{S}\n&=\\boldsymbol{X}^\\top(\\boldsymbol{R}^{-1} \\boldsymbol{X} - \\boldsymbol{X}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} (\\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{Z} + \\boldsymbol{G}^{-1})^{-1} \\boldsymbol{Z}^\\top \\boldsymbol{R}^{-1})\\boldsymbol{X}\\\\\n&=\\boldsymbol{X}^\\top\\boldsymbol{P}\\boldsymbol{X}\\\\\n&=\\boldsymbol{C}_{11}\n\\end{align*}\n So var(\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{C}^{11}=\\boldsymbol{C}_{11}^{-1}.\nSo we have \\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}}\\sim N(0,\\boldsymbol{C}_{11}^{-1}).To examine a specific form of \\boldsymbol{\\tau}, in general case, we do linear transform on \\boldsymbol{\\tau}: \\hat{\\boldsymbol{\\pi}}=\\boldsymbol{D}\\hat{\\boldsymbol{\\tau}}, where \\boldsymbol{D} is some transform matrix, so we have \\boldsymbol{D}(\\boldsymbol{\\tau}-\\hat{\\boldsymbol{\\tau}})=\\boldsymbol{\\pi}-\\hat{\\boldsymbol{\\pi}}\\sim N(0,\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top). We denote \\boldsymbol{\\Lambda}=\\boldsymbol{D}\\boldsymbol{C}_{11}^{-}\\boldsymbol{D}^\\top. If \\boldsymbol{D} is identical matrix \\boldsymbol{I}, then \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1} and \\hat{\\boldsymbol{\\pi}}=\\hat{\\boldsymbol{\\tau}}.\nWe know that A-criterion is the mean of predicted error variance of the parameter from Equation 2.5 i.e.  \n\\mathscr{A}=\\frac{1}{n_{\\tau}(n_{\\tau}-1)}\\sum_{i}\\sum_{j&lt;i}V_{ij}\n Having virance-covirance matrix \\boldsymbol{\\Lambda}=\\boldsymbol{C}_{11}^{-1}, we can rewrite the sum part as n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}.So we have [FIXME] \n\\mathscr{A}=\\frac{1}{n_{\\pi}(n_{\\pi}-1)}[n_{\\pi}tr(\\boldsymbol{\\Lambda})-\\mathbb{1}_{n_{\\pi}}^\\top\\boldsymbol{\\Lambda}\\mathbb{1}_{n_{\\pi}}]\n same result from DG Butler, Smith, and Cullis (2013)\nDerivation above indicate that \\mathscr{A}\\propto tr(\\boldsymbol{\\Lambda}), A-criterion as the mean of predicted error variance of the parameter, we prefer it as small as possible to obtain a accurate result from experiment, which means the trace of virance-covirance matrix \\boldsymbol{\\Lambda} should be as small as possible. And this is our goal on optimal experimental design.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#neighbor-balance-and-even-distribution",
    "href": "background.html#neighbor-balance-and-even-distribution",
    "title": "2  Background",
    "section": "2.3 Neighbor balance and even distribution",
    "text": "2.3 Neighbor balance and even distribution\n\n2.3.1 Concepts of NB and ED\nPiepho, Michel, and Williams (2018) emphasis the the concepts of neighbor balance and even distribution are crucial to mitigating biases and ensuring the reliability of results in row-column design.\nNeighbor balance (NB) refers to the principle that, in a row-column experimental design, the frequency with which two treatments are adjacent or near each other should not be excessively high. High adjacency frequency between two treatments can lead to mutual influence, which may cause bias to the experimental results. For example, if the effect of one treatment can spread to neighboring areas, frequent adjacency could interfere with accurate measurement of each treatment’s true effect next to it. Therefore, it is essential to control the adjacency frequency of different treatments to prevent high adjacency for two specific treatments.\nEven distribution(ED) aims to ensure that different replications of the same treatment are widely distributed across the experimental field, rather than being clustered in a specific area. This strategy helps to avoid biases caused by specific environmental conditions in certain parts of the experiment field. If replications of one treatment are over concentrated in one area, unique environmental factors in that area might affect the treatment’s performance, leading to biased observations. By evenly distributing replications, environmental interference can be minimized, so that we can enhance the reliability of the experimental results.\n(maybe some example plots or pictures)\n\n\n2.3.2 Measuring NB and ED\n\n2.3.2.1 Evaluating NB with adjacency matrix\nIn Piepho, Michel, and Williams (2018), there is a assumption that they are optimizing a binary design, which means each treatment appears only once in each row and column. Under this assumption, their balancing mechanism considers diagonal adjacency, Knight moves, and even more distant points to ensure an optimal balance. In my optimization process, I begin with a randomly selected design matrix. Consequently, my approach considers not only diagonal adjacency but also the adjacent points directly above, below, to the left, and to the right.\nI use an adjacency matrix to count the number of times each treatment is adjacent to another. This matrix serves as a crucial tool in my optimization process, enabling precise tracking and adjustment of treatment placements to achieve neighbor balance.\nWe denote the adjacency matrix as \\boldsymbol{A}, and for treatment i and j in treatment set T \\boldsymbol{A}_{ij} represents the count of times treatment i is adjacent to treatment j. Here “adjacent” means treatment j is located next to treatment i (maybe a picture to show it)\nFor Given design \\mathcal{D} and \\mathcal{D}_{r,c} represents the treatment at row r and column c. So \\boldsymbol{A}_{ij} can be expressed as:\n\n\\boldsymbol{A}_{ij}=\\sum_{r=1}^{R}\\sum_{c=1}^{C}I_{r,c}(i) F_{r,c}(j)\n where \nF_{r,c}(j)=\n\\sum_{m \\in \\{-1,0,1\\}}\\sum_{n \\in \\{-1,1\\}}I_{r+m,c+n}(j)+\\sum_{m \\in \\{-1,1\\}}I_{r+m,c}(j)\n R and C are total number of rows and columns and I_{r,c}(\\cdot) is the indicator function, which takes value under following cases \nI_{r,c}(i)=\n\\begin{cases}\n1 & \\text{if } \\mathcal{D}_{r,c}=i \\\\\n0 & \\text{if } \\mathcal{D}_{r,c}\\neq i & \\text{or } r&lt;1,r&gt;R,c&lt;1,c&gt;C\\\\\n\\end{cases}\n\nThe function F_{r,c}(j) here is actually counting the times that treatment j occurs at places around the position row r and column c.\nWe measure NB by taking the maximum of the elements in adjacency matrix \\boldsymbol{A}. Our NB criteria is \nC_{NB}=max\\{\\boldsymbol{A}_{ij}\\} \\quad i,j\\in T\n\n\n\n2.3.2.2 Evaluating ED with minimum row and column span\nThe goal of evaluating the evenness of distribution (ED) is to find the row and column spans for treatments across the entire design matrix. We would like this value as large as possible This ensures that the treatments t\\in T is distributed as evenly as possible within the rows and columns, reducing clustering and promoting a balanced design.\nThe row span for a given treatment t \\in T is defined as the difference between the maximum and minimum row indices where t appears in experiment field. \nRS(t)=max\\{r:\\mathcal{D}_{r,c}=t\\}-min\\{r:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n And the minimum row span of a design \\mathcal{D} is \nMRS(\\mathcal{D})=min\\{RS(t)\\},\\quad t \\in T\n Same for column span \nCS(t)=max\\{c:\\mathcal{D}_{r,c}=t\\}-min\\{c:\\mathcal{D}_{r,c}=t\\} \\quad 1&lt;r&lt;R,\\quad 1&lt;c&lt;C\n \nMCS(\\mathcal{D})=min\\{CS(t)\\},\\quad t \\in T\n So, for the changes in the design matrix \\mathcal{D} during the search process, we tend to accept only those changes where the Minimum Row Span (MRS) and Minimum Column Span (MCS) remain the same or become smaller.\n\n\n\n\nButler, David. 2013. “On the Optimal Design of Experiments Under the Linear Mixed Model.”\n\n\nButler, DG, AB Smith, and BR Cullis. 2013. “On Model Based Design of Comparative Experiments.” preparation.\n\n\nHenderson, Charles R. 1975. “Best Linear Unbiased Estimation and Prediction Under a Selection Model.” Biometrics, 423–47.\n\n\nHenderson, Charles R, Oscar Kempthorne, Shayle R Searle, and CM Von Krosigk. 1959. “The Estimation of Environmental and Genetic Trends from Records Subject to Culling.” Biometrics 15 (2): 192–218.\n\n\nPiepho, Hans-Peter, Volker Michel, and Emlyn Williams. 2018. “Neighbor Balance and Evenness of Distribution of Treatment Replications in Row-Column Designs.” Biometrical Journal 60 (6): 1172–89.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  }
]