# Discussion {#sec-discuss}

## Limitations

In this section, we focus on examining the limitations and shortcomings of our algorithm, along with discussing the practical constraints observed in the experimental simulation results. Our analysis is organized into the following parts: computational efficiency and convergence Rate, balancing multi-objective optimization, and limitations and generalizability in practical applications.

### Computational efficiency and convergence rate

Based on the line plots from the previous section, we observe that the convergence rate of our three algorithms significantly slow down as they approach the optimal solution. This often results in giving a near-optimal solutions, although it performs reasonably well within the limited number of iterations $M$ in our simulation. For iterative methods, the algorithm actually  explores the entire design matrix space by evaluating permutations of the matrix. This causes our algorithms, especially random selection, to have long runtimes when a large number of iterations is performed. For certain fixed design assumptions, such as the diagonal $\boldsymbol{G}$ matrix and $\boldsymbol{R}$ matrix we used previously, an algorithm that optimizes based on matrix structure could improve efficiency in such cases.

### Balancing multi-objective optimization

We aim to optimize A while avoiding bad NB and ED statistics, which essentially represents a multi-objective optimization problem. In our current approach, we incorporate filtering steps during random search permutation generation and SA neighbour generation to ensure that NB and ED improve gradually throughout the iterations. Although this approach allows for the stepwise optimization of all three statistics, it fails to bing in the correlations among them and lacks the ability to balance and trade off between the three. Using a multi-objective optimization method would allow us to place these three statistics on an equal priority level. In our current approach, however, the A-value has consistently been the primary variable driving changes in the design matrix.

### Limitations and generalizability in practical applications

When comparing the computational performance of different algorithms, we mentioned that such comparisons cannot determine which algorithm is better. For a deeper exploration of the performance of different algorithms, a controlled comparison is needed. For example, setting a common tolerance and allowing all algorithms sufficient time or iteration count to complete, which would enable a fair comparison of runtime or iteration numbers. Our analysis of the algorithms is based on a limited set of constraints, observing and comparing the behaviour of different algorithms under some conditions.

When discussing model errors, we mentioned using the R matrix for modelling. For independent plots, using linear models and the R matrix is very convenient. In @piepho2018neighbor, the ${AR1}$ model is suggested as a way to address cases where plots are correlated. We have not attempted or analysed algorithms using this type of model.

## Future Directions

### Penalty objective function

Incorporate a penalty term into the objective function. When adjacent treatments in the design are the same, this penalty term increases the value of the objective function, making the optimization process more likely to avoid having the same treatment in adjacent positions.

For example, we can define a following penalty function. For example we have a design matrix $X$, the penalty for self-adjacency is

$$
f(X) = \sum_{(i,j)\in X}I(X_i = X_j)
$$
Here $i$ and $j$ are different plots next to each other in the experiment field, and $I(X_i = X_j)$ is an indicator function equals to $1$ if $i$ plot and $j$ plot have the same treatment.

So basing on this penalty function, For a certain design $X$ we change our function into:

$$
F(X) = \bar{f}_A^{RC} + \lambda \cdot f(X)
$$
here $\lambda < 0$

Or use A-criteria :
$$
G(x) = t\mathscr{A} + (1-t)\cdot f(X)
$$
here $0\leq t \leq 1$ and we minimize it

Usually we have such mathematical programming of inequality constrained optimization problem: we minimize objective function $f_0(x)$  with inequality constrains $f_i(x)\leq 0, i\in I=\{1,2,\cdots,m\}$.And we have a well-known penalty function for this problem is
$$
F (x, \rho) = f_0(x) + \rho \sum_{i\in I}max\{f_i(x),0\}
$$
and a corresponding constrained penalty optimization problem is to minimize penalty function $F_2 (x, \rho)$, detailed information in @meng2013exactness

### Algorithm improvment

#### SA-based multiobjective optimization algorithms

When discussing the A-value, NB statistic, and ED statistic, our goal is actually to find a process that can simultaneously optimize all three, or finding a balance between three statistic criteria.SA is often used in combinatorial optimization problems, as we are using it here to optimize the arrangement of treatment plots. @suman2006survey claim that in recent studies, SA has been applied in many multi-objective optimization problems, because of its simplicity and capability of producing a Pareto set of solutions in single run with very little computational cost.

We say a solution is non-dominated if none of its objective values can be improved without worsening at least one other objective. The Pareto set (or Pareto front) is a set of non-dominated solutions in a multi-objective optimization problem.  @suman2006survey introduce several SA for multi-objective optimization.

SA-based multi-objective optimization algorithms given by @suppapitnarm2000simulated is a promising approach. Instead of @eq-prob and using permutation filtering, they give probability step write as this form
$$
P = \min(1, \Pi_{i=1}^N\exp\{\frac{-\bigtriangleup s_i}{T_i}\})
$$
Here $N$ is the number of objective functions, and for each objective function $i$, $\bigtriangleup s_i$ is the difference of objective function between two solution and $T_i$ is the current temperature. Control the optimization rates of different objective functions by setting different cooling schedules for each.

#### Memory-Based Optimization Algorithms - TABU search

Same with SA, Tabu Search is an optimization algorithm used for solving combinatorial problems. It is a type of local search method that enhances basic hill-climbing algorithms by using memory structures to avoid revisiting previously explored solutions, introduced in @butler2013model. This helps the search process escape from local optima and encourages exploration of new areas in the solution space. Tabu Search maintains a tabu list, a short-term memory that keeps track of recent moves or solutions that should not be revisited for a certain number of iterations. This is mathematically represented as a set 
$\mathcal{T}$ where recent solutions $X_t$ are stored for a fixed period $k$ iterations:
$$
\mathcal{T} = \{x_t|t \in [t-k,t)\}
$$
If a solution $x' \in \mathcal{T}$, $x'$ is considered "tabu" and cannot be revisited. Therefore, besides our two approaches here—random search with step-length and SA that probabilistically accepts worse objective function values—this type of short-term memory-enhanced algorithm (Memory-Enhanced Algorithms) helps ensure efficient computation, minimizing resource consumption while maximizing exploration within the solution space and helping escape local optima.










