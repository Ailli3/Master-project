% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
  oneside,
  openany,
  12pt,
  onecolumn]{book}

\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \ifXeTeX
    \usepackage{mathspec} % this also loads fontspec
  \else
    \usepackage{unicode-math} % this also loads fontspec
  \fi
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
  \setmainfont[]{Public Sans}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,right=25mm,bottom=30mm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{2}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Preliminary investigation of algorithms for optimal row-column designs},
  pdfauthor={Jingning Yao},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

%\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage{amsmath}	% Advanced maths commands
%\usepackage{amssymb}	% Extra maths symbols
\usepackage{rotating} % Rotating table
\usepackage[toc,page]{appendix}
\usepackage{setspace}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{natbib}


\def\TODO#1{{\color{red}{\bf [TODO:} {\it{#1}}{\bf ]}}}

%%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

\usepackage{color}
%\usepackage[normalem]{ulem}

% These are autoref macros
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{equation}

% There are editing macros
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}}
\newcommand{\cyan}[1]{{\textcolor{cyan}{#1}}}
\newcommand{\cut}[1]{{\red{\sout{#1}}}}
\newcommand{\add}[1]{{\cyan{#1}}}

% Autoref macros
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{equation}
\newcommand{\aref}[1]{\hyperref[#1]{Appendix~\ref{#1}}}

% Only include extra packages if you really need them. Common packages are:
%\usepackage{caption}
%\usepackage{subcaption}
%\captionsetup{compatibility=false} %https://tex.stackexchange.com/questions/31906/subcaption-package-compatibility-issue
\usepackage{multicol}        % Multi-column entries in tables
\usepackage{booktabs} %to include pandas latex tables
\usepackage{float}
\usepackage{bera}


\title{Preliminary investigation of algorithms for optimal row-column
designs}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Balancing Treatment Efficiency and Spatial Distribution in
Experimental Designs}
\author{Jingning Yao}
\date{23rd October 2024}

\begin{document}
  \begin{frontmatter}
  \begin{titlepage}
  %%% TITLE PAGE:
  \pagenumbering{roman}  % first use Roman numerals for page numbers

  \begin{titlepage}
    \begin{flushright}%
      \vspace{50mm}
      {\small A thesis submitted for the degree of {\it Master of
  Mathematical Sciences (Advanced)}}
      \rule[1ex]{\textwidth}{1pt}\\
      {\fontsize{9}{0} 23rd October 2024}\\
      \vspace{25mm}
      {\fontsize{34}{38}\bfseries Preliminary investigation of
  algorithms for optimal row-column designs\par}
        \vspace{12mm}
    	\parbox{\textwidth}{
  	\begin{flushright}
  		\fontsize{28}{30} Balancing Treatment Efficiency and Spatial
  Distribution in Experimental Designs
  	\end{flushright}}
  	    \vfill
      {\fontsize{20}{0}\bfseries Jingning Yao}\\
      \vspace{2mm}
      {\fontsize{8}{0} Mathematical Sciences Institue}\\
      \vspace{35mm}
      {\fontsize{10}{0}\bfseries supervised by}\\
      Dr.~Emi Tanaka, A/Prof Katherine Turner
      
      \vspace{2.0cm}
  		\includegraphics[width=0.4\textwidth]{\_extensions/anu-thesis/assets/latex/ANU\_Primary\_Horizontal\_GoldBlack.eps}\\
   \end{flushright}%

   \clearpage\thispagestyle{empty}
   \normalfont
   \vspace*{\fill}
   \noindent
   \begin{tabular}{lp{40cm}}
     {\bf Master of Mathematical Sciences (Advanced) Thesis} & \\[2mm]
     {\bf Author:} & Jingning Yao\\[2mm]
     {\bf Supervisors:} & Dr.~Emi Tanaka, A/Prof Katherine Turner\\[2mm]
     
     {\bf Project period:} & 2024-02-17 -- 2024-10-24 \\[2mm]
   \end{tabular}\\[2mm]

  \noindent Mathematical Sciences Institue\\
  \noindent Australian National University

  \end{titlepage}
  \setlength{\parindent}{0pt}
  \setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}








  \end{titlepage}
  \end{frontmatter}


\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\phantomsection\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\phantomsection\addcontentsline{toc}{section}{List of Tables}
\listoftables
\setstretch{1.2}
\mainmatter
\bookmarksetup{startatroot}

\chapter*{Declaration}\label{declaration}
\addcontentsline{toc}{chapter}{Declaration}

\markboth{Declaration}{Declaration}

The work in this thesis is my own except where otherwise stated.

Jingning yao

\bookmarksetup{startatroot}

\chapter*{Abstract}\label{abstract}
\addcontentsline{toc}{chapter}{Abstract}

\markboth{Abstract}{Abstract}

Row-Column designs are widely used in experimental design to minimize
variance and improve estimation accuracy. However, it sometimes can lead
to clustering of treatments, which negatively impacts design efficiency.
we present an approach to optimizing row-column designs using the
A-criterion, while avoiding clustering simultaneously through the
application of random search and simulated annealing algorithm. Our
method introduces some statistical criteria to quantify spatial
distribution, and by incorporating these metrics into the two mentioned
algorithms, we effectively balance the optimization between treatment
efficiency and spatial distribution. Additionally, we compare the
performance of the different algorithms, demonstrating their relative
effectiveness. This work contributes to both the theory and practice of
experimental design by offering an improved methodology for achieving
optimal and evenly distributed arrangements.

\bookmarksetup{startatroot}

\chapter*{Notation and terminology}\label{notation-and-terminology}
\addcontentsline{toc}{chapter}{Notation and terminology}

\markboth{Notation and terminology}{Notation and terminology}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3043}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6957}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Notation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\boldsymbol{y}\) & Response variable vector \\
\(\boldsymbol{u}\) & Parameter vector of blocking effects \\
\(\hat{\boldsymbol{u}}\) & Estimated parameter vector of blocking
effects by log-likelihood maximization \\
\(\boldsymbol{\tau}\) & Parameter vector of treatments effects \\
\(\hat{\boldsymbol{\tau}}\) & Estimated parameter vector of treatments
effects by log-likelihood maximization \\
\(\boldsymbol{\epsilon}\) & Parameter vector of model error \\
\(\boldsymbol{Z}\) & Blocking factor matrix \\
\(\boldsymbol{X}\) & Design matrix for treatments \\
\(\boldsymbol{G}\) & Variance matrix for random effect \\
\(\boldsymbol{R}\) & variance matrix for model error \\
\(\mathcal{A}\) & The A-value \\
\(\boldsymbol{\Lambda}\) & Variance-covariance of estimated treatments
effects \\
\(\mathcal{1}_n\) & A vector of length n with all elements equal to 1 \\
\(\boldsymbol{D}\) & Transformation matrix of treatments \\
\(\mathcal{D}\) & Design results \\
\(\mathcal{D}_{r,c}\) & The treatment at row \(r\) and column \(c\) \\
\(T\) & The set of treatments \\
\(\mathcal{U}\) & Adjacency matrix \\
\(n_r\) & Total number of rows \\
\(n_c\) & Total number of columns \\
\(n_{\tau}\) & Total kinds of treatments \\
\(C_{NB}\) & Neighbour balance criteria \\
\(MRS(\mathcal{D})\) & The minimum row span of design \(\mathcal{D}\) \\
\(MCS(\mathcal{D})\) & The minimum column span of design
\(\mathcal{D}\) \\
\(\boldsymbol{G}_{diag}\) & Diagonal \(\boldsymbol{G}\) matrix \\
\(\boldsymbol{R}_{diag}\) & Diagonal \(\boldsymbol{R}\) matrix \\
\(\boldsymbol{I}_n\) & Identity matrix with size \(n\times n\) \\
\(\sigma_{G_c}\) & Scaling constant for column effects \\
\(\sigma_{G_r}\) & Scaling constant for row effects \\
\(\sigma_{R}\) & Scaling constant for model error \\
\(A(\boldsymbol{X})\) & A-value function maps design matrix
\(\boldsymbol{X}\) to its A-value \\
\(\boldsymbol{X}_{new}\) & New design matrix generated by permutation \\
\(\mathcal{D}_{new}\) & Design results corresponding to
\(\boldsymbol{X}_{new}\) \\
\(\boldsymbol{P}_{ij}\) & An identity matrix with \(i\)-th row and
\(j\)-th row swapped \\
\(C_{NB}\) & Neighbour balance criteria of \(\mathcal{D}_{new}\) \\
\(T_{NB}\) & Tolerance for Neighbour balance criteria \\
\(T_{ED}\) & Tolerance for minimum row span and minimum column span \\
\(s\) & Step length for random search \\
\(t_{NB}\) & Tolerance for Neighbour balance criteria for pairwise
permutation \\
\(t_{ED}\) & Tolerance for minimum row span and minimum column span for
pairwise permutation \\
\(M\) & Maximum iteration number \\
\(\{\boldsymbol{X}\}\) & Finite design searching space \\
\(\{\boldsymbol{X}^*\}\) & A set of design matrix having optimal
A-value \\
\(T(t)\) & Temperature at \(t\)-th iteration \\
\(\boldsymbol{X}_0\) & Initial design matrix \\
\(n_p\) & Number of permutations \\
\(\boldsymbol{G}_s\) & Variance matrix for random effect of
simulation \\
\(\boldsymbol{R}_s\) & variance matrix for model error of simulation \\
\(A_{blocksdesign}\) & A-value calculated by design function in
blocksdesign package \\
\(T_A\) & Tolerance for A-value in simulation \\
\(A_{\bigtriangleup}\) & Difference of A-values between two design
matrix \\
\(A_{op}\) & A-value calculated by design function in blocksdesign
package in example case \\
\(A_{rs}\) & A-value result given by random search \\
\(\mathcal{D}_{rs}\) & Design result of random search \\
\(C_{SAlog}\) & Neighbour balance criteria of random search \\
\(A_{SAlog}\) & A-value result given by SA with log cooling schedule \\
\(\mathcal{D}_{SAlog}\) & Design result of SA with log cooling
schedule \\
\(C_{SAlog}\) & Neighbour balance criteria of SA with log cooling
schedule \\
\(A_{SAexp}\) & A-value result given by SA with exp cooling schedule \\
\(\mathcal{D}_{SAexp}\) & Design result of SA with exp cooling
schedule \\
\(C_{SAexp}\) & Neighbour balance criteria of SA with exp cooling
schedule \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Terminology
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Evenness of distribution (ED) & Multiple replications of a treatment are
evenly spread across the design field \\
Neighbour balance (NB) & Multiple treatments are not consistently placed
next to each other in the design \\
Pairwise permutation & Swapping two treatments in a design matrix to
explore new configurations \\
\end{longtable}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{sec-intro}

Experiments play a vital role in scientific research by helping to
establish causal relationships between variables.To achieve accurate and
reliable results in a comparative experiment, careful experimental
design is essential. In this thesis, we formally define an experimental
design as given in Definition~\ref{def-exp}.

\begin{definition}[Design]\protect\hypertarget{def-exp}{}\label{def-exp}

A comparative experiment is an experimental study involving a set of
experimental units, denoted as \(\Omega\), and a set of treatments,
denoted as \(\mathcal{T}\). The experimental design is defined as a
surjective function \(D: \Omega \rightarrow \mathcal{T}\). The sizes of
\(\Omega\) and \(\mathcal{T}\) are denoted as \(N\) and \(n_t\),
respectively. We use \(V_{\Omega} := \mathbb{R}^N\) and
\(V_{\mathcal{T}}\) to denote the vector spaces of \(\Omega\) and
\(\mathcal{T}\), respectively.

\end{definition}

In conducting an experiment, a proper design of experiment is crucial
for ensuring accurate and reliable results. A row-column design is a
particular type of experimental design where the experimental unit is
defined by the combination of two orthogonal factors, typically arranged
in rows and columns. The row-column experimental structure is common in
many biological applications, e.g.~96-well plates and crop field trials.
The treatments of interest are then randomly allocated with certain
constraints as not to confound the treatments with nuisance factors.See
Definition~\ref{def-rowcol} for a formal definition of a row-column
design. Note that rows and columns do not have to correspond to actual
physical dimensions.

Generally, a row-column design can be defined as

\begin{definition}[Row-column
design]\protect\hypertarget{def-rowcol}{}\label{def-rowcol}

A row-column design refers to an experiment where experimental units are
defined by the Cartesian product of two sets with sizes \(n_r\) and
\(n_c\), denoted as \(\mathcal{R}\) and \(\mathcal{C}\), respectively,
such that \(\Omega = \mathcal{R} \times \mathcal{C}\). We denote \(F_R\)
and \(F_C\) as the surjective functions that map \(\Omega\) to
\(\mathcal{R}\) and \(\mathcal{C}\), respectively. We use \(V_R\) and
\(V_C\) to denote vector spaces in \(\mathbb{R}^N\) for row and column
factors, respectively.

\end{definition}

In a row-column design, the row and column factors are often considered
as nuisance (or blocking) factors. The treatments of interest are then
randomly allocated with certain constraints as to reduce confounding the
treatment factors with these nuisance factors. An example of row-column
design is the \emph{Latin square design} where \(n_r = n_c = n_t\) and
each treatment appears exactly once in each row and column (see
Example~\ref{exm-lsd}).

\begin{example}[A \(3 \times 3\) Latin square
design]\protect\hypertarget{exm-lsd}{}\label{exm-lsd}

In a \(3 \times 3\) Latin square design, we let \(n_t = n_c = n_r = 3\)
and the resulting random instance of a design is as shown in
Figure~\ref{fig-lsd} with its corresponding design elements shown in
Table~\ref{tbl-lsd}.

\begin{figure}

\centering{

\includegraphics{intro_files/figure-pdf/fig-lsd-1.pdf}

}

\caption{\label{fig-lsd}A \(3 \times 3\) Latin square design.}

\end{figure}%

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0749}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0524}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0524}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0449}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 24\tabcolsep) * \real{0.0861}}@{}}

\caption{\label{tbl-lsd}The design elements of Figure~\ref{fig-lsd} with
\(\Omega\) and \(T\) as defined in Definition~\ref{def-exp} and \(F_R\)
and \(F_C\) as defined in Definition~\ref{def-rowcol}. The
\(n=9\)-vectors \(\boldsymbol{x}_{R_i}\) for \(i = 1, 2, 3\),
\(\boldsymbol{x}_{C_j}\) for \(j = 1, 2, 3\) and
\(\boldsymbol{x}_{T_k}\) for \(k = 1, 2, 3\) are the orthonormal basis
of \(V_R\), \(V_C\) and \(V_T\), respectively.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\(\omega \in \Omega\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F_R(\omega)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F_C(\omega)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(T(\omega)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{R_1}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{R_2}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{R_3}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{C_1}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{C_2}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{C_3}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{T_1}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{T_2}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\(\boldsymbol{x}_{T_3}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
2 & 1 & 2 & 2 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
3 & 1 & 3 & 3 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 \\
4 & 2 & 1 & 2 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
5 & 2 & 2 & 3 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\
6 & 2 & 3 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
7 & 3 & 1 & 3 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
8 & 3 & 2 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 \\
9 & 3 & 3 & 2 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 \\

\end{longtable}

\end{example}

Table~\ref{tbl-lsd} shows how we use a binary method to distinguish
between rows, columns, and types of treatments. For row-column designs,
we typically use a design matrix to model the arrangement of treatments.

\begin{definition}[]\protect\hypertarget{def-design-matrix}{}\label{def-design-matrix}

A design matrix \(\boldsymbol{X}\) is a matrix that represents the
arrangement of experimental units across different treatments in an
experimental design. Each row corresponds to an experimental unit, and
each column corresponds a treatment level.

\end{definition}

The last three columns of the Table~\ref{tbl-lsd} above can be viewed as
the design matrix for Example~\ref{exm-lsd}, which we will discuss it in
detail in the Chapter~\ref{sec-methods}.

In this thesis, the observational unit and experimental unit are
considered the same, thus the response of interest for the
\(\omega \in \Omega\) is a random variable denoted as \(Y_\omega\) and
its observed value (after the experiment) denoted as \(y_\omega\). We
use \(\boldsymbol{y} = (y_1, \ldots, y_n)^\top\) to denote the vector of
responses and use this interchangeably to denote a vector of random
variable as well. In its simplest form, we assume that the response can
be decomposed to the treatment and nuisance factors by assuming a linear
model,

\begin{equation}\phantomsection\label{eq-lm}{Y_\omega = \tau_{T(\omega)} + e_\omega,}\end{equation}

where \(\tau_{T(\omega)}\) is a constant value with respect to the
treatment received for \(\omega\) (referred to as the \emph{average
treatment effects}) and \(e_\omega\) is a random variable associated
with the remaining contribution from \(\omega\). Equation~\ref{eq-lm}
assumes an additive structure which is fundamental in estimating the
average treatment effect -- if this assumption does not hold to a large
extent, there is little point in conducting the experiment.

Different treatments usually have varying effects on the experimental
outcomes, known as treatment effects. The goal of our experimental
design is to more accurately measure and estimate the impact of
different treatments on the experimental results, i.e., the treatment
effects. To offer precise estimation of treatment effects, one way is to
seek the optimal value of some statistic criteria, for example,
A-criteria, minimizing the variance of elementary treatment contrasts.
\citet{butler2013optimal} has show the relation between optimizing
design and minimizing the value of A-criteria, and show some possible
algorithms to search optimal design in feasible set. These algorithms
mainly focus on comparing the arrangements of different treatments, that
is, doing permutations, and calculating their A values, optimizing
design by iterations.

However, some undesired cluster of replications or some treatment may
occur when algorithm are doing permutations along rows and columns. In
an experimental design, a treatment usually has multiple replications.
Specifically, clustering can refer to multiple replications of the same
treatment being grouped together, or different treatments' replications
frequently being grouped together in the experimental design see an
example in Figure~\ref{fig-NBED}. \citet{piepho2018neighbor} found that
such clustering is considered undesirable by experimenters who worry
that irregular environmental gradients might negatively impact multiple
replications of the same treatment, potentially leading to biased
treatment effect estimates. Williams emphasis that there is a need to
design a strategy to avoid clustering and achieve even distribution of
treatment replications among the experimental field. Two properties of
design are introduced. Evenness of distribution of treatment
replications, abbreviated as ED, and neighbour balance, abbreviated as
NB. Evenness of distribution represents how multiple replications of a
treatment are evenly spread across the design field, rather than being
placed close to each other. Neighbour balance, on the other hand,
indicates that multiple treatments are not consistently placed next to
each other in the design, promoting a more diverse adjacency
relationship between treatments. A good ED ensures every replications of
a treatment are widely spread in experimental field, and NB helps to
avoid replications of the some treatment cluster together repeatedly.
Williams introduce a scoring system to analysis ED and NB for a specific
design, and introduce a algorithm to optimize ED, NB and some average
efficiency factor can be represented by a specific statistic criteria.

\begin{figure}

\centering{

\includegraphics{intro_files/figure-pdf/fig-NBED-1.pdf}

}

\caption{\label{fig-NBED}Above, the figure presents an example of
clustering: multiple replications of treatment 8 are grouped together,
while treatments 1 and 19 are always adjacent to each other.}

\end{figure}%

We offer an optimization strategy for a design problem, which we can
improving ED and NB during optimizing statistic criteria for a design,
and avoid unwanted clustering and self-adjacency on the resulting
design.In this algorithm, we use A-criteria to evaluate the efficiency
of a design. Before the algorithm, we randomly generate a design as an
initial design, and calculate the A-criteria as initial value. We update
design by selecting a better among its neighbours. The neighbours are
pair-wise permutations of a design. Typically, we select a neighbour
from all pairwise permutations of a design for iteration, but this does
not ensure ED and NB. To ensure ED and NB during optimization, we add
some constraints when generating the pairwise permutations, only
maintain or better properties are accepted. By filtering design with bad
ED and NB, we optimize the statistic criteria such as A-value of the
design.

In the Chapter~\ref{sec-bg}, we introduced how to model row-column
designs by linear mixed, introduced the methods for calculating various
statistics and the meaning behind them, and provided relevant
mathematical proofs. In the Chapter~\ref{sec-methods}, we outlined the
relevant assumptions made for row-column designs. We introduced three
search algorithms---random search and two cooling schedules for
simulated annealing---providing detailed descriptions of their steps and
discussing their convergence properties. In the
Chapter~\ref{sec-results}, we first provide details of our simulation
set-up, presenting specific results with various figures, analyse the
performance of different algorithms, and made comparisons among them. In
the Chapter~\ref{sec-discuss}, we outline our limitations and discussed
some potential future directions.

\bookmarksetup{startatroot}

\chapter{Background}\label{sec-bg}

Linear (mixed) models are the primary method of analysis of experimental
data with response that have an approximate Normal distribution. In this
chapter, we expand on the model
\[\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau} + \boldsymbol{\epsilon}\]
to consider linear mixed models for the analysis of experimental data
with a more general experimental structure.

\section{Linear mixed model}\label{linear-mixed-model}

Linear mixed model extends linear model by incorporating additionally
incorporating random effects into the model that effectively give
greater flexibility and capability to incorporate known correlated
structures into the model. We now consider a linear mixed model
\begin{equation}\phantomsection\label{eq-lmm}{
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}+\boldsymbol{\epsilon}
}\end{equation} here \(\boldsymbol{y}\) is \(n\times 1\) vector for
\(n\) observations, \(\boldsymbol{\tau}\) is a \(t\times1\) parameter
vector of treatment factors, \(\boldsymbol{u}\) is a \(q \times1\)
parameter vector of blocking effects, and \(\boldsymbol{\epsilon}\) is
the \(n\times 1\) error vector, \(\boldsymbol{X}\) and
\(\boldsymbol{Z}\) are design matrices of dimension \(n \times t\) and
\(n \times q\) for treatment factors and blocking factors, respectively.
We here assume blocking factors are random effect, with random error
\(\boldsymbol{\epsilon}\) we have
\begin{equation}\phantomsection\label{eq-dist}{
\begin{bmatrix}
\boldsymbol{u} \\
\boldsymbol{\epsilon} 
\end{bmatrix}
\sim
N\left(
\begin{bmatrix}
\boldsymbol{0} \\
\boldsymbol{0}
\end{bmatrix}
,
\begin{bmatrix}
\boldsymbol{G} & \boldsymbol{0} \\
\boldsymbol{0} & \boldsymbol{R}
\end{bmatrix}
\right)
}\end{equation} where \(\boldsymbol{G}\) is the \(q \times q\) variance
matrix for \(\boldsymbol{u}\) and \(\boldsymbol{R}\) is \(n\times n\)
variance matrix for \(\boldsymbol{\epsilon}\).

Throughout this thesis, we assume
\(\boldsymbol{R} = \sigma^2\mathbf{I}_n\) and \(\boldsymbol{G}\) is a
block diagonal matrix where each block is a scaled identity matrix. In
practice, these structures can be more complex, e.g.
\citet{williamsConstructionCrossoverDesigns2007} consider autoregressive
and linear variance structures for crossover designs.

In addition, we assume in this thesis that the treatment effects are
fixed and blocking factors (i.e.~the row and column factors in
row-column design) are random effects. It should be noted, however, that
the treatment effects can be random effects based on the purpose of the
experiment
\citep{robinsonThatBLUPGood1991, butlerDesignFieldExperiments2014}.

\subsection{Estimation of model
parameters}\label{estimation-of-model-parameters}

\subsubsection{Fixed and random effects}\label{fixed-and-random-effects}

\citet{hendersonEstimationEnvironmentalGenetic1959} first derived the
set of equations, referred to as \emph{mixed model equations} by
\citet{smithMultiplicativeMixedModels1999}, as a precursor to the
estimation of fixed effects and predictions of random effects. These
equations are derived from maximizing the joint log density of the
response and random effects as shown in Lemma~\ref{lem-mme}.

\begin{lemma}[Mixed model
equations]\protect\hypertarget{lem-mme}{}\label{lem-mme}

The mixed model equations shown below are derived from maximising the
joint log density of \(\boldsymbol{y}\) and \(\boldsymbol{u}\).

\[\boldsymbol{C}\hat{\boldsymbol{\beta}} = \boldsymbol{W}\boldsymbol{R}^{-1}\boldsymbol{y},\]
where
\(\boldsymbol{C} = \boldsymbol{W}^\top\boldsymbol{R}^{-1}\boldsymbol{W} + \boldsymbol{G}^*\),
\(\boldsymbol{W} = \begin{bmatrix}\boldsymbol{X} & \boldsymbol{Z} \end{bmatrix}\),
\(\hat{\boldsymbol{\beta}} = (\hat{\boldsymbol{\tau}}^\top, \hat{\boldsymbol{u}}^\top)^\top\)
and
\(\boldsymbol{G}^* = \begin{bmatrix}\boldsymbol{0}_{p\times p} & \boldsymbol{0}_{p\times q} \\ \boldsymbol{0}_{q\times p} & \boldsymbol{G}^{-1}\end{bmatrix}\).

\begin{proof}
Given the model \ref{eq-lmm} with assumption in \ref{eq-dist}, we have
\(\boldsymbol{u}\sim N(\boldsymbol{0},\boldsymbol{G})\) and
\(\boldsymbol{y}|\boldsymbol{u}\sim N(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u},\boldsymbol{R})\)
with probability density functions as
\[\begin{aligned}f_u(\boldsymbol{u}) &= \frac{1}{\sqrt{(2\pi)^{q}|\boldsymbol{G}|}}\exp\left(-\frac{1}{2}\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right)\\
f_{y|u}(\boldsymbol{y}|\boldsymbol{u}) &= \frac{1}{\sqrt{(2\pi)^{n}|\boldsymbol{R}|}}\exp\left(-\frac{1}{2}(\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}))^\top\boldsymbol{R}^{-1}(\boldsymbol{y}-(\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}))\right).
\end{aligned}\] Ignoring the constants, we have \[\begin{aligned}
\log f_u(\boldsymbol{u}) &= -\frac{1}{2}\left(\log |\boldsymbol{G}|+\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right)\\
\log f_{y|u}(\boldsymbol{y}|\boldsymbol{u}) &=-\frac{1}{2}\left(\log |\boldsymbol{R}|+(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})^\top\boldsymbol{R}^{-1}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})\right).
\end{aligned}\] The joint density of \(\boldsymbol{y}\) and
\(\boldsymbol{u}\) is given as

\begin{equation}\phantomsection\label{eq-density}{f_{y,u}(\boldsymbol{y}, \boldsymbol{u}) = f_{y|u}(\boldsymbol{y}|\boldsymbol{u})f_u(\boldsymbol{u}),}\end{equation}
and taking the log of both sides for Equation~\ref{eq-density}, we have
the log of joint density as: \[\begin{aligned}
\mathcal{L} :=& \log f_{y,u}(\boldsymbol{y}, \boldsymbol{u}) \\
=& \log f_{y|u}(\boldsymbol{y}|\boldsymbol{u})+\log f_u(\boldsymbol{u})\\
=& -\frac{1}{2}\left(\log|\boldsymbol{R}|+\log|\boldsymbol{G}|+(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})^\top \mathbf{R}^{-1}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}-\boldsymbol{Z}\boldsymbol{u})+\boldsymbol{u}^\top\boldsymbol{G}^{-1}\boldsymbol{u}\right).
\end{aligned}\] We determine that
\(\frac{\partial\mathcal{L}}{\partial\boldsymbol{\tau}}=\frac{\partial\mathcal{L}}{\partial\boldsymbol{u}}=\boldsymbol{0}\),
and write the equation into a matrix form
\begin{equation}\phantomsection\label{eq-ll}{
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+ \boldsymbol{G}^{-1}
\end{bmatrix}
\begin{bmatrix}
\hat{\boldsymbol{\tau}}\\
\hat{\boldsymbol{u}}
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{y}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{y}
\end{bmatrix}.
}\end{equation}

\[
\Rightarrow (\boldsymbol{W}^\top\boldsymbol{R}^{-1}\boldsymbol{W} + \boldsymbol{G}^*)\hat{\boldsymbol{\beta}}
=\boldsymbol{W}\boldsymbol{R}^{-1}\boldsymbol{y}
\] Substituting the above \(\boldsymbol{C}\) leads to our conclusion
\end{proof}

\end{lemma}

\textbf{?@thm-blue} provides the specific form for estimation of the
parameters. :::: \{\#thm-blue\}

\section{Best linear unbiased estimates and
predictions}\label{best-linear-unbiased-estimates-and-predictions}

The solutions to the mixed models equations in Lemma~\ref{lem-mme}
(shown below) give the best linear unbiased estimates (BLUEs) for
\(\boldsymbol{\tau}\) and best linear unbiased predictions (BLUPs) for
\(\boldsymbol{u}\) provided that \(\boldsymbol{G}\) and
\(\boldsymbol{R}\) are known.

\begin{equation}\phantomsection\label{eq-esti}{\begin{aligned}
\hat{\boldsymbol{\tau}} &= (\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{y}\\
\hat{\boldsymbol{u}} &= \boldsymbol{G}\boldsymbol{Z}^\top\boldsymbol{V}^{-1}(\boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{\tau}}),
\end{aligned}}\end{equation}

where
\(\boldsymbol{V} = \boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top + \boldsymbol{R}\).
In this paper, we primarily discuss the treatment effect, so we provide
the proof for \(\hat{\boldsymbol{\tau}}\).

\begin{proof}
By cancelling \(\boldsymbol{u}\), we have \[
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})\boldsymbol{Z}^\top\boldsymbol{R}^{-1}y=\boldsymbol{X}^\top\boldsymbol{R}^{-1}y\\
\] \[
\Rightarrow \boldsymbol{X}^\top[\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}]\boldsymbol{X}\boldsymbol{\tau}=\boldsymbol{X}^\top[\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}]y\\
\] \[
\Rightarrow \boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}\boldsymbol{\tau}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}
\] where
\(\boldsymbol{P}=\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\),
let
\(\boldsymbol{C}_{11}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}\)
then we have the form of estimation for \(\hat{\boldsymbol{\tau}}\),
which is \[
\boldsymbol{C}_{11}\hat{\boldsymbol{\tau}}=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}
\] and the estimation of \(\boldsymbol{\tau}\) is
\(\hat{\boldsymbol{\tau}}=\boldsymbol{C}_{11}^{-1}\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}\),
Now, we only need to prove that \(\boldsymbol{P}=\boldsymbol{V}^{-1}\).

\[\begin{aligned}
\boldsymbol{P}\boldsymbol{V} &=(\boldsymbol{R}^{-1}-\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1})(\boldsymbol{R}+\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top - \boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top \\
&\quad \quad \quad \quad \quad \quad\quad\quad -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top\\
&=\boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}\boldsymbol{Z}^\top(\boldsymbol{I}+\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&=\boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{Z}^\top+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top)\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{I}+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G})\boldsymbol{Z}^\top\\
&= \boldsymbol{I} + \boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top -\boldsymbol{R}^{-1}\boldsymbol{Z}(\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1})^{-1}(\boldsymbol{G}^{-1}+\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z})\boldsymbol{G}\boldsymbol{Z}^\top\\
&= \boldsymbol{I}+\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top-\boldsymbol{R}^{-1}\boldsymbol{Z}\boldsymbol{G}\boldsymbol{Z}^\top\\
& = \boldsymbol{I}
\end{aligned}\] so we have
\(\hat{\boldsymbol{\tau}}=\boldsymbol{C}_{11}^{-1}\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{y}=\hat{\boldsymbol{\tau}}= (\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{V}^{-1}\boldsymbol{y}\).
\end{proof}

:::

\subsubsection{Variance parameters}\label{variance-parameters}

In practice, the variance parameters for \(\boldsymbol{G}\) and
\(\boldsymbol{R}\) are unknown and must be estimated from data. As a
consequence, the solutions in \textbf{?@thm-blue} are referred to as
Empirical-BLUEs (E-BLUE) or Empirical-BLUPs (E-BLUPs).

The variance parameters may be estimated using a method of moment
approach, however this approach can only be utilized for simple variance
structures with balanced numbers of treatments.

The modern prevalent approach to estimating variance parameters use
maximum likelihood estimates (MLEs) or residual maximum likelihood
(REML) estimates \citep{pattersonRecoveryInterblockInformation1971}. The
latter approach takes into account the loss of degrees of freedom to the
fixed effects so the estimates of variance are less biased. Interested
readers should see \citet{verbylaConditionalDerivationResidual1990} for
the derivation of REML and \citet{smithMultiplicativeMixedModels1999}
for computational strategies involving the average information algorithm
\citep{gilmourAverageInformationREML1995, johnsonRestrictedMaximumLikelihood1995}.

\subsection{Variance analyse of
estimators}\label{variance-analyse-of-estimators}

In linear mixed model analysis, when estimating the accuracy of
parameters and variance, we typically use the Fisher information matrix
to provide a method for variance analysis. For the linear mixed model
Equation~\ref{eq-lmm}, we have the log-likelihood of it is then given
as: \[
\log\ell(\boldsymbol{\tau};\boldsymbol{y}) = -\frac{n}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau})^\top(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\tau}).
\] The \((i,j)\)-th entry of the Fisher information matrix is defined as
\[
I_{ij}(\boldsymbol{\tau})=-\mathbb{E}\left(\frac{\partial^2}{\partial\tau_i\partial\tau_j}\log\ell(\boldsymbol{\tau};\boldsymbol{y})\right)
\] where \(\tau_i\) is the \(i\)-th entry of \(\boldsymbol{\tau}\).

\begin{lemma}[]\protect\hypertarget{lem-fim-lm}{}\label{lem-fim-lm}

The Fisher information matrix of Equation~\ref{eq-lm} for
\(\boldsymbol{\tau}\) is given as \[
\boldsymbol{C}_{\text{Fisher}} = -\mathbb{E}\left(\frac{\partial^2}{\partial\boldsymbol{\tau}\partial\boldsymbol{\tau}^\top}\log\ell(\boldsymbol{\tau};\boldsymbol{y})\right)
\]

\end{lemma}

And we have
\(\boldsymbol{\tau}\sim N(\hat{\boldsymbol{\tau}},\boldsymbol{C}_{\text{Fisher}}^{-1})\).
We will provide the specific form of this matrix and the related proof
in the next section.

\subsection{A-criterion}\label{a-criterion}

Now we have our estimation for the treatment factor, and experimental
design aims to further refine our design by focusing on the precision of
these estimates. Specifically, we aim to optimize the design so that the
treatment effects are estimated with minimal variance, ensuring that the
differences between any two treatment levels are as small as possible.
To achieve this, we introduce the A-value as a criterion for evaluating
the design.

Optimizing the A-value is crucial in row-column designs, for it directly
relates to the precision of the treatment effect estimates. The A-value,
a measure of design efficiency, quantifies how well the experimental
design minimizes the variability when estimating treatment effects. By
focusing on minimizing the A-value, we aim to achieve a design that
provides the most precise estimates of the difference of the effects
between two treatments. A lower A-value means that the design is more
efficient, leading to smaller variances for the difference between
treatment effect estimations.

Recall that in this thesis, we consider treatment factors
\(\boldsymbol{\tau}\) are fixed, to elucidate the influence of
A-criterion. Our discussion of the A-criterion below is always based on
this assumption. ::: \{\#def-A-value\} Basing on the model formula
Equation~\ref{eq-lmm}, and a estimation of treatment factor
\(\hat{\boldsymbol{\tau}}\) has \(n_{\tau}\) factors. A-criterion
measure the average predicted error variance of different treatments.
Let
\(V_{ij}= var(\hat{\tau}_i-\hat{\tau}_j)=var(\hat{\tau}_i)+var(\hat{\tau}_j)-2cov(\hat{\tau}_i,\hat{\tau}_j)\),
and an A-value \(\mathcal{A}\) is
\begin{equation}\phantomsection\label{eq-a}{
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}\sum_{i}\sum_{j<i}V_{ij}
}\end{equation} :::

To discover the relationship between the A-value and the design matrix,
I need to find the variance-covariance matrix of
\(\hat{\boldsymbol{\tau}}\). In fact, it can be proof that

\begin{lemma}[]\protect\hypertarget{lem-cov}{}\label{lem-cov}

For treatment effect and its estimator Equation~\ref{eq-esti}, we have
\((\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})\sim N(0,\boldsymbol{C}_{11}^{-1})\).

\begin{proof}
From Equation~\ref{eq-lmm} and basing on assumption
Equation~\ref{eq-dist} we have
\begin{equation}\phantomsection\label{eq-asp}{
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\tau}+\boldsymbol{Z}\boldsymbol{u}+\boldsymbol{\epsilon}\sim N(\boldsymbol{X}\boldsymbol{\tau},\boldsymbol{R}+\boldsymbol{ZGZ}^\top),
}\end{equation}

so also we have
\(\hat{\boldsymbol{\tau}}\sim N(\boldsymbol{\tau},\boldsymbol{X}^\top\boldsymbol{R}^{-1}(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)(\boldsymbol{X}^\top\boldsymbol{R}^{-1})^\top)\)

We denote \[\begin{aligned}
\boldsymbol{M} &= \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{X}\\
\boldsymbol{N} &= \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z}\\
\boldsymbol{J} &= \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{X}\\
\boldsymbol{K} &= \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1}\\
\end{aligned}\] In the context of row-column design, the
\(\boldsymbol{K}\) matrix is invertible. Schur complement of
\(\boldsymbol{K}\) is \[
\boldsymbol{S} = \boldsymbol{M} - \boldsymbol{N} \boldsymbol{K}^{-1} \boldsymbol{J}
\] And the inverse matrix of \(\boldsymbol{C}\) can be written as
\begin{equation}\phantomsection\label{eq-iv}{
\boldsymbol{C}^{-1}=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}=
\begin{bmatrix}
\boldsymbol{S}^{-1} & -\boldsymbol{S}^{-1} \boldsymbol{N} \boldsymbol{K}^{-1} \\
-\boldsymbol{K}^{-1} \boldsymbol{J} \boldsymbol{S}^{-1} & \boldsymbol{K}^{-1} + \boldsymbol{K}^{-1} \boldsymbol{J} \boldsymbol{S}^{-1} \boldsymbol{N} \boldsymbol{K}^{-1}
\end{bmatrix}
}\end{equation} So from Equation~\ref{eq-ll} we have \[
\begin{bmatrix}
\hat{\boldsymbol{\tau}} \\
\hat{\boldsymbol{u}}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{y}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{y}
\end{bmatrix}
=
\begin{bmatrix}
\boldsymbol{U}_1\\
\boldsymbol{U}_2
\end{bmatrix}\boldsymbol{y}
\] Here \[\begin{aligned}
&\boldsymbol{U}_1 = \boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\\
&\boldsymbol{U}_2 = \boldsymbol{C}^{21}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{22}\boldsymbol{Z}^\top\boldsymbol{R}^{-1}
\end{aligned}\]

From Equation~\ref{eq-ll} and Equation~\ref{eq-asp}, we have
\(\hat{\boldsymbol{\tau}}\sim N(\boldsymbol{\tau},\boldsymbol{U}_1(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)\boldsymbol{U}_1^\top)\)

And we have following results

\[\begin{aligned}
\begin{bmatrix}
\boldsymbol{U}_1 \\
\boldsymbol{U}_2
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X} & \boldsymbol{Z}
\end{bmatrix}
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X} & \boldsymbol{Z}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{C}^{11} & \boldsymbol{C}^{12} \\
\boldsymbol{C}^{21} & \boldsymbol{C}^{22}
\end{bmatrix}
(
\begin{bmatrix}
\boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{X}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}\\
\boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{X} & \boldsymbol{Z}^\top\boldsymbol{R}^{-1}\boldsymbol{Z}+\boldsymbol{G}^{-1}
\end{bmatrix}-
\begin{bmatrix}
0 & 0 \\
0 & \boldsymbol{G}^{-1}
\end{bmatrix})\\
&=
\boldsymbol{I}-
\begin{bmatrix}
0 & -\boldsymbol{C}^{12}\boldsymbol{G}^{-1} \\
0 & -\boldsymbol{C}^{22}\boldsymbol{G}^{-1}
\end{bmatrix}\\
&=
\begin{bmatrix}
\boldsymbol{I} & -\boldsymbol{C}^{12}\boldsymbol{G}^{-1} \\
0 & \boldsymbol{I}-\boldsymbol{C}^{22}\boldsymbol{G}^{-1}
\end{bmatrix}
\end{aligned}\]

So we have \[\begin{aligned}
\boldsymbol{U}_1\boldsymbol{X}&=\boldsymbol{I}\\
\boldsymbol{U}_1\boldsymbol{Z}&=-\boldsymbol{C}^{12}\boldsymbol{G}^{-1}\\
\end{aligned}\] For the variance of estimation we have \[\begin{aligned}
var(\hat{\boldsymbol{\tau}})
&=
\boldsymbol{U}_1(\boldsymbol{R}+\boldsymbol{ZGZ}^\top)\boldsymbol{U}_1^\top\\
&= \boldsymbol{U}_1\boldsymbol{R}\boldsymbol{U}_1^\top+\boldsymbol{U}_1\boldsymbol{ZGZ}^\top\boldsymbol{U}_1^\top\\
&= (\boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{R}^{-1}+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{R}^{-1})\boldsymbol{R}\boldsymbol{U}_1^\top+\boldsymbol{U}_1\boldsymbol{ZGZ}^\top\boldsymbol{U}_1^\top\\
&=\boldsymbol{C}^{11}\boldsymbol{X}^\top\boldsymbol{U}_1^\top+\boldsymbol{C}^{12}\boldsymbol{Z}^\top\boldsymbol{U}_1^\top+\boldsymbol{C}^{12}\boldsymbol{G}^{-1}\boldsymbol{G}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top\\
&=\boldsymbol{C}^{11}-\boldsymbol{C}^{12}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top+\boldsymbol{C}^{12}(\boldsymbol{G}^{-1})^\top(\boldsymbol{C}^{12})^\top\\
&=\boldsymbol{C}^{11}
\end{aligned}\]

what is the relation between \(\boldsymbol{C}_{11}\) and
\(\boldsymbol{C}^{11}\)? From Equation~\ref{eq-iv} we have
\(\boldsymbol{C}^{11}=\boldsymbol{S}^{-1}\) and \(\boldsymbol{S}\) is \[
\boldsymbol{S} = \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{X} - \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} (\boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1})^{-1} \boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{X}
\] And base on the complement of \(\boldsymbol{C}_{11}\), we rewrite the
\(\boldsymbol{S}\)

\[\begin{aligned}
\boldsymbol{S}
&=\boldsymbol{X}^\top(\boldsymbol{R}^{-1} - \boldsymbol{X}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} (\boldsymbol{Z}^\top \boldsymbol{R}^{-1} \boldsymbol{Z} + \boldsymbol{G}^{-1})^{-1} \boldsymbol{Z}^\top \boldsymbol{R}^{-1})\boldsymbol{X}\\
&=\boldsymbol{X}^\top\boldsymbol{P}\boldsymbol{X}\\
&=\boldsymbol{C}_{11}
\end{aligned}\]

So
\(var(\hat{\boldsymbol{\tau}})=\boldsymbol{C}^{11}=\boldsymbol{C}_{11}^{-1}\),
which means
\((\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})\sim N(0,\boldsymbol{C}_{11}^{-1})\).
\end{proof}

\end{lemma}

To examine a specific form of \(\boldsymbol{\tau}\), in general case, we
do linear transform on \(\boldsymbol{\tau}\):
\(\hat{\boldsymbol{\pi}}=\boldsymbol{D}\hat{\boldsymbol{\tau}}\), where
\(\boldsymbol{D}\) is some transform matrix, so we have
\(\boldsymbol{D}(\boldsymbol{\tau}-\hat{\boldsymbol{\tau}})=\boldsymbol{\pi}-\hat{\boldsymbol{\pi}}\sim N(0,\boldsymbol{D}\boldsymbol{C}_{11}^{-}\boldsymbol{D}^\top)\).
We denote
\(\boldsymbol{\Lambda}=\boldsymbol{D}\boldsymbol{C}_{11}^{-}\boldsymbol{D}^\top\).
If \(\boldsymbol{D}\) is identical matrix \(\boldsymbol{I}\), then
\(\boldsymbol{\Lambda}=\boldsymbol{C}_{11}^{-1}\) and
\(\hat{\boldsymbol{\pi}}=\hat{\boldsymbol{\tau}}\).

We know that A-criterion is the mean of predicted error variance of the
parameter from Equation~\ref{eq-a} i.e.~ \[
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}\sum_{i}\sum_{j<i}V_{ij}
\] Having variance-covariance matrix
\(\boldsymbol{\Lambda}=\boldsymbol{C}_{11}^{-1}\), we can rewrite the
sum part as
\(n_{\tau}tr(\boldsymbol{\Lambda})-\mathbb{1}_{n_{\tau}}^\top\boldsymbol{\Lambda}\mathbb{1}_{n_{\tau}}\).So
we have \begin{equation}\phantomsection\label{eq-an}{
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}[n_{\tau}tr(\boldsymbol{\Lambda})-\mathcal{1}_{n_{\tau}}^\top\boldsymbol{\Lambda}\mathcal{1}_{n_{\tau}}]
}\end{equation} same result from \citet{butler2013model}

Derivation above indicate that
\(\mathcal{A}\propto tr(\boldsymbol{\Lambda})\), A-criterion as the mean
of predicted error variance of the parameter, we prefer it as small as
possible to obtain a accurate result from experiment, which means the
trace of virance-covirance matrix \(\boldsymbol{\Lambda}\) should be as
small as possible. And this is our goal on optimal experimental design.

\section{Neighbor balance and eveness of
distribution}\label{neighbor-balance-and-eveness-of-distribution}

\subsection{Concepts of NB and ED}\label{concepts-of-nb-and-ed}

\citet{piepho2018neighbor} emphasis the the concepts of neighbour
balance and even distribution are crucial to mitigating biases and
ensuring the reliability of results in row-column design.

Neighbour balance (NB) refers to the principle that, in a row-column
experimental design, the frequency with which two treatments are
adjacent or near each other should not be excessively high. High
adjacency frequency between two treatments can lead to mutual influence,
which may cause bias to the experimental results. For example, if the
effect of one treatment can spread to neighbouring areas, frequent
adjacency could interfere with accurate measurement of each treatment's
true effect next to it. Therefore, it is essential to control the
adjacency frequency of different treatments to prevent high adjacency
for two specific treatments.

Even distribution (ED) aims to ensure that different replications of the
same treatment are widely distributed across the experimental field,
rather than being clustered in a specific area. This strategy helps to
avoid biases caused by specific environmental conditions in certain
parts of the experiment field. If replications of one treatment are over
concentrated in one area, unique environmental factors in that area
might affect the treatment's performance, leading to biased
observations. By evenly distributing replications, environmental
interference can be minimized, so that we can enhance the reliability of
the experimental results.

See Figure~\ref{fig-NBED} for instances where bad NB and ED occur.

\subsection{Measuring NB and ED}\label{measuring-nb-and-ed}

\subsubsection{Evaluating NB with adjacency
matrix}\label{evaluating-nb-with-adjacency-matrix}

In \citet{piepho2018neighbor}, there is a assumption that they are
optimizing a binary design, which means each treatment appears only once
in each row and column. Under this assumption, their balancing mechanism
considers diagonal adjacency, Knight moves, and even more distant points
to ensure an optimal balance. In my optimization process, I begin with a
randomly selected design matrix. Consequently, my approach considers not
only diagonal adjacency but also the adjacent points directly above,
below, to the left, and to the right.

We use an adjacency matrix to count the number of times each treatment
is adjacent to another. This matrix serves as a crucial tool in my
optimization process, enabling precise tracking and adjustment of
treatment placements to achieve neighbour balance.

We denote the adjacency matrix as \(\mathcal{U}\), and for treatment
\(i\) and \(j\) in treatment set \(T\), the symbol \(\mathcal{U}_{ij}\)
represents the count of times treatment \(i\) is adjacent to treatment
\(j\). Here ``adjacent'' means treatment \(j\) is located next to
treatment \(i\)

For Given design \(\mathcal{D}\) and \(\mathcal{D}_{r,c}\) represents
the treatment at row \(r\) and column \(c\). So \(\mathcal{U}_{ij}\) can
be expressed as:

\[
\mathcal{U}_{ij}=\sum_{r=1}^{n_r}\sum_{c=1}^{n_c}I_{r,c}(i) F_{r,c}(j)
\] where \[
F_{r,c}(j)=
\sum_{m \in \{-1,0,1\}}\sum_{n \in \{-1,1\}}I_{r+m,c+n}(j)+\sum_{m \in \{-1,1\}}I_{r+m,c}(j)
\] \(n_r\) and \(n_c\) are total number of rows and columns and
\(I_{r,c}(\cdot)\) is the indicator function, which takes value under
following cases \[
I_{r,c}(i)=
\begin{cases}
1 & \text{if } \mathcal{D}_{r,c}=i \\
0 & \text{if } \mathcal{D}_{r,c}\neq i \; \text{or } r<1,r>n_r,c<1,c>n_c\\
\end{cases}
\]

The function \(F_{r,c}(j)\) here is actually counting the times that
treatment \(j\) occurs at places around the position row \(r\) and
column \(c\).

We measure NB by taking difference of the maximum and minimum of the
elements in adjacency matrix \(\mathcal{U}\). Our NB criteria is \[
C_{NB}=\max\{\mathcal{U}_{ij}\}-\min\{\mathcal{U}_{ij}\}  \quad i,j\in T
\]

\subsubsection{Evaluating ED with minimum row and column
span}\label{evaluating-ed-with-minimum-row-and-column-span}

The goal of evaluating the evenness of distribution (ED) is to find the
row and column spans for treatments across the entire design matrix. We
would like this value as large as possible This ensures that the
treatments \(t\in T\) is distributed as evenly as possible within the
rows and columns, reducing clustering and promoting a balanced design.

The row span for a given treatment \(t \in T\) is defined as the
difference between the maximum and minimum row indices where t appears
in experiment field. \[
RS(t)=\max\{r:\mathcal{D}_{r,c}=t\}-\min\{r:\mathcal{D}_{r,c}=t\} \quad 1<r<n_r,\quad 1<c<n_c
\] And the minimum row span of a design \(\mathcal{D}\) is \[
MRS(\mathcal{D})=\min\{RS(t)\},\quad t \in T
\] Same for column span \[
CS(t)=\max\{c:\mathcal{D}_{r,c}=t\}-\min\{c:\mathcal{D}_{r,c}=t\} \quad 1<r<n_r,\quad 1<c<n_c
\] \[
MCS(\mathcal{D})=\min\{CS(t)\},\quad t \in T
\] So, for the changes in the design matrix \(\mathcal{D}\) during the
search process, we tend to accept only those changes where the Minimum
Row Span (\(MRS\)) and Minimum Column Span (\(MCS\)) remain the same or
become smaller.

\bookmarksetup{startatroot}

\chapter{Methods}\label{sec-methods}

\section{Modeling row-column design}\label{modeling-row-column-design}

As mentioned in Chapter~\ref{sec-bg}, we use a linear mixed model (LMM)
to model the row-column design having two distinct sources of variation,
typically referred to as ``row'' and ``column'' factors.

Recalling the assumptions, the row and column effects are treated as
random effects, which means that they are random factors for spatial or
systematic factors across different rows and columns of the experiment
field. The treatment effects, on the other hand, are treated as fixed
effects because they represent the primary factors of interest that we
wish to evaluate in terms of their influence on the response variable.

In Equation~\ref{eq-lmm}, the treatment effects are modelled as fixed
effects, represented by the treatment design matrix \(\boldsymbol{X}\)
with parameter vector \(\boldsymbol{\tau}\), measuring the influence of
each treatment on the response variable. The matrix \(\boldsymbol{X}\)
is constructed such that each row corresponds to an experimental unit,
and indicators in each column indicates whether a treatment is applied
or not.

The random effects are modelled through the matrix \(\boldsymbol{Z}\)
and parameter vector \(\boldsymbol{u}\). Matrix \(\boldsymbol{Z}\) is
designed to capture the row and column structure of the experimental
field, in which entries represent the position of each experimental unit
located in some specific rows and columns. The parameters in vector
\(\boldsymbol{u}\) are corresponding row and column effects. They are
assumed to follow a normal distribution with mean zero and
variance-covariance matrix \(\boldsymbol{G}\).

With \(\boldsymbol{y}\) as the vector of observed responses and
\(\boldsymbol{\epsilon}\) as error term, a row-column design can be
modelled by Equation~\ref{eq-lmm} . where \(\boldsymbol{X\tau}\)
represents the fixed treatment effects and \(\boldsymbol{Zu}\) captures
the random variations basing on rows and columns.

\subsection{Random effects matrix}\label{random-effects-matrix}

The design matrix for the random effects, which is row and column in
linear mixed model, follows a binary indicator structure, denoted as
\(\boldsymbol{Z}\). For example, we a have a \(4\times 4\) experiment
field, having \(4\) rows, \(4\) columns and \(16\) units. Then random
effects matrix should be a \(16 \times 8\) matrix containing binary
indicator as the one presented. \[
\boldsymbol{Z}_{\text{example}}=
\begin{bmatrix}
\begin{array}{cccc|cccc}
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\
\end{array}
\end{bmatrix}
\] Each row corresponds to a specific experimental unit, while the
columns represent the row and column factors in the experimental layout.
In this matrix, the first set of columns represents the column effects,
while the second set of columns represents the row effects. Each entry
in this matrix is binary, where a value of 1 indicates that the
experimental unit belongs to a specific row or column, and a 0 indicates
otherwise. For example, the first row of the matrix has a 1 in both
first and fifth columns, meaning that the corresponding unit of it is in
the first column and the first row. This structure ensures that each
unit is uniquely associated with one row and one column, and we can
model the random effects accordingly.

In a more general case, suppose we have a \(n_r\times n_c\) row-column
experiment field. We should have a random effect matrix with \(n_rn_c\)
rows and \(n_r+n_c\) columns with binary numbers. In this thesis, we
assume that the row effects and column effects are independent with each
other. However, in more complex experimental design cases, they may be
potentially correlated. The design of the random effects matrix, which
separates the row and column effects as independent variables,
simplifies the modelling process and the analysis of potential
correlations between these effects in more advanced settings. This
structure allows for easier identification and analysis of interactions
between row and column effects, making the model flexible and adaptable
to different levels of complexity in experimental designs.

\subsection{Design matrix for
treatments}\label{design-matrix-for-treatments}

The design matrix for the treatment effects is constructed to capture
the influence of each treatment on the response variable. In a
row-column experimental design, each experimental unit is assigned a
specific treatment. The entries in design matrix represents these
assignments using binary indicators. Like random effects matrix each row
in the matrix corresponds to an experimental unit, while each column
represents a different treatment. Here we still use \(4\times 4\)
experiment field as example, and suppose we have \(4\) different
treatments for each have \(4\) replications, needing \(16\) experiment
unite. An example design matrix \(\boldsymbol{X}\) for treatments should
be a \(16\times 4\) matrix with binary indicators as shown below \[
\boldsymbol{X}_{\text{example}}=
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 1\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
\end{bmatrix}
\] For a given experimental unit, that is a given row, the design matrix
contains a 1 in the column corresponding to the treatment applied to
that unit, and 0 elsewhere.This structure allows for a clear and
efficient representation of which treatment is applied to each unit. For
example, the first row of the example design matrix represents the first
treatment is applied in the unit locating on the first column, first
row.

if there are \(t\) treatments and \(N\) experimental units, the design
matrix will have \(N\) rows and \(t\) columns. Then the design matrix
for treatment \(\boldsymbol{X}\) with size \(N\times t\), should satisfy
that for any row \(n_{i}\) \[
\sum_{j=1}^{t} \boldsymbol{X}_{n_{i},j}=1
\] That is, there is only one treatment can be applied in each
experiment unit. And for any treatment \(t_j\) with \(r_j\)
replications, it has \[
\sum_{i=1}^{N}\boldsymbol{X}_{i,t_j}=r_j
\] All replications of a treatment are applied in experimental field.

\subsection{Assumptions for A-value
calculation}\label{assumptions-for-a-value-calculation}

It is important to clarify the key assumptions made in this study before
calculating A value for a row-column design. Recalling
Equation~\ref{eq-an}, for calculating A value we need the covariance
matrix for the random effects, matrix \(\boldsymbol{G}\), covariance
matrix for the error term, matrix \(\boldsymbol{R}\), transformation
matrix \(\boldsymbol{D}\), random effect matrix \(\boldsymbol{Z}\) and
treatment design matrix \(\boldsymbol{X}\). We need some basic set-up
for these matrices.

Assume that we now have a row-column matrix with \(n_r\) rows, \(n_c\)
columns and \(n_rn_c\) plots.

For the covariance matrix for the random effects, matrix
\(\boldsymbol{G}\), which captures the variability introduced by the row
and column effects. I assume it is a \((n_r+n_c)\times(n_r+n_c)\)
diagonal matrix, that is, it has following form,

\[
\boldsymbol{G}_{diag} =
\begin{bmatrix}
\sigma_{G_c}^2\boldsymbol{I}_{n_c} & 0 \\
0 & \sigma_{G_r}^2\boldsymbol{I}_{n_r}
\end{bmatrix}
\] \(\boldsymbol{I}_{n_c}\) is a \({n_c}\times{n_c}\) identity matrix,
same to \(\boldsymbol{I}_{n_r}\). And \(\sigma_{G_c}\) and
\(\sigma_{G_r}\) is a scale constant. This means that the influences of
the rows and columns are independent of each other, meaning there is no
correlation between different row and column effects in the design.

Similarly, for the covariance matrix for the error term, matrix
\(\boldsymbol{R}\), we assume that it is also diagonal, indicating that
the residual errors are uncorrelated across different experimental
units. In this case we have \[
\boldsymbol{R}_{diag} = \sigma_{R}^2\boldsymbol{I}_{n_rn_c}
\] with identity matrix \(\boldsymbol{I}_{n_rn_c}\) and scale constant
\(\sigma_{R}\).

\citet{butler2013optimal} have introduced linear transformations of the
treatment parameter vector \(\boldsymbol{\tau}\) by using a
transformation matrix \(\boldsymbol{D}\), which allows for the
investigation of linear combinations of treatments. However, in this
paper, I simplify the approach by setting \(\boldsymbol{D}\) as the
identity matrix \(\boldsymbol{I}_{RC}\). This means that we focus on the
individual effects of the treatments rather than their linear
combinations.

These assumptions makes the structure of the model becomes more
straightforward, allowing us to concentrate on the direct estimation of
treatment effects while maintaining independence among the random
effects and error terms.

It's important to note that in our design, the random effect matrix
\(\boldsymbol{Z}\) remains constant during the optimization process.This
means that while the row and column effects are accounted for as random
effects, their structure does not change.

With these assumptions in place, the A-value in the context of a
row-column design dependents only on the treatment design matrix
\(\boldsymbol{X}\). This means that the primary factor influencing the
A-value is the distribution and arrangement of treatments within the
design, and it directly impacts the variance of the treatment effect
estimates. Therefore, optimizing the A-value under these assumptions
becomes a problem of optimizing the treatment distribution in the
design, ensuring that the treatments are arranged in such a way that the
variance of the estimates is minimized.

\section{Searching Strategy}\label{searching-strategy}

Before introducing the details of the searching strategy, it is
important to establish a solid foundation by proving that the minimum of
the A-value exists. The existence of the minimum A-value implies that
the A-value has a lower bound, ensuring that the process of iteration
optimizing the experimental design is not endless. As we continue to
search for smaller A-values, this guarantees that we can eventually stop
when the A-value stabilizes or a sufficient number of iterations
reached.

This allows us to conclude that we have found an optimal or near-optimal
design. Therefore, the existence of this lower bound serves as a
critical foundation for our iterative search, giving us confidence that
the optimization will converge to a solution.

\subsection{Existence of the minimum of
A-value}\label{existence-of-the-minimum-of-a-value}

To prove that the minimum of the A-value exists, we establish a
objective function, that is, the A-value function \(A(\boldsymbol{X})\)
maps design matrix for treatment \(\boldsymbol{X}\) to its A-value. It
is a function that maps the design space to \(\mathbb{R}\). \[
A: \Omega \to \mathbb{R}, \quad \boldsymbol{X} \mapsto A(\boldsymbol{X})
\] Here \(\Omega\) is the design space contains all possible design
matrix \(\boldsymbol{X}\).

Now we proof the existence of minimum value of \(\boldsymbol{X}\), where
\(\boldsymbol{X}\in\Omega\).

\begin{proof}
Recalling Equation~\ref{eq-an}

\[
\mathcal{A}=\frac{1}{n_{\tau}(n_{\tau}-1)}[n_{\tau}tr(\boldsymbol{\Lambda})-\mathcal{1}_{n_{\tau}}^\top\boldsymbol{\Lambda}\mathcal{1}_{n_{\tau}}]
\] This expression is well-defined for all valid design matrices
\(\boldsymbol{X}\). So function \(A(\boldsymbol{X})\) is well-defined.

The A-value represents the average variance of the difference treatment
effect estimates, and since variances are always positive, the A-value
is naturally bounded from below by zero. So we have \[
A(\boldsymbol{X})\geq0
\] which implies that the A-criterion is bounded below.

In experimental design, the treatment design matrix \(\boldsymbol{X}\)
can take on a finite number of possible permutations, especially in
practical row-column designs where the number of treatments and
experimental units is fixed. In a finite search space, a lower bounded
function has its minimum value.
\end{proof}

\subsection{General structure}\label{general-structure}

In \citet{piepho2018neighbor}, the optimization of NB and ED was
typically carried out under the assumption that the A-value was already
optimal or fixed. This required identifying a set of solutions that
maintained the A-value while improving the balance and distribution
properties. In addition to assuming the A-value is fixed, another
approach they used is to randomly select a design and then optimizing ED
and NB.This process would be repeated multiple times, and the design
with the best A-value will be selected.

\begin{figure}

\centering{

\includegraphics[width=0.7\textwidth,height=\textheight]{../thesis/images/methods/method1.png}

}

\caption{\label{fig-usedmethod}Method been used}

\end{figure}%

These approach separates the optimization of ED and NB from the A-value,
while we try to merge these two process into one algorithm. We use
pairwise permutation among treatments to change treatment design during
iterations. And to avoid design with bad ED and NB, I am using some
criteria to filter the permutation, only maintain or better properties
are accepted. In this way, a row-column design that satisfies multiple
optimization requirements is achieved.

\begin{figure}

\centering{

\includegraphics[width=0.7\textwidth,height=\textheight]{../thesis/images/methods/method2.png}

}

\caption{\label{fig-mymethod}Attempting method}

\end{figure}%

Basing on optimal design search methods share a common set of features
in exploring the design space given in \citet{butler2013optimal}, my
searching method contains following part:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A calculation method for an optimal criterion for a given design
  matrix \(\boldsymbol{X}\)
\item
  An interchange policy to switch the design with in search space
  \(\Omega\)
\item
  An acceptance policy for a new design.
\item
  A stopping rule to terminate the search.
\end{enumerate}

The criterion calculation part has already been discussed earlier. We
will now introduce interchange policy of switching the design.

\subsection{Permutations and
filtering}\label{permutations-and-filtering}

We use permutations of the treatments to update the design matrix for
treatment. We randomly select two different treatments and swap them
within the design matrix during the permutation process, without making
drastic changes.

Let \(\boldsymbol{X}\) be the current design matrix for treatments,
where each row corresponds to an experimental unit, and each column
represents a treatment. Suppose we randomly select two different
treatments, \(t_i\) and \(t_j\) located in \(i\)th row and \(j\)th row
respectively. \(\boldsymbol{X}_{new}\) can be written as \[
\boldsymbol{X}_{new} = \boldsymbol{P}_{ij}\boldsymbol{X}
\] with permutation matrix defined as \[
 \boldsymbol{P}_{i j}=\left[
 \begin{array}{cccccccc}
 1 & 0 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\ 
 0 & 1 & \cdots & 0 & \cdots &0 & \cdots & 0 \\ 
 \vdots & \vdots & \ddots & \vdots & & \vdots & \cdots & \vdots \\ 
 0 & 0 & \cdots & 0 & \cdots & 1 & \cdots & 0 \\
 \vdots & \vdots & & \vdots & \ddots & \vdots & & \vdots\\
 0 & 0 & \cdots & 1 & \cdots & 0 & \cdots & 0 \\ 
 \vdots & \vdots & \cdots & \vdots & & \vdots & \ddots & \vdots \\ 
 0 & 0 & \cdots & 0 & \cdots & 0 & \cdots & 1
 \end{array}\right] 
\] It is an identity matrix with \(i\)-th row and \(j\)-th row swapped.

When performing permutations on the design matrix, we apply checks based
on the metrics of evenness of distribution (ED) and neighbour balance
(NB). The goal is to ensure that only the permutations which improve or
at least maintain desirable values for ED and NB are accepted, while
others are filtered out.

A random permutation of treatments is generated by swapping two
different treatments in the design matrix, as described earlier using a
permutation matrix. Once the new design matrix \(\boldsymbol{X}_{new}\)
with corresponding design \(\mathcal{D}_{new}\) is created, the next
step is to evaluate the quality of the permutation by calculating the ED
and NB values for the new configuration. As afore-mentioned, we have NB
and ED criteria for \(\boldsymbol{X}_{new}\), that is \(C_{NB}'\),
\(MRS(\mathcal{D}_{new})\) and \(MCS(\mathcal{D}_{new})\). Comparing the
newly generated design matrix (offspring) with the original design
matrix (parent), We accept the new permutation, if the ED and NB values
improves or maintains them without significantly worse. In practice, we
often set a tolerance for the ED and NB values. We generally allow ED
and NB to become slightly worse, as it's not necessary for them to
strictly improve or remain unchanged in every iteration. Our goal is to
achieve a balance between ED, NB, and the A-value. For instance, ED
might increase slightly while NB decreases a little, as long as the
overall balance between the three objectives is maintained. This
approach has the added benefit of lowering the acceptance threshold for
permutations, which speeds up the algorithm during the random selection
process. For instance, we set tolerance for ED and NB as \(T_{ED}\) and
\(T_{NB}\), which are none negative numbers. Current design matrix
\(\boldsymbol{X}\) corresponding design \(\mathcal{D}\) has ED and NB
value \(C_{NB}\), \(MRS(\mathcal{D})\) and \(MCS(\mathcal{D})\). New
design matrix \(\boldsymbol{X}_{new}\) mentioned above is accepted when
\begin{equation}\phantomsection\label{eq-accp}{\begin{aligned}
&(C_{NB}'\leq C_{NB}+T_{NB}) \\
\land & (MRS(\mathcal{D}_{new})\geq MRS(\mathcal{D})-T_{ED})\\
\land & (MCS(\mathcal{D}_{new})\geq MCS(\mathcal{D})-T_{ED})
\end{aligned}}\end{equation} is true.

\subsection{Random search}\label{random-search}

The random search algorithm begins with a randomly selected design
matrix. To ensure that the search is efficient and avoids getting
trapped in local optima, we introduce the concept of step length. This
parameter determines how many permutations we consider in each
iteration. Given the computational constraints, especially when the
number of treatments or rows and columns increases, it is impractical to
check all possible permutations of a design and evaluate each for
acceptance. However, we aim to explore as many permutations as possible
to avoid falling into local optima.

At each iteration, we randomly generate a set of permutations. For each
generated permutation, we apply the filtering step check by
Equation~\ref{eq-accp}. If the permutation is not accepted, we randomly
select another one and repeat the process. This continues until we have
successfully selected a number of permutations equal to the step length.

We denote step length as \(s\). tolerance for ED and NB as \(t_{ED}\)
and \(t_{NB}\). For a design matrix \(\boldsymbol{X}\), its ED criteria
- minimum row span and minimum column span is, \(mrs({\boldsymbol{X}})\)
and \(mcs({\boldsymbol{X}})\), and NB criteria is denote as
\(C_{NB}({\boldsymbol{X}})\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Input}: Original design matrix \(\boldsymbol{X}\), step length
  \(s\), tolerance for ED and NB as \(t_{ED}\) and \(t_{NB}\).
\item
  \textbf{Initialize}: \(k = 1\), ED and NB value for \(\boldsymbol{X}\)
  as \(mrs({\boldsymbol{X}})\), \(mcs({\boldsymbol{X}})\) and
  \(C_{NB}({\boldsymbol{X}})\).
\item
  While \(k < s\):

  \begin{itemize}
  \tightlist
  \item
    Generate a random permutation matrix \(\boldsymbol{P}\) by selecting
    two different treatments.
  \item
    Apply permutation: \(\boldsymbol{X_{new} = \boldsymbol{P}X}\).
  \item
    Calculate new values of ED and NB, \(mrs({\boldsymbol{X}_{new}})\),
    \(mcs({\boldsymbol{X}_{new}})\) and
    \(C_{NB}({\boldsymbol{X}_{new}})\).
  \item
    If new values satisfy
    \(mrs({\boldsymbol{X}_{new}}) >= mrs({\boldsymbol{X}}) - t_{ED}\),
    \(mcs({\boldsymbol{X}_{new}}) >= mcs({\boldsymbol{X}}) - t_{ED}\)
    and
    \(C_{NB}({\boldsymbol{X}_{new}}) <= C_{NB}({\boldsymbol{X}}) + t_{NB}\),
    accept and remember this \(\boldsymbol{X}_{new}\).
  \item
    Increment \(k\).
  \item
    If \(k = s\), output all selected permutations.
  \end{itemize}
\item
  \textbf{Output}:A set of \(k\) permutations of original design matrix
  \(\boldsymbol{X}\)
\end{enumerate}

The Random Search algorithm starts with a randomly selected initial
design matrix \(\boldsymbol{X}_0\), and we aim to minimize the A-value
associated with the design. We set a maximum number of iterations \(M\)
and a step length \(s\).

The process for each iteration can be described as follows:

We denote a random design matrix \(\boldsymbol{X}_0\), and iteration
counter \(k\). Maximum number of iteration is \(M\). And denote the
A-value for a random design matrix \(\boldsymbol{X}\) is
\(A(\boldsymbol{X})\). Current design matrix during the iteration we
have is denoted as \(\boldsymbol{X}_c\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialization}: Start with a random design matrix
  \(\boldsymbol{X}_0\), set the iteration counter `\(k = 0\), maximum
  number of iteration as \(M\) and step-length \(s\). A-value for the
  current design is \(A_c = A(\boldsymbol{X}_0)\).
\item
  \textbf{Iteration}: For each iteration \(k\), where \(k < M\):

  \begin{itemize}
  \tightlist
  \item
    Generate \(s\) random permutations \(\{\boldsymbol{X}_i\}\),
    \(i=1,2,\cdots,s\) of the current design matrix \(X_c\).
  \item
    Calculate the A-value \(A(\boldsymbol{X}_i)\)for each permutation
    \(\boldsymbol{X}_i\).
  \item
    Compare the A-values for all \(s\) permutations with the current
    design \(X_c\).
  \item
    Select the permutation with the smallest A-value,
    \(\boldsymbol{X}_j\), among the \(s\) candidates.
  \item
    If \(A(\boldsymbol{X}_j) < A(\boldsymbol{X}_c)\) update the current
    design matrix: \(\boldsymbol{X}_c = \boldsymbol{X}_j\); otherwise,
    retain the current design.
  \end{itemize}
\item
  \textbf{Termination}: Repeat this process until the maximum number of
  iterations \(M\) is reached.
\end{enumerate}

\subsection{Simulated annealing}\label{simulated-annealing}

Simulated Annealing (SA) is a global optimization algorithm inspired by
the annealing process in metallurgy. At higher temperatures, the
algorithm allows the acceptance of worse solutions to escape local
optima; as the temperature decreases, it converges toward an optimal
solution.

\citet{butler2013optimal} state that the Boltzmann probability \[
P(E)\propto e^{[-E/kt]}
\] offer a pathway to measuring the accept possibility during the
algorithm, where \(E\) is the energy of the state,for a given
temperature \(t\). The constant \(k\) is Boltzmann's constant. The
energy \(E\) corresponds to the value of the objective function, in our
case is A-value. During the iterative process, suppose we have a design
(state) having A-value (energy) \(A_1\) at time \(t\), and we are
shifting our design into a new design with A-value \(A_2\), resulting in
an energy change \(A_{\bigtriangleup} = A_2 - A_1\). If
\(A_{\bigtriangleup}\) is negative, that is \(A_2 < A_1\), we always
accept new design since we have lower A-value. If \(A_{\bigtriangleup}\)
is positive, acceptance follows the Metropolis criterion: a random
number, \(\delta \in [0,1]\), is generated, and \(A_2\) is accepted if
\(\delta \leq exp(-A_{\bigtriangleup}/kt)\). So we have acceptance rate
\(P(A_2)\) for a new A-value \(A_2\).
\begin{equation}\phantomsection\label{eq-prob}{
P(A_2)=
\begin{cases}
1 & \text{if } A_2<A_1 \\
\exp(-A_{\bigtriangleup}/kt) & \text{if } A_2>A_1\\
\end{cases}
}\end{equation} The basic elements of SA given by
\citet{bertsimas1993simulated} is as followed

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A finite set \(S\).
\item
  A real-valued cost function \(J\) defined on \(S\). Let
  \(S^*\subset S\) be the set of global minima of the function \(J\),
  assumed to be a proper subset of \(S\).
\item
  For each \(i\in S\), a set \(S(i) \subset S - {i}\), called the set of
  neighbours of \(i\).
\item
  For every \(i\), a collection of positive coefficients \(q_{ij}\),
  \(j\in S(i)\), such that \(\sum_{j\in S(i)} q_{ij} = 1\). It is
  assumed that \(j\in S(i)\), if \(i \in S(j)\).
\item
  A nonincreasing function \(T:N\rightarrow (0, \infty)\), called the
  cooling schedule. Here \(N\) is the set of positive integers, and
  \(T(t)\) is called the temperature at time \(t\).
\item
  An initial ``state'' \(x(0)\in S\).
\end{enumerate}

We are applying these elements to a SA that fits our case. Our SA having
following elements.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A finite searching space \(\{\boldsymbol{X}\}\) consisting all design
  matrix.
\item
  An A-value function \(A(\boldsymbol{X})\) defined on
  \(\{\boldsymbol{X}\}\) and it is real-valued. There is a set of
  \(\{\boldsymbol{X}^*\}\) that having optimal A-value and
  \(\{\boldsymbol{X}^*\}\in \{\boldsymbol{X}\}\)
\item
  For each design matrix \(\boldsymbol{X}\) in searching space, we
  consider all possible permutations filtered by Equation~\ref{eq-accp}
  are neighbours of \(\boldsymbol{X}\), which is a subset of searching
  space \(\{\boldsymbol{X}\}\).
\item
  All possible permutations have equal possibility to be chose, and the
  sum of the possibility is equal to \(1\).
\item
  A nonincreasing function \(T:N\rightarrow (0, \infty)\), \(T(t)\) is
  called the temperature at \(t\)-th iteration.
\item
  A random initial design \(\boldsymbol{X}_0\)
\end{enumerate}

Suppose we have a current design matrix \(\boldsymbol{X}_i\) and its
neighbours filtered by Equation~\ref{eq-accp} contains \(n_p\) numbers
of design. The next design is determined as follows:

A design matrix \(\boldsymbol{X}_j\) is randomly picked from the
neighbours of \(\boldsymbol{X}_i\). Suppose there is \(n_p\) design
matrices in the neighbours of \(\boldsymbol{X}_i\), then the probability
of selecting \(\boldsymbol{X}_j\) among all neighbours is
\(\frac{1}{n_p}\). This is actually the positive coefficients \(q_{ij}\)
afore-mentioned in basic elements of SA. We now denote
\(\boldsymbol{X}(t)\) to be the design matrix at \(t\)-th iteration.
Once \(\boldsymbol{X}_j\) is chosen, the next design matrix is
determined as follows: \[
P(\boldsymbol{X}(t+1)=\boldsymbol{X}_j|\boldsymbol{X}(t)=\boldsymbol{X}_i)
=\begin{cases}
1 & \text{if } A(\boldsymbol{X}_j)<A(\boldsymbol{X}_i) \\
\frac{1}{n_p}\exp(\frac{-(A(\boldsymbol{X}_j)-A(\boldsymbol{X}_i))}{T(t)}) & \text{if } A(\boldsymbol{X}_j)>A(\boldsymbol{X}_i)\\
\end{cases}
\]

\subsubsection{Convergence analyze}\label{convergence-analyze}

In this section, We will discuss the convergence properties of the SA
algorithm. During iteration process, temperature cooling schedule plays
an important role in convergence. It determines whether the algorithm
will reach an optimal or near-optimal solution over time. Basing on work
in \citet{sasaki1988time}, \citet{bertsimas1993simulated} gives a
conclusion on convergence properties, with afore-mentioned basic
element. Define that state \(i\) communicates with \(S^*\) at height
\(h\) if there exists a path in \(S\) that starts at \(i\) and ends at
some element of \(S^*\) and the largest value of \(J\) along the path is
\(J(i)+h\). Denote \(d^*\) be the smallest number such that every
\(i \in S\) communicates with \(S^*\) at height \(d^*\).

The SA algorithm converges if and only if \[
\lim_{t\to 0} T(t)=0
\] and \begin{equation}\phantomsection\label{eq-covcon}{
\sum_{t=1}^{\infty} \exp[-\frac{d^*}{T(t)}]=\infty
}\end{equation}

To ensure the condition above, the mostly chose cooling schedule is
\begin{equation}\phantomsection\label{eq-logcol}{
T(t) = \frac{d}{\log t}
}\end{equation} Here \(d\) is some constant. It can be initial
temperature.

\begin{proof}
It is obvious that \(\lim_{t\to 0} T(t)=0\) in Equation~\ref{eq-logcol}

Bring Equation~\ref{eq-logcol} in to Equation~\ref{eq-covcon}, we have
\[
\sum_{t=1}^{\infty} \exp[-\frac{d^*\log (t)}{d}]=\sum_{t=1}^{\infty}t^{-\frac{d^*}{d}}
\] It is a harmonic series and it diverge when \(\frac{d^*}{d}<1\), that
is, \(d^*<d\). So SA of we have a large enough \(d\) (initial
temperature).
\end{proof}

Although logarithmic cooling theoretically guarantees convergence to the
global optimum, it is computationally demanding and has a very slow
convergence rate in practice, making it less feasible for large-scale
problems. To address these limitations, exponential cooling is
introduced as a more practical alternative. While it does not offer a
theoretical guarantee of reaching the global optimum, as noted by
\citet{kirkpatrick1983optimization}. Because of limited time and
computational resource, we will try to use exponential cooling in our as
given in \citet{aarts1989simulated} and well-practised It provides
near-optimal solutions within a reasonable time, making it highly
effective for large combinatorial optimization problems.
\begin{equation}\phantomsection\label{eq-expcol}{
T(t) = T_0 \exp(-\alpha*t)
}\end{equation} Here \(T_0\) is the initial temperature, and \(\alpha\)
is the cooling rate determine how fast the temperature drop.

\subsubsection{Algorithm}\label{algorithm}

For accept probability Equation~\ref{eq-prob}, we usually start with
\(0.8\) and keep dropping when iteration goes.This probability depends
on the magnitude of change in the objective function A-value. To
simplify the initialization, we often conduct a few preliminary
iterations to observe the range and rate of change in A-values, then use
these observations to determine an initial temperature. Several studies
have explored methods for autonomously selecting this initial
temperature.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialization}: Start with a random design matrix
  \(\boldsymbol{X}_0\), set the iteration counter `\(k = 0\), maximum
  number of iteration as \(M\), A-value for the current design is
  \(A_c = A(\boldsymbol{X}_0)\), initial temperature \(T_0\) and a type
  of cooling schedule \(T(t)\).
\item
  \textbf{Iteration}: For each iteration \(k\), \(k<M\):

  \begin{itemize}
  \tightlist
  \item
    Randomly pick one permutation \(\boldsymbol{X}_{new}\) among all
    neighbours of current design \(\boldsymbol{X}_c\).
  \item
    Calculate the A-value \(A(\boldsymbol{X}_{new})\) for permutation
    \(\boldsymbol{X}_{new}\) and compare it with
    \(A(\boldsymbol{X}_c)\).
  \item
    If \(A(\boldsymbol{X}_{new}) < A(\boldsymbol{X}_c)\), update
    \(\boldsymbol{X}_c=\boldsymbol{X}_{new}\).
  \item
    If \(A(\boldsymbol{X}_{new}) > A(\boldsymbol{X}_c)\), update
    \(\boldsymbol{X}_c=\boldsymbol{X}_{new}\) with probability
    \(\exp[-(A(\boldsymbol{X}_{new}) - A(\boldsymbol{X}_c))/T(t)]\).
  \end{itemize}
\item
  \textbf{Termination}: Repeat this process until the maximum number of
  iterations \(M\) is reached.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Results}\label{sec-results}

\section{Simulation set-up}\label{simulation-set-up}

To begin the Results section, we'll look at the foundational set-up of
my simulation. In order to streamline the computation within limited
computational resources, I implemented a series of simplifications.

For \(\boldsymbol{G}\) matrix, I set it to be a diagonal matrix with
equal values along the diagonal. It implies that there is no correlation
between rows and columns, meaning that the effects for rows and columns
are considered independent. And the equal values along the diagonal
indicate that the variance of these row and column effects is the same.
In this simulation it is
\(\boldsymbol{G}_s = 10\boldsymbol{I}_{n_r+n_c}\)

For \(\boldsymbol{R}\) matrix, similarly, it is set as a diagonal matrix
with identical values along the diagonal. implying that the geographical
locations of different plots are independent of each other and variance
of residuals is uniform across plots. In this simulation it is
\(\boldsymbol{R}_s = 0.1\boldsymbol{I}_{n_r\times n_c}\)

To compare the differences between algorithms, we used the design
function from the R package blocksdesign to calculate the optimized
A-value \(A_{blocksdesign}\) basing on \citet{edmondson2020multi}, which
we then compared to the results from our iterative process.

For the consistency across all algorithms, we aimed to explore the
largest feasible range of row, column, and treatment combinations. We
set the maximum number of iterations \(M=2000\) for each algorithm. And
we add an early termination criterion: if the iterative algorithm's
A-value approaches \(A_{blocksdesign}\) within a specified tolerance
\(T_A\), the algorithm would stop iterating and output the A-value and
design matrix. In this simulation we set \(T_A=2\times 10^{-4}\)
Therefore we can evaluate the performance of different algorithms across
various combinations of row, column, and treatment numbers by comparing
the total computational time required, the actual iteration number
before reaching either \(M\) or an early termination based on
\(A_{blocksdesign}\), and the absolute difference between the
algorithm's and \(A_{blocksdesign}\).Basing on these evaluations, we can
identify how effectively each algorithm performs under different
complexity levels and determine the algorithm's efficiency and accuracy
across different cases.

In setting up random selection, the final parameter is the step length
\(s\). During runtime, the process of selecting permutations for
step-length iterations often accounts for a large portion of the total
functioning time.Additionally, this selection becomes especially
challenging when the row, column, and treatment numbers are low. In such
cases, while applying filtering criteria, it can be difficult to find a
sufficient number of unique permutations that meet the requirements, as
the design itself may not contain enough options. To ensure the
algorithm runs smoothly and completes within a reasonable time, we set
the step length to \(s=3\).

For the SA algorithm, we need to set the initial temperature \(T_0\).It
actually depends on the acceptance rate for higher A-values at the start
of the iteration process. As mentioned in the previous section, we
typically aim for an initial acceptance rate of around 0.8. That is we
hope \(\exp(-A_{\bigtriangleup}/T_0) \approx 0.8\). We often lack
precise information on \(A_{\bigtriangleup}\) before the algorithm, we
set the initial temperature based on an empirical estimate. We set
\(T_0=1\). Some adaptive algorithms for setting the initial temperature
are often discussed in the literature.

To examine combinations of row, column, and treatment numbers, we tested
a wide range of configurations. Row number ranged from 5 to 25, and
column number varied from 5 to 16. For each row and column combination,
we tested treatment numbers of 10, 20, 50, 80, and 100. To ensure that
the blocksdesign functions operated correctly and to maintain general
applicability, we ship a combination, if the product of rows and columns
was less than twice the treatment number.It ensures that each treatment
could have at least two replication. For each combination of row, column
and treatment number, we conduct three replication for more accurate
observation.

For the cases when treatment number cannot evenly divide the product of
rows and columns, suppose \(n_{plot}=n_r\times n_c\) and treatment
number is \(n_{\tau}\), we set base replication number as
\(\lfloor \frac{n_{plot}}{n_{\tau}} \rfloor\), and randomly pick
\((n_{plot}\mod n_{\tau})\) treatments assign
\(\lfloor \frac{n_{plot}}{n_{\tau}} \rfloor+1\) replication to these
treatments. This approach ensures that the number of each treatment is
as close as possible to one another.

\section{Result analyze}\label{result-analyze}

\subsection{General behviour of
algorithms}\label{general-behviour-of-algorithms}

Now we observe the overall behaviour of the algorithm, to examine the
change of A-value during the process. We here use example with
\(n_c=n_r=15\) and \(n_\tau=50\). we now use blocksdesign functions to
generate a optimal design \(\mathcal{D}_{op}\) with A-value
\(A_{op} = 0.5027\) under set-up, NB criteria
\(C_{NB}(\mathcal{D}_{op})=3\) and ED criteria
\(MRS(\mathcal{D}_{op})=MCS(\mathcal{D}_{op})=5\). See detailed
assignment as follow

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-blockdesign-1.pdf}

}

\caption{\label{fig-blockdesign}Detailed assignment for treatments given
by design function in blocksdesign package}

\end{figure}%

\subsubsection{Random selection}\label{random-selection}

We now look at how random selection behave during the process. The
iteration stops at \(549\)-th step with output A-value
\(A_{rs}=0.050478\) and NB criteria \(C_{NB}(\mathcal{D}_{rs})=4\) and
ED criteria \(MRS(\mathcal{D}_{rs})=5\) and \(MCS(\mathcal{D}_{rs})=7\).
The changes in the A-value over iterations and the specific design are
shown in Figure~\ref{fig-AvIRS} and Figure~\ref{fig-RS}.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-AvIRS-1.pdf}

}

\caption{\label{fig-AvIRS}A v.s. Iteration (Random search)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RS-1.pdf}

}

\caption{\label{fig-RS}Detailed assignment for treatments given by
random search}

\end{figure}%

From the results, the A-value obtained by Random Search is relatively
close to that of the block design, though slightly larger, as is the NB
statistic. Recall that we would like A-value and \(C_{NB}\) to be as
small as possible, and for both \(MRC\) and \(MRC\), the larger the
better. Therefore, the Random Search outcome has a larger minimum column
span, meaning it preform better on evenness of distribution.

\subsubsection{SA with log cooling
schedule}\label{sa-with-log-cooling-schedule}

For SA using the log cooling schedule Equation~\ref{eq-logcol},Unlike
random selection, the algorithm accepts higher A-values with a certain
probability, causing fluctuations in A-values throughout the iterations,
resulting in rises and falls rather than a steady decline. Details can
be seen in the Figure~\ref{fig-AvISAlog} and Figure~\ref{fig-SAlog}.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-AvISAlog-1.pdf}

}

\caption{\label{fig-AvISAlog}A v.s. Iteration (SA with log cooling
schedule)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-SAlog-1.pdf}

}

\caption{\label{fig-SAlog}Detailed assignment for treatments given by SA
with log cooling schedule}

\end{figure}%

As it indicate in the plot, the process went through all iterations,
reaching maximum iteration number \(M\). At the 2000th iteration, the
A-value is \(A_{SAlog}=0.050609\) with NB criteria
\(C_{SAlog}(\mathcal{D}_{SAlog})=5\) and ED criteria
\(MRS(\mathcal{D}_{SAlog})=7\) and \(MCS(\mathcal{D}_{SAlog})=6\).

Compared to random selection, the optimization of the A-value in SA is
less effective, since the limitation of a maximum iteration number
\(M\).Similar to the NB statistic, where SA shows a less optimal outcome
than the block design. However, the two ED statistics given by SA are
significantly improved compared to those provided by the blockdesign
function and random selection. Because of the slower decrease in
acceptance rates for higher A-values with log cooling schedule, we
observe that the algorithm occasionally continues to accept larger
A-values even in the latter stages.

\subsubsection{SA with exp cooling
schedule}\label{sa-with-exp-cooling-schedule}

This is also SA but with an exponential cooling schedule
Equation~\ref{eq-expcol}. Although exponential cooling schedule does not
theoretically guarantee SA convergence to the global optimum, it can
produce a relatively near-optimal solution within finite computational
resources and time constraints. Because of the rapid temperature
decrease in exponential cooling, the algorithm initially shows
fluctuations in A-values at the beginning of the iterations. However, as
the number of iterations increases, the acceptance rate for larger
A-values declines quickly, and algorithm tend to avoid accepting higher
A-values.Details are shown in the Figure~\ref{fig-SAexp} and
Figure~\ref{fig-SAlog}

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-AvISAexp-1.pdf}

}

\caption{\label{fig-AvISAexp}A v.s. Iteration (SA with exp cooling
schedule)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-SAexp-1.pdf}

}

\caption{\label{fig-SAexp}Detailed assignment for treatments given by SA
with exp cooling schedule}

\end{figure}%

The iteration process stopped at the \(1341\)-st iteration because the
A-value had come sufficiently close to the blockdesign A-value. The
results show that the A-value achieved is \(A_{SAexp} = 0.050473\), with
an NB statistic \(C_{SAexp}(\mathcal{D}_{rs})=4\), and ED statistics
\(MRS(\mathcal{D}_{SAexp})=5\) and \(MCS(\mathcal{D}_{SAexp})=6\).

The exponential cooling schedule achieved similar A-values and NB
statistics to those of random selection. However, its performance on the
ED statistic was less effective compared to both random selection and
log cooling. The reason may be the early termination of iterations in
exponential cooling, which limited the exploration of the whole design
matrix space.

In contrast, random selection evaluates multiple permutations (equal to
the step length \(s\)) in each iteration, which allows it to consider a
broader range of potential solutions. Meanwhile, log cooling slows down
the convergence, allow it has sufficient iterations to explore the
solution space, although it selects only one neighbour at a time.

\subsection{Analysis by Algorithm}\label{analysis-by-algorithm}

Now we begin to explore the relationship between function runtime and
the number of rows, columns, and treatments. Generally, runtime is
proportional to the number of iterations. Here, we have set the maximum
number of iterations to 2000. If in the simulation most runs reach the
maximum number of iterations, then evaluating the relationship between
runtime and rows, columns, and treatment numbers will no longer be
meaningful.Therefore, before the analysis, we need to examine the
distribution of iteration numbers. If most of the simulations do not
reach the maximum number of iterations, we can use runtime as a measure
of computational efficiency.

For random selection, many simulations did not reach the maximum number
of iterations, which can be observed in
Figure~\ref{fig-iteration-ditribution-RS}.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-iteration-ditribution-RS-1.pdf}

}

\caption{\label{fig-iteration-ditribution-RS}Proportion of Iteration
number for random selection}

\end{figure}%

So we present the relationship between runtime and the number of rows
under different treatment numbers, as shown in the figures below. As
mentioned earlier, we have three replications for each combination of
rows, columns, and treatment numbers. We calculate the average of the
three replications for each combination as the final result.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RvTRS-1.pdf}

}

\caption{\label{fig-RvTRS}The above plot shows the average runtime by
the number of rows with the color indicating the number of columns and
facetted by the number of treatments t for Random Selection algorithm.}

\end{figure}%

We can observe that, under different numbers of columns, the
relationship between rows and runtime tends to be similar. Here, we will
not discuss the origin or significance of this trend but instead focus
on the relation between variables and runtime. There is no clear
positive or negative relationship between the number of rows and
runtime. We see that as the number of rows increases, the runtime does
not consistently increase or decrease, which is related to the
randomness of random selection. During the iteration process, we might
be lucky enough to find a good design early and terminate the iteration,
or the A-value may gradually decrease over the iterations.

However, by observing the vertical axis of different subplots, we can
see that as the number of treatments increases, the range of runtime
fluctuations also increases. To demonstrate this, we present the
relationship between the number of treatments and runtime under
different rows and columns as Figure~\ref{fig-TvTRS} shows.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvTRS-1.pdf}

}

\caption{\label{fig-TvTRS}The above plot shows the average runtime by
the number of treatments with the color indicating the number of rows,
facetted by the number of columns for Random Selection algorithm.}

\end{figure}%

By observing the figure, we can see that as the number of treatments
increases, runtime shows a positive correlation, indicating that with
more treatments, random selection requires more time to find
permutations and make comparisons.

This approach leads us to consider the relationship between the number
of treatments and runtime. However, for SA with log cooling, due to the
slow convergence of log cooling and the limitation of a maximum number
of iterations, the algorithm often runs until the maximum number of
iterations is reached. This makes it difficult to observe the
relationship between the number of treatments and runtime, as runtime is
often related to the maximum number of iterations. See
Figure~\ref{fig-iteration-ditribution-SAlog} below for details.

\includegraphics{results_files/figure-pdf/unnamed-chunk-12-1.pdf}

In the figure, we can see that only when the number of rows and columns
is small, meaning the design space is limited, SA with log cooling can
terminate before reaching the maximum number of iterations. In most
other cases, the algorithm stops when it reaches the maximum number of
iterations. Therefore, we use the difference
\(d = (A_{SAlog}-A_{op})/A_{op}\) between the A-value obtained by SA
with log cooling at the maximum number of iterations and the A-value
calculated by the blockdesign function to evaluate the impact of the
number of rows, columns, and treatments on the efficiency of the
algorithm.

We first look at the influence of the number of rows on the distance.
Similar to the analysis method used for random selection, we provide
Figure~\ref{fig-RvDSAlog}.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RvDSAlog-1.pdf}

}

\caption{\label{fig-RvDSAlog}The above plot shows the difference between
\(A_{SAlog}\) and \(A_{op}\) by the number of rows with the color
indicating the number of columns and facetted by the number of
treatments for SA log Cooling algorithm.}

\end{figure}%

From the above figure, it is evident that the number of rows and columns
is positively correlated with the difference. The greater the number of
rows, the larger the difference. Similarly, the greater the number of
columns, the higher the lines (towards the red), indicating a larger
difference. Based on previous experience, we attempt to examine the
relationship between the number of treatments and distance, as shown in
the figure below.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvDSAlog-1.pdf}

}

\caption{\label{fig-TvDSAlog}The above plot shows the difference between
\(A_{SAlog}\) and \(A_{op}\) by the number of treatments, with the color
indicating the number of rows and facetted by the number of columns for
SA with log cooling algorithm.}

\end{figure}%

The results show that the treatment number is negatively correlated with
the difference. In other words, as the treatment number increases, the
resulting value (whether the iteration stops early or reaches the
maximum number of iterations) tends to be closer to the optimal value.
However, this does not necessarily mean that the absolute distance to
the optimal value decreases.

For SA with exponential cooling, we also examine the distribution of the
iteration numbers.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-iteration-ditribution-SAlog-1.pdf}

}

\caption{\label{fig-iteration-ditribution-SAlog}Proportion of Iteration
number for SA with exp cooling schedule}

\end{figure}%

It can be seen that most of the simulations did not reach the maximum
number of iterations, so we can use runtime to compare the effects of
rows, columns, and treatment numbers on the algorithm. Details in
Figure~\ref{fig-RvTSAexp}

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RvTSAexp-1.pdf}

}

\caption{\label{fig-RvTSAexp}The above plot shows the runtime by the
number of rows with the color indicating the number of columns and
facetted by the number of treatments for SA with exp cooling algorithm.}

\end{figure}%

We can see that when the treatment number is small, the influence of
rows and columns on runtime is not significant. This could be because
the design space is smaller, and for randomness, the algorithm stops
after fewer iterations. As the treatment number increases, the impact of
rows and columns on runtime becomes apparent: the greater the number of
rows and columns, the longer the runtime.This effect can also be
observed when examining the influence of the treatment number on
runtime.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvTSAexp-1.pdf}

}

\caption{\label{fig-TvTSAexp}The above plot shows the runtime by the
number of treatments with the color indicating the number of rows and
facetted by the number of columns for the filtered `SA exp Cooling'
algorithm.}

\end{figure}%

Similarly, we can observe that as the treatment number increases, the
runtime also increases. Additionally, as the number of rows increases,
the vertical axis of the plot also grows, indicating their positive
correlation.

\subsection{Comparison Between
Algorithms}\label{comparison-between-algorithms}

Now we compare the effectiveness of different algorithms. To simplify
the process, we focus on observing the impact of the number of rows,
columns, and treatments on the average runtime and the average
difference under equivalent conditions. For example, when comparing the
effect of the number of rows on the difference for different algorithms,
we take the average of the difference for different column counts and
treatment numbers at the same row level to simplify the visualization
and makes the comparison more intuitive. Before making comparisons, it
is important to note that, for our computational resource limitations,
we have limited the maximum iteration number to 2000. Therefore, our
comparison cannot determine which algorithm is better. Therefore, it
aims to compare the performance of different algorithms under the same
computational constraints. Here, we compare the effects of row number,
column number, and treatment number on the difference and runtime across
different algorithms.

\subsubsection{Comparing with
difference}\label{comparing-with-difference}

Now, we use the difference to compare the computational efficiency of
different algorithms. We present the figures showing the influence of
row number, column number, and treatment number on the difference for
the three algorithms. Note that for random selection and SA with
exponential cooling, the method for calculating the difference is the
same as for SA with logarithmic cooling, which means it represents the
relative difference, that is \(d = (A_{op}-A_{result})/A_{op}\).

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RvD-1.pdf}

}

\caption{\label{fig-RvD}The average relative difference between
\(A_{op}\) and \(A_{result}\) by the number of rows for different
algorithms, with the lines representing different algorithms.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-CvD-1.pdf}

}

\caption{\label{fig-CvD}The average relative difference between
\(A_{op}\) and \(A_{result}\) by the number of columns for different
algorithms, with the lines representing different algorithms.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvD-1.pdf}

}

\caption{\label{fig-TvD}The average relative difference between
\(A_{op}\) and \(A_{result}\) by the number of treatments for different
algorithms, with the lines representing different algorithms.}

\end{figure}%

Overall, it can be observed that for the difference, random selection
has the smallest value, SA with exponential cooling is slightly larger,
and SA with logarithmic cooling has the largest value. Moreover, SA with
logarithmic cooling is more influenced by the three variables.

In general, for all three algorithms, as the number of rows and columns
increases, the relative difference also increases. However, the
difference is negatively correlated with the treatment number. As
mentioned earlier, a smaller relative difference does not necessarily
mean that the absolute distance to the optimal value decreases. We can
use the Figure~\ref{fig-TvDabs} below to support this point.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvDabs-1.pdf}

}

\caption{\label{fig-TvDabs}The average absolute difference between
\(A_{op}\) and \(A_{result}\) by the number of treatments for different
algorithms, with the lines representing different algorithms.}

\end{figure}%

It can be seen here that if we consider the absolute difference, it is
actually positively correlated with the treatment number.

\subsubsection{Comparing with runtime}\label{comparing-with-runtime}

Now, let's see what the comparison looks like from the perspective of
runtime. We present Figure~\ref{fig-RvT}, Figure~\ref{fig-CvT}, and
Figure~\ref{fig-TvT} to illustrate this.

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-RvT-1.pdf}

}

\caption{\label{fig-RvT}the average runtime by the number of rows for
different algorithms, with the lines representing different algorithms.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-CvT-1.pdf}

}

\caption{\label{fig-CvT}The average runtime by the number of columns for
different algorithms, with the lines representing different algorithms.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{results_files/figure-pdf/fig-TvT-1.pdf}

}

\caption{\label{fig-TvT}The average runtime by the number of treatments
for different algorithms, with the lines representing different
algorithms.}

\end{figure}%

From the figures, we can see that SA with exponential cooling is the
fastest, while SA with log cooling takes a bit longer, and random
selection requires the longest time. When the number of rows, columns,
and treatments is still small, for the design space is limited and the
randomness of the algorithms, the differences are less noticeable.
However, as these numbers increase, the differences in runtime become
more significant, with random selection showing the fastest increase in
runtime. Overall, the number of rows, columns, and treatments are all
positively correlated with runtime, but the treatment number has a more
direct impact on runtime.

\subsection{Conclusion}\label{conclusion}

Now, we summarize the computational performance of the three algorithms
under constrained computational resources.

First, for random selection, as seen in the analysis above, this
algorithm provides relatively accurate results but requires a longer
runtime. When the number of treatments increases, the runtime
significantly rises. In our simulations, the step-length was set to 3.
Increasing the step-length would allow random selection to search the
design space more extensively but would require more time.

Next is Simulated Annealing (SA). Overall, the iteration speed is faster
compared to random selection. And increasing the number of rows,
columns, and treatments requires more iterations and longer runtime.
Although the log cooling schedule theoretically ensures convergence to
the global optimum, as shown in the simulations above, it may require
more iterations to reach the global optimum. On the other hand, the
exponential cooling schedule converges faster, and it often ends
iteration process early in the simulations. As mentioned earlier, while
it cannot theoretically ensure convergence to the global optimum, it can
provide an approximately optimal solution within a relatively short time
frame under constrained computational resources and avoid local optima
to some extent.

\bookmarksetup{startatroot}

\chapter{Discussion}\label{sec-discuss}

\section{Limitations}\label{limitations}

In this section, we focus on examining the limitations and shortcomings
of our algorithm, along with discussing the practical constraints
observed in the experimental simulation results. Our analysis is
organized into the following parts: computational efficiency and
convergence Rate, balancing multi-objective optimization, and
limitations and generalizability in practical applications.

\subsection{Computational efficiency and convergence
rate}\label{computational-efficiency-and-convergence-rate}

Based on the line plots from the previous section, we observe that the
convergence rate of our three algorithms significantly slow down as they
approach the optimal solution. This often results in giving a
near-optimal solutions, although it performs reasonably well within the
limited number of iterations \(M\) in our simulation. For iterative
methods, the algorithm actually explores the entire design matrix space
by evaluating permutations of the matrix. This causes our algorithms,
especially random selection, to have long runtimes when a large number
of iterations is performed. For certain fixed design assumptions, such
as the diagonal \(\boldsymbol{G}\) matrix and \(\boldsymbol{R}\) matrix
we used previously, an algorithm that optimizes based on matrix
structure could improve efficiency in such cases.

\subsection{Balancing multi-objective
optimization}\label{balancing-multi-objective-optimization}

We aim to optimize A while avoiding bad NB and ED statistics, which
essentially represents a multi-objective optimization problem. In our
current approach, we incorporate filtering steps during random search
permutation generation and SA neighbour generation to ensure that NB and
ED improve gradually throughout the iterations. Although this approach
allows for the stepwise optimization of all three statistics, it fails
to bing in the correlations among them and lacks the ability to balance
and trade off between the three. Using a multi-objective optimization
method would allow us to place these three statistics on an equal
priority level. In our current approach, however, the A-value has
consistently been the primary variable driving changes in the design
matrix.

\subsection{Limitations and generalizability in practical
applications}\label{limitations-and-generalizability-in-practical-applications}

When comparing the computational performance of different algorithms, we
mentioned that such comparisons cannot determine which algorithm is
better. For a deeper exploration of the performance of different
algorithms, a controlled comparison is needed. For example, setting a
common tolerance and allowing all algorithms sufficient time or
iteration count to complete, which would enable a fair comparison of
runtime or iteration numbers. Our analysis of the algorithms is based on
a limited set of constraints, observing and comparing the behaviour of
different algorithms under some conditions.

When discussing model errors, we mentioned using the R matrix for
modelling. For independent plots, using linear models and the R matrix
is very convenient. In \citet{piepho2018neighbor}, the \({AR1}\) model
is suggested as a way to address cases where plots are correlated. We
have not attempted or analysed algorithms using this type of model.

\section{Future Directions}\label{future-directions}

\subsection{Penalty objective
function}\label{penalty-objective-function}

Incorporate a penalty term into the objective function. When adjacent
treatments in the design are the same, this penalty term increases the
value of the objective function, making the optimization process more
likely to avoid having the same treatment in adjacent positions.

For example, we can define a following penalty function. For example we
have a design matrix \(X\), the penalty for self-adjacency is

\[
f(X) = \sum_{(i,j)\in X}I(X_i = X_j)
\] Here \(i\) and \(j\) are different plots next to each other in the
experiment field, and \(I(X_i = X_j)\) is an indicator function equals
to \(1\) if \(i\) plot and \(j\) plot have the same treatment.

So basing on this penalty function, For a certain design \(X\) we change
our function into:

\[
F(X) = \bar{f}_A^{RC} + \lambda \cdot f(X)
\] here \(\lambda < 0\)

Or use A-criteria : \[
G(x) = t\mathcal{A} + (1-t)\cdot f(X)
\] here \(0\leq t \leq 1\) and we minimize it

Usually we have such mathematical programming of inequality constrained
optimization problem: we minimize objective function \(f_0(x)\) with
inequality constrains \(f_i(x)\leq 0, i\in I=\{1,2,\cdots,m\}\).And we
have a well-known penalty function for this problem is \[
F (x, \rho) = f_0(x) + \rho \sum_{i\in I}max\{f_i(x),0\}
\] and a corresponding constrained penalty optimization problem is to
minimize penalty function \(F_2 (x, \rho)\), detailed information in
\citet{meng2013exactness}

\subsection{Algorithm improvment}\label{algorithm-improvment}

\subsubsection{SA-based multiobjective optimization
algorithms}\label{sa-based-multiobjective-optimization-algorithms}

When discussing the A-value, NB statistic, and ED statistic, our goal is
actually to find a process that can simultaneously optimize all three,
or finding a balance between three statistic criteria.SA is often used
in combinatorial optimization problems, as we are using it here to
optimize the arrangement of treatment plots. \citet{suman2006survey}
claim that in recent studies, SA has been applied in many
multi-objective optimization problems, because of its simplicity and
capability of producing a Pareto set of solutions in single run with
very little computational cost.

We say a solution is non-dominated if none of its objective values can
be improved without worsening at least one other objective. The Pareto
set (or Pareto front) is a set of non-dominated solutions in a
multi-objective optimization problem. \citet{suman2006survey} introduce
several SA for multi-objective optimization.

SA-based multi-objective optimization algorithms given by
\citet{suppapitnarm2000simulated} is a promising approach. Instead of
Equation~\ref{eq-prob} and using permutation filtering, they give
probability step write as this form \[
P = \min(1, \Pi_{i=1}^N\exp\{\frac{-\bigtriangleup s_i}{T_i}\})
\] Here \(N\) is the number of objective functions, and for each
objective function \(i\), \(\bigtriangleup s_i\) is the difference of
objective function between two solution and \(T_i\) is the current
temperature. Control the optimization rates of different objective
functions by setting different cooling schedules for each.

\subsubsection{Memory-Based Optimization Algorithms - TABU
search}\label{memory-based-optimization-algorithms---tabu-search}

Same with SA, Tabu Search is an optimization algorithm used for solving
combinatorial problems. It is a type of local search method that
enhances basic hill-climbing algorithms by using memory structures to
avoid revisiting previously explored solutions, introduced in
\citet{butler2013model}. This helps the search process escape from local
optima and encourages exploration of new areas in the solution space.
Tabu Search maintains a tabu list, a short-term memory that keeps track
of recent moves or solutions that should not be revisited for a certain
number of iterations. This is mathematically represented as a set
\(\mathcal{T}\) where recent solutions \(X_t\) are stored for a fixed
period \(k\) iterations: \[
\mathcal{T} = \{x_t|t \in [t-k,t)\}
\] If a solution \(x' \in \mathcal{T}\), \(x'\) is considered ``tabu''
and cannot be revisited. Therefore, besides our two approaches
here---random search with step-length and SA that probabilistically
accepts worse objective function values---this type of short-term
memory-enhanced algorithm (Memory-Enhanced Algorithms) helps ensure
efficient computation, minimizing resource consumption while maximizing
exploration within the solution space and helping escape local optima.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\renewcommand{\bibsection}{}
\bibliography{ref.bib}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Tools}\label{sec-tools}

This thesis was written using \href{https://quarto.org/}{Quarto 1.4.550}
\citep{quarto} and the following system and R packages:

\begin{verbatim}
- Session info ---------------------------------------------------------------
 setting  value
 version  R version 4.4.1 (2024-06-14 ucrt)
 os       Windows 11 x64 (build 22631)
 system   x86_64, mingw32
 ui       RTerm
 language (EN)
 collate  English_United States.utf8
 ctype    English_United States.utf8
 tz       Australia/Sydney
 date     2024-10-23
 pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)

- Packages -------------------------------------------------------------------
 package     * version date (UTC) lib source
 cli           3.6.3   2024-06-21 [1] CRAN (R 4.4.1)
 digest        0.6.36  2024-06-23 [1] CRAN (R 4.4.1)
 evaluate      0.24.0  2024-06-10 [1] CRAN (R 4.4.1)
 fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.4.1)
 htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.4.1)
 jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.4.1)
 knitr         1.47    2024-05-29 [1] CRAN (R 4.4.1)
 later         1.3.2   2023-12-06 [1] CRAN (R 4.4.1)
 processx      3.8.4   2024-03-16 [1] CRAN (R 4.4.1)
 ps            1.7.6   2024-01-18 [1] CRAN (R 4.4.1)
 quarto        1.4.4   2024-07-20 [1] CRAN (R 4.4.1)
 Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.4.1)
 rlang         1.1.4   2024-06-04 [1] CRAN (R 4.4.1)
 rmarkdown     2.27    2024-05-17 [1] CRAN (R 4.4.1)
 rstudioapi    0.16.0  2024-03-24 [1] CRAN (R 4.4.1)
 sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.4.1)
 tinytex       0.51    2024-05-06 [1] CRAN (R 4.4.1)
 xfun          0.45    2024-06-16 [1] CRAN (R 4.4.1)
 yaml          2.3.8   2023-12-11 [1] CRAN (R 4.4.1)

 [1] C:/Users/yaoya/AppData/Local/R/win-library/4.4
 [2] C:/Program Files/R/R-4.4.1/library

------------------------------------------------------------------------------
\end{verbatim}


\backmatter


\end{document}
